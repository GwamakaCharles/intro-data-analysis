<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Models | Introduction to Data Analysis</title>
  <meta name="description" content="Course Material for I2DA-2019 in Cognitive Science at University Osnabrück" />
  <meta name="generator" content="bookdown 0.14.1 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Models | Introduction to Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course Material for I2DA-2019 in Cognitive Science at University Osnabrück" />
  <meta name="github-repo" content="michael-franke/intro-data-analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Models | Introduction to Data Analysis" />
  
  <meta name="twitter:description" content="Course Material for I2DA-2019 in Cognitive Science at University Osnabrück" />
  

<meta name="author" content="" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="basics-of-probability-theory.html"/>
<link rel="next" href="inference.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span. { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#testing-showcasing"><i class="fa fa-check"></i><b>0.1</b> Testing / Showcasing</a><ul>
<li class="chapter" data-level="0.1.1" data-path="index.html"><a href="index.html#quotes"><i class="fa fa-check"></i><b>0.1.1</b> Quotes</a></li>
<li class="chapter" data-level="0.1.2" data-path="index.html"><a href="index.html#infobox"><i class="fa fa-check"></i><b>0.1.2</b> Infobox</a></li>
<li class="chapter" data-level="0.1.3" data-path="index.html"><a href="index.html#plots"><i class="fa fa-check"></i><b>0.1.3</b> Plots</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>I Foundations</b></span></li>
<li class="chapter" data-level="1" data-path="general-introduction.html"><a href="general-introduction.html"><i class="fa fa-check"></i><b>1</b> General Introduction</a></li>
<li class="chapter" data-level="2" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>2</b> Data</a></li>
<li class="chapter" data-level="3" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html"><i class="fa fa-check"></i><b>3</b> Basics of Probability Theory</a><ul>
<li class="chapter" data-level="3.1" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#probability"><i class="fa fa-check"></i><b>3.1</b> Probability</a><ul>
<li class="chapter" data-level="3.1.1" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#outcomes-events-observations"><i class="fa fa-check"></i><b>3.1.1</b> Outcomes, events, observations</a></li>
<li class="chapter" data-level="3.1.2" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#probability-distributions"><i class="fa fa-check"></i><b>3.1.2</b> Probability distributions</a></li>
<li class="chapter" data-level="3.1.3" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#interpretations-of-probability"><i class="fa fa-check"></i><b>3.1.3</b> Interpretations of probability</a></li>
<li class="chapter" data-level="3.1.4" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#urns-and-frequencies"><i class="fa fa-check"></i><b>3.1.4</b> Urns and frequencies</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#structured-events-marginal-distributions"><i class="fa fa-check"></i><b>3.2</b> Structured events &amp; marginal distributions</a><ul>
<li class="chapter" data-level="3.2.1" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#probability-table-for-a-flip--draw-scenario"><i class="fa fa-check"></i><b>3.2.1</b> Probability table for a flip-&amp;-draw scenario</a></li>
<li class="chapter" data-level="3.2.2" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#structured-events-and-joint-probability-distributions"><i class="fa fa-check"></i><b>3.2.2</b> Structured events and joint-probability distributions</a></li>
<li class="chapter" data-level="3.2.3" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#marginalization"><i class="fa fa-check"></i><b>3.2.3</b> Marginalization</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#conditional-probability"><i class="fa fa-check"></i><b>3.3</b> Conditional probability</a><ul>
<li class="chapter" data-level="3.3.1" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#bayes-rule"><i class="fa fa-check"></i><b>3.3.1</b> Bayes rule</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#random-variables"><i class="fa fa-check"></i><b>3.4</b> Random variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#notation-terminology"><i class="fa fa-check"></i><b>3.4.1</b> Notation &amp; terminology</a></li>
<li class="chapter" data-level="3.4.2" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#cumulative-distribution-functions-mass-density"><i class="fa fa-check"></i><b>3.4.2</b> Cumulative distribution functions, mass &amp; density</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#expected-value-variance"><i class="fa fa-check"></i><b>3.5</b> Expected value &amp; variance</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>4</b> Models</a><ul>
<li class="chapter" data-level="4.1" data-path="models.html"><a href="models.html#overview"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="models.html"><a href="models.html#two-notions-of-probability-revisited"><i class="fa fa-check"></i><b>4.2</b> Two notions of probability (revisited)</a><ul>
<li class="chapter" data-level="4.2.1" data-path="models.html"><a href="models.html#frequentism-probabilities-as-properties-of-the-world"><i class="fa fa-check"></i><b>4.2.1</b> Frequentism — Probabilities as properties of the world</a></li>
<li class="chapter" data-level="4.2.2" data-path="models.html"><a href="models.html#bayesianism-probabilities-as-subjective-beliefs"><i class="fa fa-check"></i><b>4.2.2</b> Bayesianism — Probabilities as subjective beliefs</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="models.html"><a href="models.html#likelihood-prior-posterior"><i class="fa fa-check"></i><b>4.3</b> Likelihood, Prior, &amp; Posterior</a><ul>
<li class="chapter" data-level="4.3.1" data-path="models.html"><a href="models.html#probability-density-function-vs.likelihood-function"><i class="fa fa-check"></i><b>4.3.1</b> Probability density function vs. Likelihood function</a></li>
<li class="chapter" data-level="4.3.2" data-path="models.html"><a href="models.html#prior-posterior"><i class="fa fa-check"></i><b>4.3.2</b> Prior &amp; Posterior</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="models.html"><a href="models.html#modeling"><i class="fa fa-check"></i><b>4.4</b> Modeling</a><ul>
<li class="chapter" data-level="4.4.1" data-path="models.html"><a href="models.html#introductory-example"><i class="fa fa-check"></i><b>4.4.1</b> Introductory example</a></li>
<li class="chapter" data-level="4.4.2" data-path="models.html"><a href="models.html#steps-of-data-analysis"><i class="fa fa-check"></i><b>4.4.2</b> Steps of Data Analysis</a></li>
<li class="chapter" data-level="4.4.3" data-path="models.html"><a href="models.html#notation"><i class="fa fa-check"></i><b>4.4.3</b> Notation</a></li>
<li class="chapter" data-level="4.4.4" data-path="models.html"><a href="models.html#an-outlook-hierarchical-models"><i class="fa fa-check"></i><b>4.4.4</b> An outlook: Hierarchical models</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="models.html"><a href="models.html#further-examples"><i class="fa fa-check"></i><b>4.5</b> Further examples</a><ul>
<li class="chapter" data-level="4.5.1" data-path="models.html"><a href="models.html#difference-between-two-groups"><i class="fa fa-check"></i><b>4.5.1</b> Difference between two groups</a></li>
<li class="chapter" data-level="4.5.2" data-path="models.html"><a href="models.html#simple-linear-regression-with-one-metric-predictor"><i class="fa fa-check"></i><b>4.5.2</b> Simple linear regression with one metric predictor</a></li>
<li class="chapter" data-level="4.5.3" data-path="models.html"><a href="models.html#notation-simple-regression-model"><i class="fa fa-check"></i><b>4.5.3</b> Notation Simple Regression model</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="models.html"><a href="models.html#further-elaboration-on-modeling-in-anticipation-of-the-topic-estimation"><i class="fa fa-check"></i><b>4.6</b> Further elaboration on modeling (in anticipation of the topic “estimation”)</a><ul>
<li class="chapter" data-level="4.6.1" data-path="models.html"><a href="models.html#beta-binomial-model---one-group-revisited"><i class="fa fa-check"></i><b>4.6.1</b> Beta-Binomial model - one group (revisited)</a></li>
<li class="chapter" data-level="4.6.2" data-path="models.html"><a href="models.html#beta-binomial-model---two-groups-revisited"><i class="fa fa-check"></i><b>4.6.2</b> Beta-Binomial model - two groups (revisited)</a></li>
<li class="chapter" data-level="4.6.3" data-path="models.html"><a href="models.html#simple-linear-regression-model-revisited"><i class="fa fa-check"></i><b>4.6.3</b> Simple linear regression model (revisited)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>5</b> Inference</a></li>
<li class="chapter" data-level="6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>6</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="7" data-path="model-comparison.html"><a href="model-comparison.html"><i class="fa fa-check"></i><b>7</b> Model Comparison</a></li>
<li class="chapter" data-level="8" data-path="bayesian-hypothesis-testing.html"><a href="bayesian-hypothesis-testing.html"><i class="fa fa-check"></i><b>8</b> Bayesian hypothesis testing</a></li>
<li class="chapter" data-level="9" data-path="model-criticism.html"><a href="model-criticism.html"><i class="fa fa-check"></i><b>9</b> Model criticism</a></li>
<li class="part"><span><b>II Appliied (generalized) linear modeling</b></span></li>
<li class="chapter" data-level="10" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>10</b> Simple linear regression</a></li>
<li class="chapter" data-level="11" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>11</b> Logistic regression</a></li>
<li class="chapter" data-level="12" data-path="multinomial-regression.html"><a href="multinomial-regression.html"><i class="fa fa-check"></i><b>12</b> Multinomial regression</a></li>
<li class="chapter" data-level="13" data-path="ordinal-regression.html"><a href="ordinal-regression.html"><i class="fa fa-check"></i><b>13</b> Ordinal regression</a></li>
<li class="chapter" data-level="14" data-path="hierarchical-regression.html"><a href="hierarchical-regression.html"><i class="fa fa-check"></i><b>14</b> Hierarchical regression</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html"><i class="fa fa-check"></i><b>A</b> Appendix: Common probability distributions</a><ul>
<li class="chapter" data-level="A.1" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#an-important-family-the-exponential-family"><i class="fa fa-check"></i><b>A.1</b> An important family: The Exponential Family</a></li>
<li class="chapter" data-level="A.2" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#excursos-information-entropy-and-maximum-entropy-principal"><i class="fa fa-check"></i><b>A.2</b> Excursos: “Information Entropy” and “Maximum Entropy Principal”</a><ul>
<li class="chapter" data-level="A.2.1" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#information-entropy"><i class="fa fa-check"></i><b>A.2.1</b> Information Entropy</a></li>
<li class="chapter" data-level="A.2.2" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#deriving-probability-distributions-using-the-maximum-entropy-principle"><i class="fa fa-check"></i><b>A.2.2</b> Deriving Probability Distributions using the Maximum Entropy Principle</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#selected-continous-distributions-of-random-variables"><i class="fa fa-check"></i><b>A.3</b> Selected continous distributions of random variables</a><ul>
<li class="chapter" data-level="A.3.1" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#normal-distribution"><i class="fa fa-check"></i><b>A.3.1</b> Normal distribution</a></li>
<li class="chapter" data-level="A.3.2" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#chi-square-distribution"><i class="fa fa-check"></i><b>A.3.2</b> Chi-square distribution</a></li>
<li class="chapter" data-level="A.3.3" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#f-distribution"><i class="fa fa-check"></i><b>A.3.3</b> F distribution</a></li>
<li class="chapter" data-level="A.3.4" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#student-t-distribution-not-part-of-exponential-family"><i class="fa fa-check"></i><b>A.3.4</b> Student t-distribution (not part of exponential family)</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#selected-discrete-distributions-of-random-variables"><i class="fa fa-check"></i><b>A.4</b> Selected discrete distributions of random variables</a><ul>
<li class="chapter" data-level="A.4.1" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#binomial-distribution"><i class="fa fa-check"></i><b>A.4.1</b> Binomial distribution</a></li>
<li class="chapter" data-level="A.4.2" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#poisson-distribution"><i class="fa fa-check"></i><b>A.4.2</b> Poisson distribution</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#understanding-distributions-as-random-variables"><i class="fa fa-check"></i><b>A.5</b> Understanding distributions as random variables</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="models" class="section level1">
<h1><span class="header-section-number">4</span> Models</h1>
<div id="overview" class="section level2">
<h2><span class="header-section-number">4.1</span> Overview</h2>
<p>In the first section we introduce the  based on a short revision of the debate between  and . After a short overview we formalize the conceptual ideas and the components of the presented model. We finish by discussing further example models.</p>
</div>
<div id="two-notions-of-probability-revisited" class="section level2">
<h2><span class="header-section-number">4.2</span> Two notions of probability (revisited)</h2>
<p>What are probabilities?
Two viewpoints can be distinguished: Probabilities exist “outside in the world” or they are “subjective beliefs” <span class="citation">(Kruschke <a href="#ref-kruschke2015">2015</a>)</span>. Although both notions imply different approaches how to deal with probabilities, the mathematical properties are quite similar <span class="citation">(Kruschke <a href="#ref-kruschke2015">2015</a>)</span>.</p>
<div id="frequentism-probabilities-as-properties-of-the-world" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Frequentism — Probabilities as properties of the world</h3>
<p>When thinking about probabilities as existing “outside in the world” one has to consider the random process that produces the observed data. A Frequentist imagine this random process beeing repeated a large number of times so that the probability of an outcome is the number of observed outcomes devided by the total number of observations <span class="math inline">\(n\)</span> <span class="citation">(Dobson and Barnett <a href="#ref-dobson2008">2008</a>)</span>. Frequentism is an approach that searches for relative frequencies in a large number of trials <span class="citation">(Vallverdú <a href="#ref-vallverdu2015">2016</a>)</span>.
The parameter <span class="math inline">\(\theta\)</span>, the probability of interest, is the value of the relative frequency when <span class="math inline">\(n\)</span> becomes infinitely large. Consequently, the parameter <span class="math inline">\(\theta\)</span> can be estimated from the observed data, <span class="math inline">\(\hat{\theta}\)</span>, by maximizing the likelihood function (see section “Likelihood, Prior &amp; Posterior”) <span class="citation">(Dobson and Barnett <a href="#ref-dobson2008">2008</a>)</span>.</p>
</div>
<div id="bayesianism-probabilities-as-subjective-beliefs" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Bayesianism — Probabilities as subjective beliefs</h3>
<p>Another notion of probabilities is to think of them as “beliefs” inside one’s mind.</p>

<p>The core of Bayesian methods is Bayes’ theorem which describes how prior belief is combined with observed data:</p>
<p><span class="math display">\[P(H|Data)=\frac{P(Data|H)*P(H)}{P(Data)}\]</span></p>

<p><span class="math display">\[Posterior=\frac{Likelihood*Prior}{Marginal\textrm{ } Likelihood},\]</span></p>
<p>The job of the “marginal Likelihood” in the denominator is to standardize the posterior and thus to ensure it sums up to one (integrates to one). Therefore, the key lesson of Bayes’ theorem is <span class="citation">(McElreath <a href="#ref-mcelreath2015">2015</a>)</span>:
<span class="math display">\[Posterior \propto Likelihood * Prior\]</span></p>

<p>A parameter in <em>Bayesian methods</em> is conceptualized as a random variable with its own distribution (the posterior) that summarizes the current state of knowledge. The expected value of the posterior is the best guess about the true value of the parameter and its variability reflects the amount of uncertainty <span class="citation">(Kline <a href="#ref-kline2013">2013</a>)</span>.</p>
<p>In <em>Frequentist statistics</em>, a parameter is seen as a constant that should be estimated with sample statistics <span class="citation">(Kline <a href="#ref-kline2013">2013</a>)</span>.</p>
</div>
</div>
<div id="likelihood-prior-posterior" class="section level2">
<h2><span class="header-section-number">4.3</span> Likelihood, Prior, &amp; Posterior</h2>
<p>Before the topic of is introduced, some conceptual notions are neccessary.</p>
<div id="probability-density-function-vs.likelihood-function" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Probability density function vs. Likelihood function</h3>
<p>As already know from the “probability”-lecture, for a coin flip the probability of each outcome can be described with the , as there exist two discrete outcomes (head or tail) and a constant probability <span class="math inline">\(\theta\)</span>:
<span class="math display">\[p([X=x]|\theta)=\theta^{[x]}(1-\theta)^{(1-[x])}\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\theta\)</span> is the probability of “head” for the coin flip;</li>
<li>the bracket [ ] indicates that the particular parameter is treated as unknown.</li>
</ul>
<p>With the formula above the probability of <span class="math inline">\(\theta\)</span> is treated as “known”. Accordingly, the distribution of possible outcomes can be derived.</p>
<p>But, as already seen in the introductory part, one might be interested in the value of <span class="math inline">\(\theta\)</span> by a given data set. Then <span class="math inline">\(\theta\)</span> is unknown and the data are observed. Treating <span class="math inline">\(\theta\)</span> as parameters instead of <span class="math inline">\(x\)</span> leads to the  — a mathematical formula that specifies the plausibility of the data. It states the probability of any possible observation:
<span class="math display">\[p(X=x|[\theta])=[\theta]^x(1-[\theta])^{(1-x)}\]</span></p>
<p>Please be aware that through exchanging the roles of <span class="math inline">\(x\)</span> and <span class="math inline">\(\theta\)</span> in the second equation (likelihood function) this function is no longer a probability distribution and thus does not integrate to 1.</p>
</div>
<div id="prior-posterior" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Prior &amp; Posterior</h3>
<p>The clearest difference between frequentist and Bayesian methods is the incorporation of <em>prior information</em> in the Bayesian framework. The posterior distribution results by combining the likelihood with the prior information.</p>
<p>When an uninformative (e.g. uniform) prior is used then the posterior is completely dependent on the data. The influence of the prior on the posterior depends on their relative weighting. Remember that the posterior is the conditional distribution of the parameter given the data. The mathematical procedure behind it depicts .</p>
<p>Consider for example a Beta distribution with the parameters a and b: <span class="math inline">\(\beta \sim Beta(a,b)\)</span>. Assuming a coin flip experiment, the parameters of a Beta distribution can be interpreted as <span class="math inline">\(a=\)</span>no. of heads and $b = $no. of tails, thus, <span class="math inline">\(n=a+b\)</span>. Consequently, Beta(1,1) has a lower weight and thus influence on the posterior than Beta(5,5).</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="co"># simulate coin flip data set</span></a>
<a class="sourceLine" id="cb5-2" data-line-number="2">sample.space &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb5-3" data-line-number="3">theta &lt;-<span class="st"> </span><span class="fl">0.5</span>              <span class="co"># probability of a success (here: head)</span></a>
<a class="sourceLine" id="cb5-4" data-line-number="4">X &lt;-<span class="st"> </span><span class="dv">30</span>                   <span class="co"># number of trials in the experiment</span></a>
<a class="sourceLine" id="cb5-5" data-line-number="5">n &lt;-<span class="st"> </span><span class="dv">10</span>                   <span class="co"># number of observations</span></a>
<a class="sourceLine" id="cb5-6" data-line-number="6">k &lt;-<span class="st"> </span><span class="dv">0</span>                    <span class="co"># number of heads [initialization]</span></a>
<a class="sourceLine" id="cb5-7" data-line-number="7"></a>
<a class="sourceLine" id="cb5-8" data-line-number="8">## repeat experiment N-times</a>
<a class="sourceLine" id="cb5-9" data-line-number="9"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="st"> </span>n) {</a>
<a class="sourceLine" id="cb5-10" data-line-number="10">  k[i] &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">sample</span>(sample.space, <span class="dt">size =</span> X, <span class="dt">replace =</span> <span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb5-11" data-line-number="11">                     <span class="dt">prob =</span> <span class="kw">c</span>(theta, <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta)))</a>
<a class="sourceLine" id="cb5-12" data-line-number="12">}</a>
<a class="sourceLine" id="cb5-13" data-line-number="13"></a>
<a class="sourceLine" id="cb5-14" data-line-number="14">## show results in a tibble</a>
<a class="sourceLine" id="cb5-15" data-line-number="15">coin.flip1 &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="st">&quot;n&quot;</span> =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">1</span>, <span class="dt">to=</span>n, <span class="dt">by=</span><span class="dv">1</span>),</a>
<a class="sourceLine" id="cb5-16" data-line-number="16">                     <span class="st">&quot;k&quot;</span> =<span class="st"> </span>k,</a>
<a class="sourceLine" id="cb5-17" data-line-number="17">                     <span class="st">&quot;x&quot;</span> =<span class="st"> </span>X )</a>
<a class="sourceLine" id="cb5-18" data-line-number="18"></a>
<a class="sourceLine" id="cb5-19" data-line-number="19"><span class="co">#show the different prior distributions </span></a>
<a class="sourceLine" id="cb5-20" data-line-number="20"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb5-21" data-line-number="21">Beta1 &lt;-<span class="st"> </span><span class="kw">rbeta</span>(<span class="dt">n=</span><span class="fl">1e5</span>, <span class="dt">shape1=</span><span class="dv">1</span>, <span class="dt">shape2=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb5-22" data-line-number="22">Beta2 &lt;-<span class="st"> </span><span class="kw">rbeta</span>(<span class="dt">n=</span><span class="fl">1e5</span>, <span class="dt">shape1=</span><span class="dv">10</span>, <span class="dt">shape2=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb5-23" data-line-number="23"><span class="kw">plot</span>(<span class="kw">density</span>(Beta1), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">4</span>), <span class="dt">main=</span><span class="st">&quot;Prior 1: Beta(1,1)&quot;</span>, <span class="dt">xlab=</span><span class="kw">expression</span>(theta))</a>
<a class="sourceLine" id="cb5-24" data-line-number="24"><span class="kw">plot</span>(<span class="kw">density</span>(Beta2), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">4</span>), <span class="dt">main=</span><span class="st">&quot;Prior 2: Beta(10,10)&quot;</span>, <span class="dt">xlab=</span><span class="kw">expression</span>(theta))</a></code></pre></div>
<p><img src="I2DA_files/figure-html/unnamed-chunk-10-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="co">#show posterior distributions weighted by priors</span></a>
<a class="sourceLine" id="cb6-2" data-line-number="2"></a>
<a class="sourceLine" id="cb6-3" data-line-number="3">prior1 &lt;-<span class="st"> </span><span class="kw">set_prior</span>(<span class="st">&quot;beta(1,1)&quot;</span>, <span class="dt">class =</span> <span class="st">&quot;Intercept&quot;</span>)</a>
<a class="sourceLine" id="cb6-4" data-line-number="4">prior2 &lt;-<span class="st"> </span><span class="kw">set_prior</span>(<span class="st">&quot;beta(10,10)&quot;</span>, <span class="dt">class =</span> <span class="st">&quot;Intercept&quot;</span>)</a>
<a class="sourceLine" id="cb6-5" data-line-number="5"></a>
<a class="sourceLine" id="cb6-6" data-line-number="6">model1 &lt;-<span class="st"> </span><span class="kw">brm</span>(<span class="dt">data=</span>coin.flip1, <span class="dt">formula=</span> k<span class="op">|</span><span class="kw">trials</span>(x) <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">prior=</span>prior1, </a>
<a class="sourceLine" id="cb6-7" data-line-number="7">              <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>))</a></code></pre></div>
<pre><code>## 
## SAMPLING FOR MODEL &#39;d233e2cb513249af99cc8901673a6dea&#39; NOW (CHAIN 1).
## Chain 1: Rejecting initial value:
## Chain 1:   Error evaluating the log probability at the initial value.
## Chain 1: Exception: beta_lpdf: Random variable is 1.21153, but must be less than or equal to 1  (in &#39;model291bdc52912_d233e2cb513249af99cc8901673a6dea&#39; at line 22)
## 
## Chain 1: Rejecting initial value:
## Chain 1:   Error evaluating the log probability at the initial value.
## Chain 1: Exception: beta_lpdf: Random variable is -0.661984, but must be &gt;= 0!  (in &#39;model291bdc52912_d233e2cb513249af99cc8901673a6dea&#39; at line 22)
## 
## Chain 1: Rejecting initial value:
## Chain 1:   Error evaluating the log probability at the initial value.
## Chain 1: Exception: beta_lpdf: Random variable is 1.60251, but must be less than or equal to 1  (in &#39;model291bdc52912_d233e2cb513249af99cc8901673a6dea&#39; at line 22)
## 
## Chain 1: Rejecting initial value:
## Chain 1:   Error evaluating the log probability at the initial value.
## Chain 1: Exception: beta_lpdf: Random variable is 1.48169, but must be less than or equal to 1  (in &#39;model291bdc52912_d233e2cb513249af99cc8901673a6dea&#39; at line 22)
## 
## Chain 1: Rejecting initial value:
## Chain 1:   Error evaluating the log probability at the initial value.
## Chain 1: Exception: beta_lpdf: Random variable is -1.9142, but must be &gt;= 0!  (in &#39;model291bdc52912_d233e2cb513249af99cc8901673a6dea&#39; at line 22)
## 
## Chain 1: 
## Chain 1: Gradient evaluation took 1.1e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.075007 seconds (Warm-up)
## Chain 1:                0.059783 seconds (Sampling)
## Chain 1:                0.13479 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;d233e2cb513249af99cc8901673a6dea&#39; NOW (CHAIN 2).
## Chain 2: Rejecting initial value:
## Chain 2:   Error evaluating the log probability at the initial value.
## Chain 2: Exception: beta_lpdf: Random variable is -1.05163, but must be &gt;= 0!  (in &#39;model291bdc52912_d233e2cb513249af99cc8901673a6dea&#39; at line 22)
## 
## Chain 2: Rejecting initial value:
## Chain 2:   Error evaluating the log probability at the initial value.
## Chain 2: Exception: beta_lpdf: Random variable is -0.649745, but must be &gt;= 0!  (in &#39;model291bdc52912_d233e2cb513249af99cc8901673a6dea&#39; at line 22)
## 
## Chain 2: Rejecting initial value:
## Chain 2:   Error evaluating the log probability at the initial value.
## Chain 2: Exception: beta_lpdf: Random variable is -0.842446, but must be &gt;= 0!  (in &#39;model291bdc52912_d233e2cb513249af99cc8901673a6dea&#39; at line 22)
## 
## Chain 2: Rejecting initial value:
## Chain 2:   Error evaluating the log probability at the initial value.
## Chain 2: Exception: beta_lpdf: Random variable is -0.243793, but must be &gt;= 0!  (in &#39;model291bdc52912_d233e2cb513249af99cc8901673a6dea&#39; at line 22)
## 
## Chain 2: 
## Chain 2: Gradient evaluation took 1e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.082612 seconds (Warm-up)
## Chain 2:                0.059407 seconds (Sampling)
## Chain 2:                0.142019 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;d233e2cb513249af99cc8901673a6dea&#39; NOW (CHAIN 3).
## Chain 3: Rejecting initial value:
## Chain 3:   Error evaluating the log probability at the initial value.
## Chain 3: Exception: beta_lpdf: Random variable is -1.51227, but must be &gt;= 0!  (in &#39;model291bdc52912_d233e2cb513249af99cc8901673a6dea&#39; at line 22)
## 
## Chain 3: Rejecting initial value:
## Chain 3:   Error evaluating the log probability at the initial value.
## Chain 3: Exception: beta_lpdf: Random variable is -0.7489, but must be &gt;= 0!  (in &#39;model291bdc52912_d233e2cb513249af99cc8901673a6dea&#39; at line 22)
## 
## Chain 3: Rejecting initial value:
## Chain 3:   Error evaluating the log probability at the initial value.
## Chain 3: Exception: beta_lpdf: Random variable is 1.69145, but must be less than or equal to 1  (in &#39;model291bdc52912_d233e2cb513249af99cc8901673a6dea&#39; at line 22)
## 
## Chain 3: Rejecting initial value:
## Chain 3:   Error evaluating the log probability at the initial value.
## Chain 3: Exception: beta_lpdf: Random variable is -1.11063, but must be &gt;= 0!  (in &#39;model291bdc52912_d233e2cb513249af99cc8901673a6dea&#39; at line 22)
## 
## Chain 3: Rejecting initial value:
## Chain 3:   Error evaluating the log probability at the initial value.
## Chain 3: Exception: beta_lpdf: Random variable is -1.26977, but must be &gt;= 0!  (in &#39;model291bdc52912_d233e2cb513249af99cc8901673a6dea&#39; at line 22)
## 
## Chain 3: Rejecting initial value:
## Chain 3:   Error evaluating the log probability at the initial value.
## Chain 3: Exception: beta_lpdf: Random variable is 1.27315, but must be less than or equal to 1  (in &#39;model291bdc52912_d233e2cb513249af99cc8901673a6dea&#39; at line 22)
## 
## Chain 3: Rejecting initial value:
## Chain 3:   Error evaluating the log probability at the initial value.
## Chain 3: Exception: beta_lpdf: Random variable is 1.24464, but must be less than or equal to 1  (in &#39;model291bdc52912_d233e2cb513249af99cc8901673a6dea&#39; at line 22)
## 
## Chain 3: Rejecting initial value:
## Chain 3:   Error evaluating the log probability at the initial value.
## Chain 3: Exception: beta_lpdf: Random variable is -0.140077, but must be &gt;= 0!  (in &#39;model291bdc52912_d233e2cb513249af99cc8901673a6dea&#39; at line 22)
## 
## Chain 3: 
## Chain 3: Gradient evaluation took 9e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.100956 seconds (Warm-up)
## Chain 3:                0.095623 seconds (Sampling)
## Chain 3:                0.196579 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;d233e2cb513249af99cc8901673a6dea&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 7e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.078142 seconds (Warm-up)
## Chain 4:                0.06993 seconds (Sampling)
## Chain 4:                0.148072 seconds (Total)
## Chain 4:</code></pre>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" data-line-number="1">samples.model1 &lt;-<span class="st"> </span><span class="kw">posterior_samples</span>(model1)</a>
<a class="sourceLine" id="cb8-2" data-line-number="2"></a>
<a class="sourceLine" id="cb8-3" data-line-number="3">model2 &lt;-<span class="st"> </span><span class="kw">brm</span>(<span class="dt">data=</span>coin.flip1, <span class="dt">formula=</span> k<span class="op">|</span><span class="kw">trials</span>(x) <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">prior=</span>prior2, </a>
<a class="sourceLine" id="cb8-4" data-line-number="4">              <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>))</a></code></pre></div>
<pre><code>## 
## SAMPLING FOR MODEL &#39;4d8687896c7ea933ac28802adabeca38&#39; NOW (CHAIN 1).
## Chain 1: Rejecting initial value:
## Chain 1:   Error evaluating the log probability at the initial value.
## Chain 1: Exception: beta_lpdf: Random variable is -1.10496, but must be &gt;= 0!  (in &#39;model291b653ea6cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22)
## 
## Chain 1: Rejecting initial value:
## Chain 1:   Error evaluating the log probability at the initial value.
## Chain 1: Exception: beta_lpdf: Random variable is -1.6886, but must be &gt;= 0!  (in &#39;model291b653ea6cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22)
## 
## Chain 1: Rejecting initial value:
## Chain 1:   Error evaluating the log probability at the initial value.
## Chain 1: Exception: beta_lpdf: Random variable is 1.79361, but must be less than or equal to 1  (in &#39;model291b653ea6cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22)
## 
## Chain 1: Rejecting initial value:
## Chain 1:   Error evaluating the log probability at the initial value.
## Chain 1: Exception: beta_lpdf: Random variable is -0.0589351, but must be &gt;= 0!  (in &#39;model291b653ea6cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22)
## 
## Chain 1: 
## Chain 1: Gradient evaluation took 1.2e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.021599 seconds (Warm-up)
## Chain 1:                0.016664 seconds (Sampling)
## Chain 1:                0.038263 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;4d8687896c7ea933ac28802adabeca38&#39; NOW (CHAIN 2).
## Chain 2: Rejecting initial value:
## Chain 2:   Error evaluating the log probability at the initial value.
## Chain 2: Exception: beta_lpdf: Random variable is -1.66405, but must be &gt;= 0!  (in &#39;model291b653ea6cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22)
## 
## Chain 2: 
## Chain 2: Gradient evaluation took 8e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.021193 seconds (Warm-up)
## Chain 2:                0.018562 seconds (Sampling)
## Chain 2:                0.039755 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;4d8687896c7ea933ac28802adabeca38&#39; NOW (CHAIN 3).
## Chain 3: Rejecting initial value:
## Chain 3:   Error evaluating the log probability at the initial value.
## Chain 3: Exception: beta_lpdf: Random variable is 1.015, but must be less than or equal to 1  (in &#39;model291b653ea6cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22)
## 
## Chain 3: 
## Chain 3: Gradient evaluation took 8e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.020799 seconds (Warm-up)
## Chain 3:                0.021982 seconds (Sampling)
## Chain 3:                0.042781 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;4d8687896c7ea933ac28802adabeca38&#39; NOW (CHAIN 4).
## Chain 4: Rejecting initial value:
## Chain 4:   Error evaluating the log probability at the initial value.
## Chain 4: Exception: beta_lpdf: Random variable is 1.22015, but must be less than or equal to 1  (in &#39;model291b653ea6cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22)
## 
## Chain 4: Rejecting initial value:
## Chain 4:   Error evaluating the log probability at the initial value.
## Chain 4: Exception: beta_lpdf: Random variable is 1.17242, but must be less than or equal to 1  (in &#39;model291b653ea6cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22)
## 
## Chain 4: Rejecting initial value:
## Chain 4:   Error evaluating the log probability at the initial value.
## Chain 4: Exception: beta_lpdf: Random variable is 1.25885, but must be less than or equal to 1  (in &#39;model291b653ea6cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22)
## 
## Chain 4: Rejecting initial value:
## Chain 4:   Error evaluating the log probability at the initial value.
## Chain 4: Exception: beta_lpdf: Random variable is -1.17079, but must be &gt;= 0!  (in &#39;model291b653ea6cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22)
## 
## Chain 4: Rejecting initial value:
## Chain 4:   Error evaluating the log probability at the initial value.
## Chain 4: Exception: beta_lpdf: Random variable is 1.90056, but must be less than or equal to 1  (in &#39;model291b653ea6cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22)
## 
## Chain 4: 
## Chain 4: Gradient evaluation took 7e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.021258 seconds (Warm-up)
## Chain 4:                0.016841 seconds (Sampling)
## Chain 4:                0.038099 seconds (Total)
## Chain 4:</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb10-1" data-line-number="1">samples.model2 &lt;-<span class="st"> </span><span class="kw">posterior_samples</span>(model2)</a>
<a class="sourceLine" id="cb10-2" data-line-number="2"></a>
<a class="sourceLine" id="cb10-3" data-line-number="3"></a>
<a class="sourceLine" id="cb10-4" data-line-number="4"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb10-5" data-line-number="5">Beta1 &lt;-<span class="st"> </span><span class="kw">rbeta</span>(<span class="dt">n=</span><span class="fl">1e5</span>, <span class="dt">shape1=</span><span class="dv">1</span>, <span class="dt">shape2=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb10-6" data-line-number="6">Beta2 &lt;-<span class="st"> </span><span class="kw">rbeta</span>(<span class="dt">n=</span><span class="fl">1e5</span>, <span class="dt">shape1=</span><span class="dv">10</span>, <span class="dt">shape2=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb10-7" data-line-number="7"><span class="kw">plot</span>(<span class="kw">density</span>(Beta1), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">4</span>), <span class="dt">main=</span><span class="st">&quot;Prior 1: Beta(1,1)&quot;</span>,</a>
<a class="sourceLine" id="cb10-8" data-line-number="8">     <span class="dt">xlab=</span><span class="kw">expression</span>(theta), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb10-9" data-line-number="9"><span class="kw">plot</span>(<span class="kw">density</span>(Beta2), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">4</span>), <span class="dt">main=</span><span class="st">&quot;Prior 2: Beta(10,10)&quot;</span>, </a>
<a class="sourceLine" id="cb10-10" data-line-number="10">     <span class="dt">xlab=</span><span class="kw">expression</span>(theta), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb10-11" data-line-number="11"><span class="kw">plot</span>(<span class="kw">density</span>(<span class="kw">inv_logit_scaled</span>(samples.model1<span class="op">$</span>b_Intercept)), </a>
<a class="sourceLine" id="cb10-12" data-line-number="12">     <span class="dt">main=</span><span class="st">&quot;Posterior with prior Beta(1,1)&quot;</span>, </a>
<a class="sourceLine" id="cb10-13" data-line-number="13">     <span class="dt">xlab=</span><span class="kw">expression</span>(theta), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="fl">0.48</span>,<span class="fl">0.60</span>))</a>
<a class="sourceLine" id="cb10-14" data-line-number="14"><span class="kw">plot</span>(<span class="kw">density</span>(<span class="kw">inv_logit_scaled</span>(samples.model2<span class="op">$</span>b_Intercept)), </a>
<a class="sourceLine" id="cb10-15" data-line-number="15">     <span class="dt">main=</span><span class="st">&quot;Posterior with prior Beta(10,10)&quot;</span>, </a>
<a class="sourceLine" id="cb10-16" data-line-number="16">     <span class="dt">xlab=</span><span class="kw">expression</span>(theta), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="fl">0.48</span>,<span class="fl">0.60</span>))</a></code></pre></div>
<p><img src="I2DA_files/figure-html/04-brms-models-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>A lot of effort has been done in the area of “Objective Baysian data analysis” in order to develop “uninformativ prior distributions”, because a lot of people feel uncomfortable using informative priors — seeing them as biased or unscientific. On the contrary, most people interpret results based on their prior experiences and in this light, prior information is just a way to quantify this <span class="citation">(Dobson and Barnett <a href="#ref-dobson2008">2008</a>)</span>. It is “just” important that the definition of the prior distributions make sense (at every stage) in the model.</p>
<div id="exursos-prior-predictive-distribution" class="section level4">
<h4><span class="header-section-number">4.3.2.1</span> Exursos: Prior predictive distribution</h4>
<p>So far we have seen that the  over parameters captures the initial assumptions or state of knowledge about the psychological variables they represent <span class="citation">(Lee and Wagenmakers <a href="#ref-leeWagen2014">2014</a>)</span>, in the above example this variable is <span class="math inline">\(\theta\)</span>.</p>
<p>Considering these initial assumptions in terms of prior distributions allow to make predictions about what data we would expect given the model and current state of knowledge. This distribution is called . It is a distribution over data, and gives the relative probability of different observable outcomes before any data have been seen <span class="citation">(Lee and Wagenmakers <a href="#ref-leeWagen2014">2014</a>)</span>.</p>
<p>For the coin flip model we consider a flat prior distribution: Beta(1,1). The prior predictive distribution would therefore look like:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1">## prior predictive distribution</a>
<a class="sourceLine" id="cb11-2" data-line-number="2">##coin flip example with prior: Beta(1,1) </a>
<a class="sourceLine" id="cb11-3" data-line-number="3"></a>
<a class="sourceLine" id="cb11-4" data-line-number="4">n &lt;-<span class="st"> </span><span class="fl">1e5</span></a>
<a class="sourceLine" id="cb11-5" data-line-number="5">p &lt;-<span class="st"> </span><span class="kw">rbeta</span>(<span class="dt">n=</span>n,<span class="dt">shape1 =</span> <span class="dv">1</span>,<span class="dt">shape2 =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb11-6" data-line-number="6">x &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="dt">n=</span>n,<span class="dt">size=</span><span class="dv">30</span>, <span class="dt">prob=</span>p)</a>
<a class="sourceLine" id="cb11-7" data-line-number="7"></a>
<a class="sourceLine" id="cb11-8" data-line-number="8"><span class="kw">hist</span>(x, <span class="dt">freq =</span> <span class="ot">FALSE</span>, <span class="dt">main=</span><span class="st">&quot;Prior predictive distribution&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.15</span>), <span class="dt">xlab=</span><span class="st">&quot;k - number of heads&quot;</span>)</a></code></pre></div>
<p><img src="I2DA_files/figure-html/unnamed-chunk-11-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="modeling" class="section level2">
<h2><span class="header-section-number">4.4</span> Modeling</h2>
<div id="introductory-example" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Introductory example</h3>
<p>As introductory example a coin flip experiment is considered. The question is if a particular coin is . In order to investigate this question a coin is flipped <span class="math inline">\(x\)</span> times (=trials) and the number of success (i.e. number of “head”) <span class="math inline">\(k\)</span> is recorded. This is repeated <span class="math inline">\(n\)</span> times (=observations).</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb12-1" data-line-number="1"><span class="co">#this code is just copy&amp;paste from above [it is included again for better comprehension]</span></a>
<a class="sourceLine" id="cb12-2" data-line-number="2"><span class="co">#simulate coin flip data set</span></a>
<a class="sourceLine" id="cb12-3" data-line-number="3">sample.space &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb12-4" data-line-number="4">theta &lt;-<span class="st"> </span><span class="fl">0.5</span>              <span class="co"># probability of a success (here: head)</span></a>
<a class="sourceLine" id="cb12-5" data-line-number="5">X &lt;-<span class="st"> </span><span class="dv">30</span>                   <span class="co"># number of trials in the experiment</span></a>
<a class="sourceLine" id="cb12-6" data-line-number="6">n &lt;-<span class="st"> </span><span class="dv">10</span>                   <span class="co"># number of observations</span></a>
<a class="sourceLine" id="cb12-7" data-line-number="7">k &lt;-<span class="st"> </span><span class="dv">0</span>                    <span class="co"># number of heads [initialization]</span></a>
<a class="sourceLine" id="cb12-8" data-line-number="8"></a>
<a class="sourceLine" id="cb12-9" data-line-number="9">## repeat experiment N-times</a>
<a class="sourceLine" id="cb12-10" data-line-number="10"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="st"> </span>n) {</a>
<a class="sourceLine" id="cb12-11" data-line-number="11">  k[i] &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">sample</span>(sample.space, <span class="dt">size =</span> X, <span class="dt">replace =</span> <span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb12-12" data-line-number="12">                     <span class="dt">prob =</span> <span class="kw">c</span>(theta, <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta)))</a>
<a class="sourceLine" id="cb12-13" data-line-number="13">}</a>
<a class="sourceLine" id="cb12-14" data-line-number="14"></a>
<a class="sourceLine" id="cb12-15" data-line-number="15">## show results in a tibble</a>
<a class="sourceLine" id="cb12-16" data-line-number="16">coin.flip &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="st">&quot;n&quot;</span> =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">1</span>, <span class="dt">to=</span>n, <span class="dt">by=</span><span class="dv">1</span>),</a>
<a class="sourceLine" id="cb12-17" data-line-number="17">                    <span class="st">&quot;k&quot;</span> =<span class="st"> </span>k,</a>
<a class="sourceLine" id="cb12-18" data-line-number="18">                    <span class="st">&quot;x&quot;</span> =<span class="st"> </span>X</a>
<a class="sourceLine" id="cb12-19" data-line-number="19">) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb12-20" data-line-number="20"><span class="st">  </span><span class="kw">print</span>()</a></code></pre></div>
<pre><code>## # A tibble: 10 x 3
##        n     k     x
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1     1    14    30
##  2     2    14    30
##  3     3    17    30
##  4     4    12    30
##  5     5    18    30
##  6     6    11    30
##  7     7    14    30
##  8     8    16    30
##  9     9    18    30
## 10    10    17    30</code></pre>
<p>The above table shows the observed outcome, but how the underlying probability of coming up  can be derived from that data set?</p>
</div>
<div id="steps-of-data-analysis" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Steps of Data Analysis</h3>
<p>The approach described here is based on <span class="citation">(McElreath <a href="#ref-mcelreath2015">2015</a>, <span class="citation">@kruschke2015</span>)</span>. Although the approach is introduced in a Bayesian context, it can be used as a general guideline (with some caveats):</p>
<ul>
<li>Identify the relevant variables according to the hypothesis (Measurement scales, predicted vs. predictor variables).</li>
<li>Define the descriptive model for the relevant variables.
<ul>
<li>likelihood distribution (distribution of each outcome variable that defines the plausibility of individual observations)</li>
<li>parameters (define and name all parameters of the model in order to relate the likelihood to the predictor variable(s))</li>
</ul></li>
<li>Bayesian context: Specify prior distribution(s).</li>
</ul>
<p>Further steps that will be subject of later chapters:</p>
<ul>
<li>Inference and interpretation of the results.</li>
<li>Model checking (Is the defined model adequate?)</li>
</ul>
<p><em>In the following we are interested in the question if a certain coin is biased.</em></p>
<p><strong>First step</strong> is to <em>identify the relevant variables</em>. For the coin flip experiment a coin is flipped <span class="math inline">\(n\)</span> times, whereby each observation consists of <span class="math inline">\(x\)</span> trials. The variable  <span class="math inline">\(Y\)</span> is dichotomous with the possible outcomes “head” and “tail”. For each observation the outcome is recorded: “0” for coming up tail and “1” for coming up head. The data are summarized for each observation. The variable <span class="math inline">\(k\)</span> indicates the number of heads coming up in <span class="math inline">\(x\)</span> trials.</p>
<p>In <strong>the second step</strong> a <em>descriptive model for the identified variables</em> has to be defined. An underlying probability <span class="math inline">\(\theta\)</span> is assumed, indicating the probability of heads coming up <span class="math inline">\(p(y=1)\)</span>. The probability that the outcome is head, given a value of parameter <span class="math inline">\(\theta\)</span>, is the value of <span class="math inline">\(\theta\)</span> <span class="citation">(Kruschke <a href="#ref-kruschke2015">2015</a>, 109)</span>. Formally, this can be written as
<span class="math display">\[p(y=1|\theta)=\theta\]</span>
As only two outcomes of <span class="math inline">\(Y\)</span> exists, the probability that the outcome is tail is the complementary probability <span class="math inline">\(1-\theta\)</span>. Both probabilities can be combined in one probability expression:
<span class="math display">\[Pr(Y|n,\theta)=\frac{n!}{y!(n-y)!}\theta^{y}(1-\theta)^{n-y}.\]</span>
This probability distribution is called the <strong>Binomial distribution</strong>. The fracture at the beginning indicates how many ordered sequences of <span class="math inline">\(n\)</span> outcomes a count <span class="math inline">\(y\)</span> have, therefore the important conceptional part is the latter one.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1"><span class="co"># Plot probability distribution: What would be the expected observed number </span></a>
<a class="sourceLine" id="cb14-2" data-line-number="2"><span class="co"># of &quot;head&quot; given the underlying prob. theta?</span></a>
<a class="sourceLine" id="cb14-3" data-line-number="3"> </a>
<a class="sourceLine" id="cb14-4" data-line-number="4"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb14-5" data-line-number="5">## theta=0.2</a>
<a class="sourceLine" id="cb14-6" data-line-number="6"><span class="kw">hist</span>(</a>
<a class="sourceLine" id="cb14-7" data-line-number="7">  <span class="kw">rbinom</span>(<span class="dt">n =</span> <span class="fl">1e6</span>, <span class="dt">size =</span> <span class="dv">30</span>, <span class="dt">prob =</span> <span class="fl">0.2</span>),</a>
<a class="sourceLine" id="cb14-8" data-line-number="8">  <span class="dt">xlab =</span> <span class="st">&quot;k&quot;</span>,</a>
<a class="sourceLine" id="cb14-9" data-line-number="9">  <span class="dt">main =</span> <span class="st">&quot;Binomial(1e6,0.2)&quot;</span>,</a>
<a class="sourceLine" id="cb14-10" data-line-number="10">  <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">30</span>),</a>
<a class="sourceLine" id="cb14-11" data-line-number="11">  <span class="dt">freq =</span> <span class="ot">FALSE</span></a>
<a class="sourceLine" id="cb14-12" data-line-number="12">)</a>
<a class="sourceLine" id="cb14-13" data-line-number="13">## theta=0.5</a>
<a class="sourceLine" id="cb14-14" data-line-number="14"><span class="kw">hist</span>(</a>
<a class="sourceLine" id="cb14-15" data-line-number="15">  <span class="kw">rbinom</span>(<span class="dt">n =</span> <span class="fl">1e6</span>, <span class="dt">size =</span> <span class="dv">30</span>, <span class="dt">prob =</span> <span class="fl">0.5</span>),</a>
<a class="sourceLine" id="cb14-16" data-line-number="16">  <span class="dt">xlab =</span> <span class="st">&quot;k&quot;</span>,</a>
<a class="sourceLine" id="cb14-17" data-line-number="17">  <span class="dt">main =</span> <span class="st">&quot;Binomial(1e6,0.5)&quot;</span>,</a>
<a class="sourceLine" id="cb14-18" data-line-number="18">  <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">30</span>),</a>
<a class="sourceLine" id="cb14-19" data-line-number="19">  <span class="dt">freq =</span> <span class="ot">FALSE</span></a>
<a class="sourceLine" id="cb14-20" data-line-number="20">)</a>
<a class="sourceLine" id="cb14-21" data-line-number="21">## theta=0.8</a>
<a class="sourceLine" id="cb14-22" data-line-number="22"><span class="kw">hist</span>(</a>
<a class="sourceLine" id="cb14-23" data-line-number="23">  <span class="kw">rbinom</span>(<span class="dt">n =</span> <span class="fl">1e6</span>, <span class="dt">size =</span> <span class="dv">30</span>, <span class="dt">prob =</span> <span class="fl">0.8</span>),</a>
<a class="sourceLine" id="cb14-24" data-line-number="24">  <span class="dt">xlab =</span> <span class="st">&quot;k&quot;</span>,</a>
<a class="sourceLine" id="cb14-25" data-line-number="25">  <span class="dt">main =</span> <span class="st">&quot;Binomial(1e6,0.8)&quot;</span>,</a>
<a class="sourceLine" id="cb14-26" data-line-number="26">  <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">30</span>),</a>
<a class="sourceLine" id="cb14-27" data-line-number="27">  <span class="dt">freq =</span> <span class="ot">FALSE</span></a>
<a class="sourceLine" id="cb14-28" data-line-number="28">)</a></code></pre></div>
<p><img src="I2DA_files/figure-html/unnamed-chunk-13-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>When the coin is flipped only once, then the probabilty can be written as:
<span class="math display">\[Pr(Y|\theta)=\theta^{y}(1-\theta)^{1-y}.\]</span>
This special variant of the Binomial distribution is the so-called <strong>Bernoulli distribution</strong>. To see the connection to the first considerations: When the outcome “head” is observed the equation reduces to <span class="math inline">\(Pr(y=1|\theta)=\theta\)</span> and when the outcome “tail” is observed the equation results in <span class="math inline">\(Pr(y=0|\theta)=(1-\theta).\)</span></p>
<p>Accordingly, for the introductory example it can be noted that the coin flip variable <span class="math inline">\(Y\)</span>  Binomial distribution. (Note: For Bayes’ rule the  is needed. Remember, the likelihood function treats <span class="math inline">\(\theta\)</span> as unknow and the data as known. This role of parameter is exchanged in a probability distribution.)</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1"><span class="co"># calculate the Liklihood function</span></a>
<a class="sourceLine" id="cb15-2" data-line-number="2">binomial.likelihood &lt;-<span class="st"> </span><span class="cf">function</span>(n, k, theta){theta<span class="op">^</span>k<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>theta)<span class="op">^</span>(n<span class="op">-</span>k)}</a>
<a class="sourceLine" id="cb15-3" data-line-number="3">theta &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">0</span>, <span class="dt">to=</span><span class="dv">1</span>, <span class="dt">by=</span><span class="fl">0.01</span>)</a>
<a class="sourceLine" id="cb15-4" data-line-number="4"></a>
<a class="sourceLine" id="cb15-5" data-line-number="5"><span class="co"># Plot likelihood: What would be the expected underlying prob. theta given </span></a>
<a class="sourceLine" id="cb15-6" data-line-number="6"><span class="co"># observed number of &quot;head&quot; in 100 observations?</span></a>
<a class="sourceLine" id="cb15-7" data-line-number="7"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb15-8" data-line-number="8"><span class="kw">plot</span>(theta, <span class="kw">binomial.likelihood</span>(<span class="dv">100</span>,<span class="dv">20</span>,theta), <span class="dt">xlab=</span><span class="kw">expression</span>(theta), </a>
<a class="sourceLine" id="cb15-9" data-line-number="9">     <span class="dt">ylab=</span><span class="st">&quot;likelihood&quot;</span>, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</a>
<a class="sourceLine" id="cb15-10" data-line-number="10"><span class="kw">plot</span>(theta,<span class="kw">binomial.likelihood</span>(<span class="dv">100</span>,<span class="dv">50</span>,theta), <span class="dt">xlab=</span><span class="kw">expression</span>(theta), </a>
<a class="sourceLine" id="cb15-11" data-line-number="11">     <span class="dt">ylab=</span><span class="st">&quot;likelihood&quot;</span>, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</a>
<a class="sourceLine" id="cb15-12" data-line-number="12"><span class="kw">plot</span>(theta,<span class="kw">binomial.likelihood</span>(<span class="dv">100</span>,<span class="dv">80</span>,theta), <span class="dt">xlab=</span><span class="kw">expression</span>(theta), </a>
<a class="sourceLine" id="cb15-13" data-line-number="13">     <span class="dt">ylab=</span><span class="st">&quot;likelihood&quot;</span>, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</a></code></pre></div>
<p><img src="I2DA_files/figure-html/unnamed-chunk-14-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><strong>The third step</strong> is solely a , that is the <em>incorporation of prior knowledge</em>. What do we believe about the coin bias <span class="math inline">\(\theta\)</span> before seeing the data? Assuming that no expectation about <span class="math inline">\(\theta\)</span> exists a priori, indicating that all values of <span class="math inline">\(\theta\)</span> between 0 and 1 are equally probable. This can be modeled by a uniform distribution or as already visualized as Beta distribution with parameters a=1 and b=1 (see following figure).</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb16-1" data-line-number="1"><span class="co"># Modelling prior knowledge &quot;ignorance&quot;</span></a>
<a class="sourceLine" id="cb16-2" data-line-number="2"></a>
<a class="sourceLine" id="cb16-3" data-line-number="3"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb16-4" data-line-number="4">## simulated a uniform(0,1) distribution</a>
<a class="sourceLine" id="cb16-5" data-line-number="5">rethinking<span class="op">::</span><span class="kw">dens</span>(<span class="kw">runif</span>(<span class="dt">n=</span><span class="fl">1e6</span>,<span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">1.5</span>), </a>
<a class="sourceLine" id="cb16-6" data-line-number="6">     <span class="dt">xlab=</span><span class="kw">expression</span>(theta), <span class="dt">main=</span><span class="st">&quot;Uniform(0,1)&quot;</span>)</a>
<a class="sourceLine" id="cb16-7" data-line-number="7">## simulates a beta(1,1) distribution</a>
<a class="sourceLine" id="cb16-8" data-line-number="8">rethinking<span class="op">::</span><span class="kw">dens</span>(<span class="kw">rbeta</span>(<span class="dt">n=</span><span class="fl">1e6</span>,<span class="dt">shape1=</span><span class="dv">1</span>,<span class="dt">shape2=</span><span class="dv">1</span>), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">1.5</span>), </a>
<a class="sourceLine" id="cb16-9" data-line-number="9">     <span class="dt">xlab=</span><span class="kw">expression</span>(theta), <span class="dt">main=</span><span class="st">&quot;Beta(1,1)&quot;</span>)</a></code></pre></div>
<p>So far, the coin flip model is define conceptionally. In the following some notational considerations have to be made.</p>
</div>
<div id="notation" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Notation</h3>
<div id="textual-notation" class="section level4">
<h4><span class="header-section-number">4.4.3.1</span> Textual notation</h4>
<p>In the textual notation, first the prior assumptions (if the Bayesian perspective is taken) are described. For the coin flip example this is:
<span class="math display">\[\theta \sim Beta(1,1).\]</span>
The symbol “<span class="math inline">\(\sim\)</span>” means “is distributed as”, thus, the above equation says before seeing the data all possible values of <span class="math inline">\(\theta\)</span> between 0 and 1 are assumed to be equally likely.</p>
<p>Subsequently, the descriptive model for the data has to be defined. As already described in the section above, it is assumed that the observed data (upcoming of heads <span class="math inline">\(k\)</span>) are distributed as Binomial distribution with given <span class="math inline">\(n\)</span> (number of observations) and unknown <span class="math inline">\(\theta\)</span>. This relation is denoted symbollically as
<span class="math display">\[k\sim Binomial(\theta|n).\]</span>
To summarize the current model (whereby the prior knowledge is only considered from a Bayesian perspective):
<span class="math display">\[\theta \sim Beta(1,1),\]</span>
<span class="math display">\[k\sim Binomial(\theta|n).\]</span></p>
</div>
<div id="graphical-notation" class="section level4">
<h4><span class="header-section-number">4.4.3.2</span> Graphical notation</h4>
<p>When models get very complex and incorporate many parameters it can be difficult to tease out all relations between the model components. In such a situation a graphical notation of a model might be helpful. In the following the convention described in Wagenmakers and Lee’s  (2014) is used: The graph structure is used to indicate dependencies between the variables, with children depending on their parents <span class="citation">(Lee and Wagenmakers <a href="#ref-leeWagen2014">2014</a>)</span>. General conventions:</p>
<ul>
<li>Nodes - problem relevant variables,</li>
<li>shaded nodes - observed variables,</li>
<li>unshaded nodes - unobserved variables,</li>
<li>circular nodes - continuous variables,</li>
<li>square nodes - discrete variables,</li>
<li>single line - stochastic dependency, and</li>
<li>double line - deterministic dependency.</li>
</ul>
<p>For the introductory example this indicates:</p>
<ul>
<li>relevant variables: number of trials (<span class="math inline">\(n\)</span>), number of success (<span class="math inline">\(k\)</span>) and probability for a success (<span class="math inline">\(\theta\)</span>),</li>
<li>observed variables: <span class="math inline">\(n\)</span> and <span class="math inline">\(k\)</span>,</li>
<li>unobserved variables: <span class="math inline">\(\theta\)</span>,</li>
<li>continuous variable: <span class="math inline">\(\theta\)</span>,</li>
<li>discrete variables: <span class="math inline">\(n\)</span> and <span class="math inline">\(k\)</span>.</li>
</ul>
<p>In the next step the dependencies have to be determined:</p>
<p>The number of success <span class="math inline">\(k\)</span> depends on the probability of a success <span class="math inline">\(\theta\)</span> as well as on the number of trials <span class="math inline">\(n\)</span>.</p>
<p>Finally, the graphical structure together with the textual notation can be represented:</p>
<div class="figure">
<img src="https://github.com/michael-franke/intro-data-analysis/blob/master/chapters/images/Graph_Binom.png" alt="Graphical notation Beta-Binomial Modell - One group" />
<p class="caption">Graphical notation Beta-Binomial Modell - One group</p>
</div>
</div>
</div>
<div id="an-outlook-hierarchical-models" class="section level3">
<h3><span class="header-section-number">4.4.4</span> An outlook: Hierarchical models</h3>
<p>Often data can be considered as part of an overall structure. Single observations can be modelled belonging into different groups. These groups in turn are part of a superordinate group etc. Such information are presented in a model in form of a hierarchy.</p>
<p>For example, consider again the coin flip experiment. The outcome of  is influenced by the probability <span class="math inline">\(\theta\)</span>. Further, <span class="math inline">\(\theta\)</span> is assumed to be distributed as Beta(1,1). Remember that the parameter a and b of a Beta-distribution can be considered in this context as: <span class="math inline">\(a=\)</span>number of heads and <span class="math inline">\(b=\)</span> number of tails, consequently, <span class="math inline">\(n=a+b\)</span>.</p>
<div id="reparameterization-of-a-beta-distribution" class="section level4">
<h4><span class="header-section-number">4.4.4.1</span> Reparameterization of a Beta distribution</h4>
<p>Probability distributions can be described by their  and  (or dispersion). The  of a Beta distribution is defined as:</p>
<p><span class="math inline">\(\omega=\frac{a-1}{a+b-2}\)</span>,</p>
<p>and the concentration as:</p>
<p><span class="math inline">\(\kappa=a+b.\)</span></p>
<p>The nice thing is, that the definition of the  as well as of the  consists solely of the parameters a and b. Therefore, it is possible to re-express the parameters of a Beta density in terms of <span class="math inline">\(\omega\)</span> and <span class="math inline">\(\kappa\)</span>, such that:</p>
<p><span class="math display">\[Beta(a,b)=Beta\left(\omega(\kappa -2)+1, (1-\omega)(\kappa -2)+1\right).\]</span></p>
<p><em>Why this is useful? And what is its value in connection with hierarchical modeling?</em></p>
<p>Return back to the coin flip experiment. So far, the parameters of the prior on <span class="math inline">\(\theta\)</span> are fixed: <span class="math inline">\(a=1\)</span> and <span class="math inline">\(b=1\)</span>. Assume that we get further information: The manifacturing process of the coins has a bias near <span class="math inline">\(\omega\)</span> (example taken from <span class="citation">(Kruschke <a href="#ref-kruschke2015">2015</a>)</span>). But how to incorporate this additional knowledge in the model?</p>
<p>At this point, the hierarchy and the reparameterization come into play. Hierarchy because a further assumption is placed on top of the existing model and reparameterization, because we want to express the prior in terms of the mode <span class="math inline">\(\omega\)</span>.</p>
<p>Such that the model can be assumed as follows:</p>
<div class="figure">
<img src="https://github.com/michael-franke/intro-data-analysis/blob/master/chapters/images/binom-onelevel.png" alt="Graphical notation hierarchical Beta-Binomial Modell - One group" />
<p class="caption">Graphical notation hierarchical Beta-Binomial Modell - One group</p>
</div>
<p>Now, the parameters of the hyperpriors (Gamma and Beta) are fixed, but they can be treated as parameters as well..as such hierarchical models can be created with any degree of complexity:</p>
<div class="figure">
<img src="https://github.com/michael-franke/intro-data-analysis/blob/master/chapters/images/hierarchical-model.png" alt="Graphical notation hierarchical Beta-Binomial Modell - One group" />
<p class="caption">Graphical notation hierarchical Beta-Binomial Modell - One group</p>
</div>
</div>
</div>
</div>
<div id="further-examples" class="section level2">
<h2><span class="header-section-number">4.5</span> Further examples</h2>
<div id="difference-between-two-groups" class="section level3">
<h3><span class="header-section-number">4.5.1</span> Difference between two groups</h3>
<p>In the introductory example we asked for the underlying probility <span class="math inline">\(\theta\)</span> of a single coin that was flipped repeatedley.
Consider now, that a second coin <span class="math inline">\(y_2\)</span> is introduced. One question that arises might be for example: </p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1"><span class="co">#simulate flipps of two coins </span></a>
<a class="sourceLine" id="cb17-2" data-line-number="2">sample.space &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb17-3" data-line-number="3">##First coin:</a>
<a class="sourceLine" id="cb17-4" data-line-number="4">theta1 &lt;-<span class="st"> </span><span class="fl">0.5</span>              <span class="co"># probability of a success (here: head)</span></a>
<a class="sourceLine" id="cb17-5" data-line-number="5">X1 &lt;-<span class="st"> </span><span class="dv">30</span>                   <span class="co"># number of trials in the experiment</span></a>
<a class="sourceLine" id="cb17-6" data-line-number="6">n1 &lt;-<span class="st"> </span><span class="dv">100</span>                  <span class="co"># number of observations</span></a>
<a class="sourceLine" id="cb17-7" data-line-number="7">k1 &lt;-<span class="st"> </span><span class="dv">0</span>                    <span class="co"># number of heads [initialization]</span></a>
<a class="sourceLine" id="cb17-8" data-line-number="8"></a>
<a class="sourceLine" id="cb17-9" data-line-number="9"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="st"> </span>n1) {</a>
<a class="sourceLine" id="cb17-10" data-line-number="10">  k1[i] &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">sample</span>(sample.space, <span class="dt">size =</span> X1, <span class="dt">replace =</span> <span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb17-11" data-line-number="11">                     <span class="dt">prob =</span> <span class="kw">c</span>(theta1, <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta1)))</a>
<a class="sourceLine" id="cb17-12" data-line-number="12">}</a>
<a class="sourceLine" id="cb17-13" data-line-number="13">##Second coin:</a>
<a class="sourceLine" id="cb17-14" data-line-number="14">theta2 &lt;-<span class="st"> </span><span class="fl">0.7</span>              <span class="co"># probability of a success (here: head)</span></a>
<a class="sourceLine" id="cb17-15" data-line-number="15">X2 &lt;-<span class="st"> </span><span class="dv">30</span>                   <span class="co"># number of trials in the experiment</span></a>
<a class="sourceLine" id="cb17-16" data-line-number="16">n2 &lt;-<span class="st"> </span><span class="dv">100</span>                  <span class="co"># number of observations</span></a>
<a class="sourceLine" id="cb17-17" data-line-number="17">k2 &lt;-<span class="st"> </span><span class="dv">0</span>                    <span class="co"># number of heads [initialization]</span></a>
<a class="sourceLine" id="cb17-18" data-line-number="18"></a>
<a class="sourceLine" id="cb17-19" data-line-number="19">## repeat experiment N-times</a>
<a class="sourceLine" id="cb17-20" data-line-number="20"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="st"> </span>n2) {</a>
<a class="sourceLine" id="cb17-21" data-line-number="21">  k2[i] &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">sample</span>(sample.space, <span class="dt">size =</span> X2, <span class="dt">replace =</span> <span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb17-22" data-line-number="22">                     <span class="dt">prob =</span> <span class="kw">c</span>(theta2, <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta2)))</a>
<a class="sourceLine" id="cb17-23" data-line-number="23">}</a>
<a class="sourceLine" id="cb17-24" data-line-number="24"></a>
<a class="sourceLine" id="cb17-25" data-line-number="25">## show results in a tibble</a>
<a class="sourceLine" id="cb17-26" data-line-number="26">coin.flip2 &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="st">&quot;coin&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="kw">replicate</span>(n1,<span class="st">&quot;coin1&quot;</span>), <span class="kw">replicate</span>(n2,<span class="st">&quot;coin2&quot;</span>)),</a>
<a class="sourceLine" id="cb17-27" data-line-number="27">                     <span class="st">&quot;n&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">1</span>, <span class="dt">to=</span>n1, <span class="dt">by=</span><span class="dv">1</span>), <span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">1</span>, <span class="dt">to=</span>n2, <span class="dt">by=</span><span class="dv">1</span>)),</a>
<a class="sourceLine" id="cb17-28" data-line-number="28">                     <span class="st">&quot;k&quot;</span> =<span class="st"> </span><span class="kw">c</span>(k1,k2),</a>
<a class="sourceLine" id="cb17-29" data-line-number="29">                     <span class="st">&quot;x&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="kw">replicate</span>(n1,X1), <span class="kw">replicate</span>(n2,X2))</a>
<a class="sourceLine" id="cb17-30" data-line-number="30">) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb17-31" data-line-number="31"><span class="st">  </span><span class="kw">print</span>()</a></code></pre></div>
<pre><code>## # A tibble: 200 x 4
##    coin      n     k     x
##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 coin1     1    15    30
##  2 coin1     2    12    30
##  3 coin1     3    17    30
##  4 coin1     4    15    30
##  5 coin1     5    14    30
##  6 coin1     6    15    30
##  7 coin1     7    13    30
##  8 coin1     8    16    30
##  9 coin1     9    14    30
## 10 coin1    10    14    30
## # … with 190 more rows</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" data-line-number="1"><span class="co">#Plotting the observed results</span></a>
<a class="sourceLine" id="cb19-2" data-line-number="2"><span class="kw">ggplot</span>(<span class="dt">data=</span>coin.flip2,<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x=</span>k, <span class="dt">fill=</span>coin ))<span class="op">+</span></a>
<a class="sourceLine" id="cb19-3" data-line-number="3"><span class="st">          </span><span class="kw">geom_histogram</span>()</a></code></pre></div>
<p><img src="I2DA_files/figure-html/unnamed-chunk-17-1.png" width="672" style="display: block; margin: auto;" /></p>
<div id="conceptual-steps-for-modeling" class="section level4">
<h4><span class="header-section-number">4.5.1.1</span> Conceptual steps for modeling</h4>
<p>We suppose that the underlying probabilities of the two coins correspond to <em>different</em> latent variables <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span>.</p>
<p><strong>First step</strong> is again the <em>identification of the relevant variables</em> according to the research question. As already indicated for the “one coin” example we have:</p>
<ul>
<li>the observed number of heads <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span> (for each coin, respectively), which is influenced by</li>
<li>the number of observations <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> and by</li>
<li>the underlying probabilities <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span>.</li>
</ul>
<p>Furthermore, from a conceptional perspective, we are interested in the <em>difference between the coin biases</em>. Therefore a further variable will be introduced <span class="math inline">\(\delta\)</span>, defined by:
<span class="math display">\[\delta =\theta_1 - \theta_2.\]</span></p>
<p>The <em>distributional assumptions</em>, according to the <strong>second and third step</strong>, can be adopted from the “one coin” example, such that the graphical notation (including the textual notation) can be denoted as follows:</p>
</div>
<div id="notation-beta-binomial-model---two-groups" class="section level4">
<h4><span class="header-section-number">4.5.1.2</span> Notation Beta-Binomial Model - Two Groups</h4>
<div class="figure">
<img src="https://github.com/michael-franke/intro-data-analysis/blob/master/chapters/images/Graph_Factorial.png" alt="Graphical notation Beta-Binomial Modell - Two groups" />
<p class="caption">Graphical notation Beta-Binomial Modell - Two groups</p>
</div>
</div>
</div>
<div id="simple-linear-regression-with-one-metric-predictor" class="section level3">
<h3><span class="header-section-number">4.5.2</span> Simple linear regression with one metric predictor</h3>
<p>The following example originates from a data set in which speed of cars and the distance taken to stop was recorded. It is a simple data set good for introducing the basic ideas for simple linear regression.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb20-1" data-line-number="1"><span class="co">#The &quot;cars&quot; data set</span></a>
<a class="sourceLine" id="cb20-2" data-line-number="2"><span class="kw">data</span>(cars)</a>
<a class="sourceLine" id="cb20-3" data-line-number="3"><span class="co">#take a look at the variables included in the data set</span></a>
<a class="sourceLine" id="cb20-4" data-line-number="4"><span class="kw">str</span>(cars)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    50 obs. of  2 variables:
##  $ speed: num  4 4 7 7 8 9 10 10 10 11 ...
##  $ dist : num  2 10 4 22 16 10 18 26 34 17 ...</code></pre>
<p>One possible question could be how much the stopping distance increases when the speed of a car increases.</p>
<div id="conceptual-steps-for-modeling-1" class="section level4">
<h4><span class="header-section-number">4.5.2.1</span> Conceptual steps for modeling</h4>
<p>First step is to <strong>identify the relevant variables</strong>. In this case these are “speed” measured in mph and “distance” measured in ft, thus, both variables are metric variables. As distance will be predicted from speed. The  is “distance” and the  is “speed”. A scatter plot can visualize a possible relationship between both variables.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" data-line-number="1"><span class="kw">plot</span>(<span class="dt">x=</span>cars<span class="op">$</span>speed,<span class="dt">y=</span>cars<span class="op">$</span>dist, <span class="dt">type=</span><span class="st">&quot;p&quot;</span>, <span class="dt">main=</span><span class="st">&quot;scatter plot of cars data set&quot;</span>, </a>
<a class="sourceLine" id="cb22-2" data-line-number="2">     <span class="dt">ylab=</span><span class="st">&quot;distance in ft&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;speed in mph&quot;</span>)</a></code></pre></div>
<p><img src="I2DA_files/figure-html/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Next step is to define a <strong>descriptive model of the data</strong>. According to the scatter plot it is not too absurd to think that distance might be proportional to speed. Therefore, a linear relationship between both variables can be assumed, where speed is used i order to predict distance. But how can the distribution of the predicted variable “distance” be described? The following plot shows in blue the density of the actual distance values.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb23-1" data-line-number="1"><span class="co">#density of distance values in blue </span></a>
<a class="sourceLine" id="cb23-2" data-line-number="2"><span class="co">#(in black simulation of a normal distribution)</span></a>
<a class="sourceLine" id="cb23-3" data-line-number="3"><span class="kw">dens</span>(cars<span class="op">$</span>dist, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">norm.comp =</span> <span class="ot">TRUE</span>, <span class="dt">main=</span><span class="st">&quot;Distribution of distance&quot;</span>, </a>
<a class="sourceLine" id="cb23-4" data-line-number="4">     <span class="dt">xlab=</span><span class="st">&quot;distance in ft&quot;</span>)</a></code></pre></div>
<p>Although the distribution of “distance” values is not identical to the corresponding normal distribution, it can be assumed that the values follow a <em>normal distribution</em>. The underlying consideration is that the distance values <span class="math inline">\(y_i\)</span> are distributed randomly according to a normal distribution around the predicted value <span class="math inline">\(\hat{y}\)</span> and with a standard deviation denoted with <span class="math inline">\(\sigma\)</span>. This can be denoted as:
<span class="math display">\[y_i\sim Normal(\mu, \sigma).\]</span>
The index <span class="math inline">\(i\)</span> indicates each element (i.e. car) of the list <span class="math inline">\(y\)</span>, which in turn is the list of distances.</p>
<p>In the third step, a Bayesian perspective is taken the <strong>prior knowlege</strong> (before seeing the data) has to be defined. The parameters of the current model are the predicted value <span class="math inline">\(\mu\)</span> and the standard deviation <span class="math inline">\(\sigma\)</span>. For the parameter <span class="math inline">\(\mu\)</span> a normal distribution can be assumend with parameters that reflect the estimated values from the sample.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1"><span class="co">#descriptive statistics from the sample</span></a>
<a class="sourceLine" id="cb24-2" data-line-number="2"><span class="kw">tibble</span>(<span class="dt">variables=</span><span class="kw">c</span>(<span class="st">&quot;speed&quot;</span>, <span class="st">&quot;distance&quot;</span>),</a>
<a class="sourceLine" id="cb24-3" data-line-number="3">       <span class="dt">mean=</span><span class="kw">c</span>(<span class="kw">mean</span>(cars<span class="op">$</span>speed),<span class="kw">mean</span>(cars<span class="op">$</span>dist)),</a>
<a class="sourceLine" id="cb24-4" data-line-number="4">       <span class="dt">sigma =</span> <span class="kw">c</span>(<span class="kw">sd</span>(cars<span class="op">$</span>speed), <span class="kw">sd</span>(cars<span class="op">$</span>dist)))</a></code></pre></div>
<pre><code>## # A tibble: 2 x 3
##   variables  mean sigma
##   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;
## 1 speed      15.4  5.29
## 2 distance   43.0 25.8</code></pre>
<p><span class="math display">\[\mu\sim Normal(43,26)\]</span></p>
<p>For the standard deviation <span class="math inline">\(\sigma\)</span> a uniform distribution is assumed:
<span class="math display">\[\sigma\sim Uniform(0,40)\]</span></p>
</div>
<div id="excursos-identically-and-independently-distributed" class="section level4">
<h4><span class="header-section-number">4.5.2.2</span> Excursos: Identically and independently distributed ()</h4>
<p>The short model description <span class="math inline">\(y_i\sim Normal(\mu, \sigma)\)</span> incorporates often already an assumption about the distribution of distance-values: They are . Often the abbreviation  can be found for this assumption:
<span class="math display">\[y_i\overset{\text{iid}}{\sim} Normal(\mu, \sigma).\]</span></p>
The abbreviation  indicates that each value <span class="math inline">\(y_i\)</span> has the same probability function, independent of the other <span class="math inline">\(y\)</span> values and using the same parameters <span class="citation">(McElreath <a href="#ref-mcelreath2015">2015</a>)</span>. This is hardly ever true (why hierarchical modeling is very attractive). For example, thinking about the cars in the current example data set. Some cars may be of different types or even the same type but different batches. But the question is: Is this underlying dependency relevant for the model? If yes, this information has to be added in the model (e.g. in form of a hierachical model). Janyes states it as follows:

<p><span class="citation">(Jaynes <a href="#ref-jaynes2003">2003</a>, 339)</span>. But if one do not know any relevant underlying relationships the most conservative distribution to use is . Note, that the stated assumptions define how the model represents a problem and not how the world should be understood. For example, there might exist underlying correlations but on the overall distribution there influence tends towards zero. In such cases it remains usefull to assume iid <span class="citation">(McElreath <a href="#ref-mcelreath2015">2015</a>)</span>.</p>
</div>
</div>
<div id="notation-simple-regression-model" class="section level3">
<h3><span class="header-section-number">4.5.3</span> Notation Simple Regression model</h3>
<div class="figure">
<img src="https://github.com/michael-franke/intro-data-analysis/blob/master/chapters/images/Graph_SimpReg.png" alt="Graphical notation Simplre Regression model" />
<p class="caption">Graphical notation Simplre Regression model</p>
</div>
</div>
</div>
<div id="further-elaboration-on-modeling-in-anticipation-of-the-topic-estimation" class="section level2">
<h2><span class="header-section-number">4.6</span> Further elaboration on modeling (in anticipation of the topic “estimation”)</h2>
<div id="beta-binomial-model---one-group-revisited" class="section level3">
<h3><span class="header-section-number">4.6.1</span> Beta-Binomial model - one group (revisited)</h3>
<p>Sofar the existence of the underlying probability <span class="math inline">\(\theta\)</span> for observing head as outcome of a coin flip has been discussed. But the estimation of <span class="math inline">\(\theta\)</span> has been ignored until yet. Although “estimation” will be topic of next chapter, it is helpful at this point to discuss the introduced models further. In order to estimate <span class="math inline">\(\theta\)</span>  are needed. When it comes to estimation exactly this/these parameter(s) will be the result(s), therefore is is important to see already the connection to the models that were developed in this chapter.</p>
<p>For the coin flip example the value of interest is the underlying probability, thus, only one parameter is needed:<span class="math inline">\(\beta_0\)</span>. (Note: Latin letters are used when we refer to the sample, greek letters are used when we refer to the population.)</p>
<p>How is <span class="math inline">\(\beta_0\)</span> linked to the latent variable <span class="math inline">\(\theta\)</span>?</p>
<p>Considering for example the simplest case: a  (see next plot left side). The problem which arises at this point is that <span class="math inline">\(\theta\)</span> represents a probability, and is therefore bounded to the range 0-1 (grey shaded area).</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="co">#Different relationships between the parameter and expected value </span></a>
<a class="sourceLine" id="cb26-2" data-line-number="2">x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span><span class="op">-</span><span class="dv">4</span>, <span class="dt">to=</span><span class="dv">4</span>, <span class="dt">length.out =</span> <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb26-3" data-line-number="3">y &lt;-<span class="st"> </span>x                 <span class="co">#linear relationship</span></a>
<a class="sourceLine" id="cb26-4" data-line-number="4">y.log &lt;-<span class="st"> </span><span class="kw">inv_logit_scaled</span>(x)   <span class="co">#logistic relationship</span></a>
<a class="sourceLine" id="cb26-5" data-line-number="5"></a>
<a class="sourceLine" id="cb26-6" data-line-number="6"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))   <span class="co">#set both plot beside each other</span></a>
<a class="sourceLine" id="cb26-7" data-line-number="7"><span class="kw">plot</span>(x, y, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>, <span class="dt">ylab=</span><span class="kw">expression</span>(theta), <span class="dt">xlab=</span><span class="kw">expression</span>(beta[<span class="dv">0</span>]))</a>
<a class="sourceLine" id="cb26-8" data-line-number="8"><span class="kw">rect</span>(<span class="op">-</span><span class="dv">5</span>, <span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dt">col =</span> <span class="kw">rgb</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">4</span>), <span class="dt">border =</span> <span class="ot">NA</span>)</a>
<a class="sourceLine" id="cb26-9" data-line-number="9"><span class="kw">plot</span>(x, y.log, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>, <span class="dt">ylab=</span><span class="kw">expression</span>(logit<span class="op">~</span>(theta)), <span class="dt">xlab=</span><span class="kw">expression</span>(beta[<span class="dv">0</span>]))</a>
<a class="sourceLine" id="cb26-10" data-line-number="10"><span class="kw">rect</span>(<span class="op">-</span><span class="dv">5</span>, <span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dt">col =</span> <span class="kw">rgb</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">4</span>), <span class="dt">border =</span> <span class="ot">NA</span>)</a></code></pre></div>
<p><img src="I2DA_files/figure-html/unnamed-chunk-22-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>A mathematical transformation is needed such that the parameter <span class="math inline">\(\beta_0\)</span> can take any value while <span class="math inline">\(\theta\)</span> is bounded to the range 0-1. One transformation that offers exactly this possibility is the  (see aboth plot right side)</p>
<p><span class="math display">\[logit(\theta) = \beta_0.\]</span>
As the underlying assumption maps the parameter to the latent variable <span class="math inline">\(\theta\)</span> (and not the other way around) from a conceptional point of view the  is more appropriate, which is the  in this case:</p>
<p><span class="math display">\[\theta = logistic(\beta_0).\]</span>
It is defined as
<span class="math display">\[\theta=\frac{exp(\beta_0)}{1+exp(\beta_0)}.\]</span></p>
<p>Both expression,  and  link achieve mathematically the same result but it is conceptionally just a different matter of emphasis <span class="citation">(Kruschke <a href="#ref-kruschke2015">2015</a>)</span>.</p>
<div id="notation-of-beta-binomial-model---one-group-revisited" class="section level4">
<h4><span class="header-section-number">4.6.1.1</span> Notation of beta-binomial model - one group (revisited)</h4>
<p>The current descriptive model incorporates the idea that parameter <span class="math inline">\(\beta_0\)</span> is estimated from the given sample. It defines the latent variable <span class="math inline">\(\theta\)</span>. The parameter is maped to <span class="math inline">\(\theta\)</span> by a logistic link function. The underlying probability <span class="math inline">\(\theta\)</span> designates the observed number of upcoming heads. The number of upcoming heads in turn, is assumed to be distributed as Binomial distribution.</p>
<!-- [here graphical notation including textual notation... but I don't know how to write it properly down... what is the right order????] -->
</div>
</div>
<div id="beta-binomial-model---two-groups-revisited" class="section level3">
<h3><span class="header-section-number">4.6.2</span> Beta-Binomial model - two groups (revisited)</h3>
<p>In the above model for two coins the latent variable <span class="math inline">\(\delta\)</span> was already introduced. It is defined by the difference between the underlying probabilities <span class="math inline">\(\theta_1-\theta_2\)</span>.
Which parameters should be used in order to estimate the difference between both groups? As we will see, it turns out that the same mathematical form can be used, as one would use for simple linear regression:</p>
<p><span class="math display">\[\theta_j=\beta_0+\beta_1*X_{Group_j},\]</span>
<span class="math display">\[\textrm{with } X_{Group_j}=\begin{cases}
0, \textrm{if coin 2,}\\
1, \textrm{if coin 1.}
\end{cases}\]</span></p>
<p>Considering <em>coin 2</em>, the above equation would result in
<span class="math display">\[\theta_2 = \beta_0,\]</span>
which is the  and indicates the proportion of head coming up for coin 2.</p>
<p>Considering by contrast coin 1, then the equation would result in:
<span class="math display">\[\theta_1 = \beta_0 + \beta_1.\]</span>
The proportion of coming up head for coin 1 has to be calcuated by summing up the  <span class="math inline">\(\beta_0\)</span> and the  <span class="math inline">\(\beta_1\)</span>.</p>
<p>Taken togehter: <em>What is the interpretation of the slope <span class="math inline">\(\beta_1\)</span>?</em>
The difference <span class="math inline">\(\delta=\theta_1-\theta2\)</span> is
<span class="math display">\[\theta_1 - \theta_2 = (\beta_0+\beta_1)-\beta_0=\beta_1=\delta,\]</span>
the slope <span class="math inline">\(\beta_1\)</span>, thus, we can see that this parameterization enables us to estimate the difference between two groups. When in comes to estimation and interpretation the results will be the intercept <span class="math inline">\(b_0\)</span> and the slope <span class="math inline">\(b_1\)</span>.</p>
</div>
<div id="simple-linear-regression-model-revisited" class="section level3">
<h3><span class="header-section-number">4.6.3</span> Simple linear regression model (revisited)</h3>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-dobson2008">
<p>Dobson, Annette J., and Adrian G. Barnett. 2008. <em>An Introduction to Generalized Linear Models</em>. Chapman; Hall/CRC.</p>
</div>
<div id="ref-jaynes2003">
<p>Jaynes, Edwin T. 2003. <em>Probability Theory: The Logic of Science</em>. Cambridge university press.</p>
</div>
<div id="ref-kline2013">
<p>Kline, Rex B. 2013. <em>Beyond Significance Testing: Statistics Reform in the Behavioral Sciences</em>. American Psychological Association.</p>
</div>
<div id="ref-kruschke2015">
<p>Kruschke, John. 2015. <em>Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan</em>. Academic Press.</p>
</div>
<div id="ref-leeWagen2014">
<p>Lee, Michael D., and Eric-Jan Wagenmakers. 2014. <em>Bayesian cognitive modeling: A practical course</em>. Cambridge university press.</p>
</div>
<div id="ref-mcelreath2015">
<p>McElreath, Richard. 2015. <em>Statistical Rethinking: A Bayesian Course with Examples in R and Stan</em>. Chapman; Hall/CRC.</p>
</div>
<div id="ref-vallverdu2015">
<p>Vallverdú, Jordi. 2016. <em>Bayesians Versus Frequentists: A Philosophical Debate on Statistical Reasoning</em>. Heidelberg: Springer.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="basics-of-probability-theory.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="inference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["I2DA.epub", "I2DA.pdf"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
