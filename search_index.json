[
["index.html", "Introduction to Data Analysis Preface", " Introduction to Data Analysis last rendered at: 2019-10-11 17:01:31 Preface } } The book introduces key concepts of data analysis from a frequentist and a Bayesian tradition. It uses R to handle, plot and analyze data. It relies on simulation to illustrate selected statistical concepts. "],
["testing-showcasing.html", "0.1 Testing / Showcasing", " 0.1 Testing / Showcasing Don’t pay too much attention to what is written here. 0.1.1 Quotes This is a quote: Tidy datasets […] have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. — Wickham (2014) 0.1.2 Infobox At certain stages, possibly at the end of chapters or after important concepts, we might want to use a special infobox (see .infobox in styles.css) to summarise it or give food for thought. Like this: A horse walks into a bar and orders a pint. The barkeep says “you’re in here pretty often. Think you might be an alcoholic?”, to which the horse says “I don’t think I am.”, and vanishes from existence. See, the joke is about Descartes’ famous philosophy of ’I think therefore, I am&quot;, but to explain that part before the rest of the joke would be to put Descartes before the horse. We can have boxes with different icons for different purposes, using {block, type='see styles.css'}: This might be useful for excercises or general questions. Do you like it? Sometimes there are things that are really important, like exceptions to general rules. This box might be appropriate for these. To use markdown/latex inside the boxes, use {block2, ...}: “More research needs to be done” as an infobox. And to use entire code blocks within such boxes, we need to make use of raw html (like &lt;div class=&quot;beware&quot;&gt;): This is just a test. # a roll of 5d6 might look like this: sample(1:6, 5, replace = TRUE) ## [1] 6 2 1 6 3 Hear, hear! It’s 2:30 PM and all is good! 0.1.3 Plots This is a plot of quite a famous dataset (Anscombe 1973): tibble( grp = rep(c(&quot;I&quot;, &quot;II&quot;, &quot;III&quot;, &quot;IV&quot;), each = 11), x = c(anscombe$x1, anscombe$x2, anscombe$x3, anscombe$x4), y = c(anscombe$y1, anscombe$y2, anscombe$y3, anscombe$y4) ) %&gt;% ggplot(aes(x, y)) + geom_smooth(method = lm, se = F, color = &quot;black&quot;) + geom_point(color = project_colors[3], size = 2) + scale_y_continuous(breaks = scales::pretty_breaks()) + scale_x_continuous(breaks = scales::pretty_breaks()) + labs(title = &quot;Anscombe&#39;s Quartet&quot;, x = NULL, y = NULL, subtitle = bquote(y == 0.5 * x + 3 ~ (R^2 %~~% .667) ~ &quot;for all datasets&quot;)) + facet_wrap(~grp, ncol = 2, scales = &quot;free_x&quot;) + theme(strip.background = element_rect(fill = &quot;#f2f2f2&quot;, colour = &quot;white&quot;)) (#fig:ch-preface-a_plot_example_01)Anscombe’s Quartet: four different data sets all of which receive the same correlation score. Here’s a plot showcasing our custom palellet for discrete factors: ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + geom_point(size=3) Figure 0.1: A figure to test the contrast of the first three project colors 0.1.4 Shiny Apps Embedding Shiny Apps is as easy as pie! knitr::include_app(&quot;https://marauderpixie.shinyapps.io/dist_chisq/&quot;, height = &quot;800px&quot;) References "],
["general-introduction.html", "1 General Introduction", " 1 General Introduction This chapter lays out the learning goals of this course (Section 1.1 and describes how these goals are to be achieved (Section 1.2). Section 1.3 introduces a doal approach to statistical inference. Section 1.4 details which technical tools and methods are covered here, and which are not. "],
["intro-learning-goals.html", "1.1 Learning goals", " 1.1 Learning goals "],
["intro-course-structure.html", "1.2 Course structure", " 1.2 Course structure "],
["intro-dual-approach.html", "1.3 Dual approach to statistical inference", " 1.3 Dual approach to statistical inference "],
["intro-tools-methods.html", "1.4 Tools and methods covered (and not covered) here", " 1.4 Tools and methods covered (and not covered) here "],
["basics-of-r.html", "2 Basics of R", " 2 Basics of R Content covered: TBF Learning goal: familiarize yourself with the basic of R and the tidyverse "],
["general-remarks.html", "2.1 General remarks", " 2.1 General remarks R is specialized programming language for statistics/data science base R vs tidyverse packages, CRAN, github resources: official (base) R intro: An Introduction to R book on DS with tidyverse: R for Data Science (R4DS) "],
["installation-and-use.html", "2.2 Installation and use", " 2.2 Installation and use installation of R itself RStudio how to install packages which packages to install for this course tidyverse, brms, … "],
["first-steps.html", "2.3 First steps", " 2.3 First steps scripts and sessions interpreted language using RStudio calculations, variable assignments good naming conventions comments sectioning in RStudio good practices for literate programming everything is vector/matrix based declare vector with c() indexing starting from 1 matrices in “column-major mode” (unlike NumPy!) -&gt; first index moves fastest (m = matrix(1:6, ncol = 3)) ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 matrix indexing starts with row m[1,] ## [1] 1 3 5 vectors, therefore, are column vectors: x = c(1,0,1) m %*% x ## [,1] ## [1,] 6 ## [2,] 8 getting help / information about function / vignettes "],
["data-types.html", "2.4 Data types", " 2.4 Data types 2.4.1 Numbers standard number precision is double 2.4.2 Booleans 2.4.3 Special values NA, NaN, Inf, … what else? 2.4.4 Characters (= strings) strings are called characters typeof(&quot;huhu&quot;) ## [1] &quot;character&quot; paste, grep, cat, sprintf… what else? vector of characters chr.vector = c(&quot;huhu&quot;, &quot;hello&quot;, &quot;huhu&quot;, &quot;ciao&quot;) chr.vector ## [1] &quot;huhu&quot; &quot;hello&quot; &quot;huhu&quot; &quot;ciao&quot; 2.4.5 Factors factors track levels factor(chr.vector) ## [1] huhu hello huhu ciao ## Levels: ciao hello huhu ordered factors arrange their levels factor(chr.vector, ordered = T, levels = c(&quot;huhu&quot;, &quot;ciao&quot;, &quot;hello&quot;)) ## [1] huhu hello huhu ciao ## Levels: huhu &lt; ciao &lt; hello 2.4.6 Lists, data frames &amp; tibbles lists are key-value pairs my_list = list(dudu = 1, chacha = c(&quot;huhu&quot;, &quot;ciao&quot;)) data frames as lists of same-length vectors exp_data = data.frame( trial = 1:5, condition = factor(c(&quot;C1&quot;, &quot;C2&quot;, &quot;C1&quot;, &quot;C3&quot;, &quot;C2&quot;), ordered = T), response = c(121, 133, 119, 102, 156)) exp_data ## trial condition response ## 1 1 C1 121 ## 2 2 C2 133 ## 3 3 C1 119 ## 4 4 C3 102 ## 5 5 C2 156 access colums exp_data$condition ## [1] C1 C2 C1 C3 C2 ## Levels: C1 &lt; C2 &lt; C3 access rows exp_data[3,] ## trial condition response ## 3 3 C1 119 tibbles are data frames in the tidyverse as_tibble(exp_data) ## # A tibble: 5 x 3 ## trial condition response ## &lt;int&gt; &lt;ord&gt; &lt;dbl&gt; ## 1 1 C1 121 ## 2 2 C2 133 ## 3 3 C1 119 ## 4 4 C3 102 ## 5 5 C2 156 tibbles do but data frames don’t allow for dynamic construction: some differences my_tibble = tibble(x = 1:10, y = x^2) ## dynamic construction possible my_dataframe = data.frame(x = 1:10, y = x^2) ## ERROR :/ "],
["functions.html", "2.5 Functions", " 2.5 Functions named and anonymous "],
["loops-and-maps.html", "2.6 Loops and maps", " 2.6 Loops and maps for loops functional programming with apply purrr the philosophy of “passing functions to functions” piping "],
["data.html", "3 Data", " 3 Data learning goal: how to arrange, summarize and visualize (aspects of data) to address a question of interest (“hypothesis-driven data poking”) different kinds of data summary statistics data wrangling data plotting "],
["kinds-of-data.html", "3.1 Kinds of data", " 3.1 Kinds of data "],
["summary-statistics.html", "3.2 Summary statistics", " 3.2 Summary statistics 3.2.1 Exploring numerical data in a vector The first part of the Mental Chronometry experiment is a simple reaction time task. # read a data set of reaction times (as a tibble) RTs &lt;- read_csv(&quot;https://tinyurl.com/y4jo8mox&quot;) # check its content glimpse(RTs) ## Observations: 3,980 ## Variables: 1 ## $ RT &lt;dbl&gt; 5.605802, 5.318120, 5.361292, 5.669881, 5.370638, 5.327876, 5… "],
["data-wrangling.html", "3.3 Data Wrangling", " 3.3 Data Wrangling "],
["data-plotting.html", "3.4 Data Plotting", " 3.4 Data Plotting Numerical summaries of complex data always incur information loss. Still lossy, but less so is visualization. Any serious data analysis should start with a process in which the analyst becomes intimate with the data at hand. Visualization is an integral part of data-intimacy. Consider a famous dataset available in R (Anscombe 1973): glimpse(anscombe %&gt;% as_tibble) ## Observations: 11 ## Variables: 8 ## $ x1 &lt;dbl&gt; 10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5 ## $ x2 &lt;dbl&gt; 10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5 ## $ x3 &lt;dbl&gt; 10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5 ## $ x4 &lt;dbl&gt; 8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8 ## $ y1 &lt;dbl&gt; 8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, … ## $ y2 &lt;dbl&gt; 9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4… ## $ y3 &lt;dbl&gt; 7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, … ## $ y4 &lt;dbl&gt; 6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, … There are four pairs of \\(x\\) and \\(y\\) coordinates. Unfortunately, these are stored in long format with two pieces of information buried inside of the column name: \\(x3\\) contains the information that this column contains the \\(x\\) coordinates for the 3rd pair. This is rather untidy. But, after the last section on data wrangling, we can tidy up quickly: tidy_anscombe = anscombe %&gt;% as_tibble %&gt;% pivot_longer( everything(), names_to = c(&quot;.value&quot;, &quot;grp&quot;), names_pattern = &quot;(.)(.)&quot; ) %&gt;% mutate(grp = paste0(&quot;Group &quot;, grp)) tidy_anscombe ## # A tibble: 44 x 3 ## grp x y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Group 1 10 8.04 ## 2 Group 2 10 9.14 ## 3 Group 3 10 7.46 ## 4 Group 4 8 6.58 ## 5 Group 1 8 6.95 ## 6 Group 2 8 8.14 ## 7 Group 3 8 6.77 ## 8 Group 4 8 5.76 ## 9 Group 1 13 7.58 ## 10 Group 2 13 8.74 ## # … with 34 more rows Here are some summary statistics for each of the four pairs: tidy_anscombe %&gt;% group_by(grp) %&gt;% summarise( mean_x = mean(x), mean_y = mean(y), min_x = min(x), min_y = min(y), max_x = max(x), max_y = max(y), r_squared = cor(x,y)^2 ) ## # A tibble: 4 x 8 ## grp mean_x mean_y min_x min_y max_x max_y r_squared ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Group 1 9 7.50 4 4.26 14 10.8 0.667 ## 2 Group 2 9 7.50 4 3.1 14 9.26 0.666 ## 3 Group 3 9 7.5 4 5.39 14 12.7 0.666 ## 4 Group 4 9 7.50 8 5.25 19 12.5 0.667 These numeric indicators suggest that each pair of \\(x\\) and \\(y\\) values is very similar. Only the ranges seem to differ. A brilliant example of how misleading numeric statistics can be, as compared to a plot of the data: tidy_anscombe %&gt;% ggplot(aes(x, y)) + geom_smooth(method = lm, se = F, color = &quot;black&quot;) + geom_point(color = project_colors[3], size = 2) + scale_y_continuous(breaks = scales::pretty_breaks()) + scale_x_continuous(breaks = scales::pretty_breaks()) + labs(title = &quot;Anscombe&#39;s Quartet&quot;, x = NULL, y = NULL, subtitle = bquote(y == 0.5 * x + 3 ~ (R^2 %~~% .667) ~ &quot;for all datasets&quot;)) + facet_wrap(~grp, ncol = 2, scales = &quot;free_x&quot;) + theme(strip.background = element_rect(fill = &quot;#f2f2f2&quot;, colour = &quot;white&quot;)) Figure 3.1: Anscombe’s Quartet: four different data sets all of which receive the same correlation score. References "],
["basics-of-probability-theory.html", "4 Basics of Probability Theory", " 4 Basics of Probability Theory Content covered: axiomatic definition, interpretation, joint distributions, marginalization, conditional probability &amp; Bayes rule, random variables: discrete and continuous, expected values &amp; variance Learning goal: get comfortable with basic notions of probability theory "],
["probability.html", "4.1 Probability", " 4.1 Probability 4.1.1 Outcomes, events, observations We are interested in the space \\(\\Omega\\) of all elementary outcome \\(\\omega_1, \\omega_2, \\dots\\) of a process or event whose execution is (partially) random or unknown. Elementary outcomes are mutually exclusive. The set \\(\\Omega\\) exhausts all possibilities.1 Example. The set of elementary outcomes of a single coin flip is \\(\\Omega_{\\text{coin flip}} = \\left \\{ \\text{heads}, \\text{tails} \\right \\}\\). The elementary outcomes of tossing a six-sided die is \\(\\Omega_{\\text{standard die}} = \\{\\) ⚀, ⚁, ⚂, ⚃, ⚄, ⚅ \\(\\}\\).2 An event \\(A\\) is a subset of \\(\\Omega\\). Think of an event as a (possibly partial) observation. We might observe, for instance, not the full outcome of tossing a die, but only that there is a dot in the middle. This would correspond to the event \\(A = \\{\\) ⚀, ⚂, ⚄ \\(\\}\\), i.e., observing an odd numbered outcome. The trivial observation \\(A = \\Omega\\) and the impossible observation \\(A = \\emptyset\\) are counted as events, too. The latter is included for technical reasons. For any two events \\(A, B \\subseteq \\Omega\\), standard set operations correspond to logical connectives in the usual way. For example, the conjunction \\(A \\cap B\\) is the observation of both \\(A\\) and \\(B\\); the disjunction \\(A \\cup B\\) is the observation that it is either \\(A\\) or \\(B\\); the negation of \\(A\\), \\(\\overline{A} = \\left \\{ \\omega \\in \\Omega \\mid \\omega \\not \\in A \\right \\}\\), is the observation that it is not \\(A\\). 4.1.2 Probability distributions A probability distribution \\(P\\) over \\(\\Omega\\) is a function \\(P \\ \\colon \\ \\mathfrak{P}(\\Omega) \\rightarrow \\mathbb{R}\\) that assigns to all events \\(A \\subseteq \\Omega\\) a real number (from the unit interval, see A1 below), such that the following (so-called Kolmogorov axioms) are satisfied: A1. \\(0 \\le P(A) \\le 1\\) A2. \\(P(\\Omega) = 1\\) A3. $P(A_1 A_2 A_3 ) = P(A_1) + P(A_2) + P(A_3) + $ whenever \\(A_1, A_2, A_3, \\dots\\) are mutually exclusive3 Occasionally we encounter notation \\(P \\in \\Delta(\\Omega)\\) to express that \\(P\\) is a probability distribution over \\(\\Omega\\). (E.g., in physics, theoretical economics or game theory. Less so in psychology or statistics.) If \\(\\omega \\in \\Omega\\) is an elementary event, we often write \\(P(\\omega)\\) as a shorthand for \\(P(\\left \\{ \\omega \\right \\})\\). In fact, if \\(\\Omega\\) is finite, it suffices to assign probabilities to elementary outcomes. A number of rules follow immediately from of this definition (prove this!): C1. \\(P(\\emptyset) = 0\\) C2. \\(P(\\overline{A}) = 1 - P(A)\\) C3. \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) for any \\(A, B \\subseteq \\Omega\\) 4.1.3 Interpretations of probability It is reasonably safe, at least preliminarily, to think of probability, as defined above, as a handy mathematical primitive which is useful for certain applications. There are at least three ways of thinking about where this primitive probability might come from, roughly paraphrasable like so: Frequentist: Probabilities are generalizations of intuitions/facts about frequencies of events in repeated executions of a random event. Subjectivist: Probabilities are subjective beliefs by a rational agent who is uncertain about the outcome of a random event. Realist: Probabilities are a property of an intrinsically random world. 4.1.4 Urns and frequencies Think of an urn as a container which contains a number of \\(N &gt; 1\\) balls. Balls can be of different color. For example, let us suppose that our urn has \\(k &gt; 0\\) black balls and \\(N-k\\) white balls. (There is at least one black and one white ball.) For a single random draw from our urn we have: \\(\\Omega_{\\text{our urn}} = \\left \\{ \\text{white}, \\text{black} \\right \\}\\). If we imagine an infinite sequence of single draws from our urn, putting whichever ball we drew back in after every draw, the limiting proportion with which we draw a black ball is \\(\\frac{k}{N}\\). (If in doubt, execute this experiment. By hand or by computer.) This statement about frequency is what motivates saying that the probability of drawing a black ball on a single trial is (or should be4) \\(P(\\text{black}) = \\frac{k}{N}\\). For simplicity of exposure, we gloss over subtleties arising when dealing with infinite sets \\(\\Omega\\). We make up for this when we define probability density functions for continuous random variables, which is all the uncountable infinity that we will usually be concerned with in applied statistics.↩ Think of \\(\\Omega\\) as a partition of the space of all possible ways in which the world could be, where we lump together into one partition cell all ways in which the world could be that are equivalent regarding those aspects of reality that we are interested in. We do not care whether the coin lands in the mud or in the sand. It only matters whether it came up heads or tails. Each elementary event can be realized in myriad ways. \\(\\Omega\\) is our, the modellers’, first crude simplification of nature, abstracting away aspects we currently do not care about.↩ A3 is the axiom of countable additivity. Finite additivity may be enough for finite or countable sets \\(\\Omega\\), but infinite additivity is necessary for full generality in the uncountable case.↩ If probabilities are subjective beliefs, a rational agent is, in a sense, normatively required to assign exactly this probability.↩ "],
["structured-events-marginal-distributions.html", "4.2 Structured events &amp; marginal distributions", " 4.2 Structured events &amp; marginal distributions 4.2.1 Probability table for a flip-&amp;-draw scenario Suppose we have two urns. Both have \\(N=10\\) balls. Urn 1 has \\(k_1=2\\) black and \\(N-k_1 = 8\\) white balls. Urn 2 has \\(k_2=4\\) black and \\(N-k_2=6\\) white balls. We sometimes draw from urn 1, sometimes from urn 2. To decide, we flip a fair coin. If it comes up heads, we draw from urn 1; if it comes up tails, we draw from urn 2. An elementary outcome of this two-step process of flip-&amp;-draw is a pair \\(\\langle \\text{outcome-flip}, \\text{outcome-draw} \\rangle\\). The set of all possible such outcomes is: \\[\\Omega_{\\text{flip-&amp;-draw}} = \\left \\{ \\langle \\text{heads}, \\text{black} \\rangle, \\langle \\text{heads}, \\text{white} \\rangle, \\langle \\text{tails}, \\text{black} \\rangle, \\langle \\text{tails}, \\text{white} \\rangle \\right \\}\\,.\\] The probability of event \\(\\langle \\text{heads}, \\text{black} \\rangle\\) is given by multiplying the probability of seeing “heads” on the first flip, which happens with probability \\(0.5\\), and then drawing a black ball, which happens with probability \\(0.2\\), so that \\(P(\\langle \\text{heads}, \\text{black} \\rangle) = 0.5 \\times 0.2 = 0.1\\). The probability distribution over \\(\\Omega_{\\text{flip-draw}}\\) is consequently as in Table 4.1. (If in doubt, start flipping &amp; drawing and count your outcomes.) Table 4.1: Joint probability table for the flip-&amp;-draw scenario black white heads \\(0.5 \\times 0.2 = 0.1\\) \\(0.5 \\times 0.4 = 0.2\\) tails \\(0.5 \\times 0.8 = 0.4\\) \\(0.5 \\times 0.6 = 0.3\\) 4.2.2 Structured events and joint-probability distributions Table 4.1 is an example of a joint probability distribution over a structured event space, which here has two dimensions. Since our space of outcomes is the Cartesian product of two simpler outcome spaces, namely \\(\\Omega_{flip-\\&amp;-draw} = \\Omega_{flip} \\times \\Omega_{draw}\\),5 we can use notation \\(P(\\text{heads}, \\text{black})\\) as shorthand for \\(P(\\langle \\text{heads}, \\text{black} \\rangle)\\). More generally, if \\(\\Omega = \\Omega_1 \\times \\dots \\Omega_n\\), we can think of \\(P \\in \\Delta(\\Omega)\\) as a joint probability distribution over \\(n\\) subspaces. 4.2.3 Marginalization If \\(P\\) is a joint-probability distribution over event space \\(\\Omega = \\Omega_1 \\times \\dots \\Omega_n\\), the marginal distribution over subspace \\(\\Omega_i\\), \\(1 \\le i \\le n\\) is the probability distribution that assigns to all \\(A_i \\subseteq \\Omega_i\\) the probability:6 \\[ P(A_i) = \\sum_{A_1 \\subseteq \\Omega_{1}, \\dots , A_{i-1} \\subseteq \\Omega_{i-1}, A_{i+1} \\subseteq \\Omega_{i+1}, \\dots, A_n \\subseteq \\Omega_n} P(A_1, \\dots, A_{i-1}, A_{i}, A_{i+1}, \\dots A_n) \\] For example, the marginal distribution over coin flips derivable from the joint probability distribution in Table 4.1 gives \\(P(\\text{heads}) = P(\\text{tails}) = 0.5\\), since the sum of each row is exactly \\(0.5\\). The marginal distribution over flips derivable from Table 4.1 has \\(P(\\text{black}) = 0.3\\) and \\(P(\\text{black}) = 0.7\\).7 With \\(\\Omega_{\\text{flip}} = \\left \\{ \\text{heads}, \\text{tails} \\right \\}\\) and \\(\\Omega_{\\text{draw}} = \\left \\{ \\text{black}, \\text{white} \\right \\}\\).↩ This notation, using \\(\\sum\\), assumes that subspaces are countable. In other cases, a parallel definition with integrals can be used.↩ The term ``marginal distribution’’ derives from such probability tables, where traditionally the sum of each row/column was written in the margins.↩ "],
["conditional-probability.html", "4.3 Conditional probability", " 4.3 Conditional probability Fix probability distribution \\(P \\in \\Delta(\\Omega)\\) and events \\(A,B \\subseteq \\Omega\\). The conditional probability of \\(A\\) given \\(B\\), written as \\(P(A \\mid B)\\), gives the probability of \\(A\\) on the assumption that \\(B\\) is true.8 It is defined like so: \\[P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\\] Conditional probabilities are only defined when \\(P(B) &gt; 0\\).9 Example. If a dice is unbiased, each of its six faces has equal probability to come up after a toss. The probability of event \\(B = \\{\\) ⚀, ⚂, ⚄ \\(\\}\\) that the tossed number is odd has probability \\(P(B) = \\frac{1}{2}\\). The probability of event \\(A = \\{\\) ⚂, ⚃, ⚄, ⚅ \\(\\}\\) that the tossed number is bigger than two is \\(P(A) = \\frac{2}{3}\\). The probability that the tossed number is bigger than two odd is \\(P(A \\cap B) = P(\\{\\) ⚂, ⚄ \\(\\}) = \\frac{1}{3}\\). The conditional probability of tossing a number that is bigger than two, when we know that the toss is even, is \\(P(A \\mid B) = \\frac{1 / 3}{1 / 2} = \\frac{2}{3}\\). Algorithmically, conditional probability first rules out all events in which \\(B\\) is not true and then simply renormalizes the probabilities assigned to the remaining events in such a way that the relative probabilities of surviving events remains unchanged. Given this, another way of interpreting conditional probability is that \\(P(A \\mid B)\\) is what a rational agent believe about \\(A\\) after observing that \\(B\\) is in fact true and nothing more. The agent rules out, possibly hypothetically, that \\(B\\) is false, but otherwise does not change opinion about the relative probabilities of anything that is compatible with \\(B\\). 4.3.1 Bayes rule Looking back at the joint-probability distribution in Table 4.1, the conditional probability \\(P(\\text{black} \\mid \\text{heads})\\) of drawing a black ball, given that the initial coin flip showed heads, can be calculated as follows: \\[ P(\\text{black} \\mid \\text{heads}) = \\frac{P(\\text{black} , \\text{heads})}{P(\\text{heads})} = \\frac{0.1}{0.5} = 0.2 \\] This calculation, however, is quite spurious. We knew that already from the way the flip-&amp;-draw scenario was set up. After flipping heads, we draw from urn 1, which has \\(k=2\\) out of \\(N=10\\) black balls, so clearly: if the flip is heads, then the probability of a black ball is \\(0.2\\). Indeed, in a step-wise random generation process like the flip-&amp;-draw scenario, some conditional probabilities are very clear, and sometimes given by definition. These are, usually, the conditional probabilities that define how the process unfolds forward in time, so to speak. Bayes rule is a way of expressing, in a manner of speaking, conditional probabilities in terms of the “reversed” conditional probabilities: \\[P(B \\mid A) = \\frac{P(A \\mid B) \\times P(B)}{P(A)}\\] Bayes rule is straightforward corollary of the definition of conditional probabilities, according to which \\(P(A \\cap B) = P(A \\mid B) \\times P(B)\\), so that: \\[ P(B \\mid A) = \\frac{P(A \\cap B)}{P(A)} = \\frac{P(A \\mid B) \\cdot P(B)}{P(A)} \\] Bayes rule allows for reasoning backwards from observed causes to likely underlying effects. When we have a feed-forward model of how unobservable effects probabilistically constrain observable outcomes, Bayes rule allows us to draw inferences about latent/unobservable variables based on the observation of their downstream effects. Consider yet again the flip-&amp;-draw scenario. But now assume that Jones flipped the coin and drew a ball. We see that it is black. What is the probability that it was drawn from urn 1, equivalently, that the coin landed heads? It is not \\(P(\\text{heads}) = 0.5\\), the so-called prior probability of the coin landing heads. It is a conditional probability, also called the posterior probability,10 namely \\(P(\\text{heads} \\mid \\text{black})\\), but one that is not as easy and straightforward to write down as the reverse \\(P(\\text{black} \\mid \\text{heads})\\) of which we said above that it is an almost trivial part of the set up of the flip-&amp;-draw scenario. It is here that Bayes rule has its purpose: \\[ P(\\text{heads} \\mid \\text{black}) = \\frac{P(\\text{black} \\mid \\text{heads}) \\times P(\\text{heads})}{P(\\text{black})} = \\frac{0.2 \\times 0.5}{0.3} = \\frac{1}{3} \\] This result is quite intuitive. Drawing a black ball from urn 2 (i.e., after seeing tails) is twice as likely as drawing a black ball from urn 1 (i.e., after seeing heads). Consequently, after seeing a black ball drawn, with equal probabilities of heads and tails, the probability that the coin landed tails is also twice as large as that it landed heads. We also verbalize this as “the conditional probability of \\(A\\) conditioned on \\(B\\).”↩ Updating with events which have probability zero entails far more severe adjustments of the underlying belief system than just ruling out information hitherto considered possible. Formal systems that capture such belief revision are studied in formal epistemology Halpern (2003) .↩ The terms prior and posterior make sense when we think about an agent’s belief state before (prior to) and after (posterior to) an observation.↩ "],
["random-variables.html", "4.4 Random variables", " 4.4 Random variables We have so far define a probability distribution as a function that assigns a probability to each subset of the space \\(\\Omega\\) of elementary outcomes. A special case occurs when we are interested in a space of numeric outcomes. A random variable is a function \\(X \\ \\colon \\ \\Omega \\rightarrow \\mathbb{R}\\) that assigns to each elementary outcome a numerical value. Example. For a single flip of a coin we have \\(\\Omega_{\\text{coin flip}} = \\left \\{ \\text{heads}, \\text{tails} \\right \\}\\). A usual way of mapping this onto numerical outcomes is to define \\(X_{\\text{coin flip}} \\ \\colon \\ \\text{heads} \\mapsto 1; \\text{tails} \\mapsto 0\\). Less trivially, consider flipping a coin two times. Elementary outcomes should be individuated by the outcome of the first flip and the outcome of the second flip, so that we get: \\[ \\Omega_{\\text{two flips}} = \\left \\{ \\langle \\text{heads}, \\text{heads} \\rangle, \\langle \\text{heads}, \\text{tails} \\rangle, \\langle \\text{tails}, \\text{heads} \\rangle, \\langle \\text{tails}, \\text{tails} \\rangle \\right \\} \\] Consider the random variable \\(X_{\\text{two flips}}\\) that counts the total number of heads. Crucially, \\(X_{\\text{two flips}}(\\langle \\text{heads}, \\text{tails} \\rangle) = 1 = X_{\\text{two flips}}(\\langle \\text{tails}, \\text{heads} \\rangle)\\). We assign the same numerical value to different elementary outcomes. 4.4.1 Notation &amp; terminology Traditionally random variables are represented by capital letters, like \\(X\\). Variables for the numeric values they take on are written as small letters, like \\(x\\). We write \\(P(X = x)\\) as a shorthand for the probability \\(P(\\left \\{ \\omega \\in \\Omega \\mid X(\\omega) = 2 \\right \\})\\) that an event occurs that is mapped onto \\(x\\) by random variable \\(X\\). For example, if our coin is fair, then \\(P(X_{\\text{two flips}} = x) = 0.5\\) for \\(x=1\\) and \\(0.25\\) otherwise. Similarly, we can also write \\(P(X \\le x)\\) for the probability of observing an event that \\(X\\) maps to a number not bigger than \\(x\\). If the range of \\(X\\) is countable, we say that \\(X\\) is discrete. For ease of exposition, we may say that if the range of \\(X\\) is an interval of real numbers, \\(X\\) is called continuous. 4.4.2 Cumulative distribution functions, mass &amp; density For a discrete random variable \\(X\\), the cumulative distribution function \\(F_X\\) associated with \\(X\\) is defined as: \\[ F_X(x) = P(X \\le x) = \\sum_{x&#39; \\in \\left \\{ \\text{Rng}(X) \\mid x&#39; \\le x \\right \\}} P(X = x) \\] The probability mass function \\(f_x\\) associated with \\(X\\) is defined as: \\[ f_X(x) = P(X = x) \\] Example. Suppose we flip a coin with a bias of \\(\\theta\\) \\(n\\) times. What is the probability that we will see heads \\(k\\) times? If we map the outcome of heads to 1 and tails to 0, this probability is given by the , as follows: \\[ \\text{Binom}(K = k ; n, \\theta) = \\binom{n}{k} \\, \\theta^{k} \\, (1-\\theta)^{n-k} \\] Here \\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\) is the binomial coefficient. It gives the number of possibilities of drawing an unordered set with \\(k\\) elements from a set with a total of \\(n\\) elements. Figure 4.1 gives an example of the Binomial distribution, concretely its probability mass function, for two values of the coin’s bias, \\(\\theta = 0.25\\) or \\(\\theta = 0.5\\), when flipping the coin \\(n=24\\) times. Figure 4.2 gives the corresponding cumulative distributions. Figure 4.1: Examples of the Binomial distribution. The \\(y\\)-axis give the probability of seeing \\(k\\) heads when flipping a coin \\(n=24\\) times with a bias of either \\(\\theta = 0.25\\) or \\(\\theta = 0.5\\). Figure 4.2: Examples of the cumulative distribution of the Binomial. The \\(y\\)-axis gives the probability of seeing \\(k\\) or less outcomes of heads when flipping a coin \\(n=24\\) times with a bias of either \\(\\theta = 0.25\\) or \\(\\theta = 0.5\\). For a continuous random variable \\(X\\), the probability \\(P(X = x)\\) will usually be zero: it is virtually impossible that we will see precisely the value \\(x\\) realized in a random event that can realize uncountably many numerical values of \\(X\\). However, \\(P(X \\le x)\\) does take workable values and so we define the \\(F_X\\) associated with \\(X\\) as: \\[ F_X(x) = P(X \\le x) \\] Instead of a probability mass function, we derive a probability density function from the cumulative function as: \\[ f_X(x) = F&#39;(x) \\] A probability density function can take values greater than one, unlike a probability mass function. Example. The Gaussian or Normal distribution characterizes many natural distributions of measurements which are symmetrically spread around a central tendency. It is defined as: \\[ \\mathcal{N}(X = x ; \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\sigma^2 \\pi}} \\exp \\left ( - \\frac{(x-\\mu)^2}{2 \\sigma^2} \\right) \\] where parameter \\(\\mu\\) is the mean, the central tendency, and parameter \\(\\sigma\\) is the standard deviation. Figure 4.3 gives examples of the probability density function of two normal distributions. Figure 4.4 gives the corresponding cumulative distribution functions. Figure 4.3: Examples of the Normal distribution. In both cases \\(\\mu = 0\\), once with \\(\\sigma = 1\\) and once with \\(\\sigma = 4\\) Figure 4.4: Examples of the cumulative normal distribution corresponding to the previous probability density functions. "],
["expected-value-variance.html", "4.5 Expected value &amp; variance", " 4.5 Expected value &amp; variance The expected value of a random variable \\(X\\) is a measure of central tendency. It tells us, like the name suggests, which average value of \\(X\\) we can expect when repeatedly sampling from \\(X\\). If \\(X\\) is continuous, the expected value is: \\[ \\mathbb{E}_X = \\sum_{x} x \\times f_X(x) \\] If \\(X\\) is continuous, it is: \\[ \\mathbb{E}_X = \\int x \\times f_X(x) \\ \\text{d}x \\] The expected value is also frequently called the mean. The variance of a random variable \\(X\\) is a measure of how much likely values of \\(X\\) are spread or clustered around the expected value. If \\(X\\) is discrete, the variance is: \\[ \\text{Var}(X) = \\sum_x (\\mathbb{E}_X - x)^2 \\times f_X(x) \\] If \\(X\\) is continuous, it is: \\[ \\text{Var}(X) = \\int (\\mathbb{E}_X - x)^2 \\times f_X(x) \\ \\text{d}x \\] Example. If we flip a coin with bias \\(\\theta = 0.25\\) a total of \\(n=24\\), we expect on average to see \\(n \\times\\theta = 24 \\times 0.25 = 6\\) outcomes showing heads. The variance is \\(n \\times\\theta \\times(1-\\theta) = 24 \\times 0.25 \\times 0.75 = \\frac{24 \\times 3}{16} = \\frac{18}{4} = 4.5\\). The expected value of a normal distribution is just its mean \\(\\mu\\) and its variance is \\(\\sigma^2\\). "],
["two-approaches-to-statistical-inference.html", "5 Two approaches to statistical inference ", " 5 Two approaches to statistical inference "],
["overview.html", "5.1 Overview", " 5.1 Overview In the first section we introduce the based on a short revision of the debate between and . After a short overview we formalize the conceptual ideas and the components of the presented model. We finish by discussing further example models. "],
["two-notions-of-probability-revisited.html", "5.2 Two notions of probability (revisited)", " 5.2 Two notions of probability (revisited) What are probabilities? Two viewpoints can be distinguished: Probabilities exist “outside in the world” or they are “subjective beliefs” (Kruschke 2015). Although both notions imply different approaches how to deal with probabilities, the mathematical properties are quite similar (Kruschke 2015). 5.2.1 Frequentism — Probabilities as properties of the world When thinking about probabilities as existing “outside in the world” one has to consider the random process that produces the observed data. A Frequentist imagine this random process beeing repeated a large number of times so that the probability of an outcome is the number of observed outcomes devided by the total number of observations \\(n\\) (Dobson and Barnett 2008). Frequentism is an approach that searches for relative frequencies in a large number of trials (Vallverdú 2016). The parameter \\(\\theta\\), the probability of interest, is the value of the relative frequency when \\(n\\) becomes infinitely large. Consequently, the parameter \\(\\theta\\) can be estimated from the observed data, \\(\\hat{\\theta}\\), by maximizing the likelihood function (see section “Likelihood, Prior &amp; Posterior”) (Dobson and Barnett 2008). 5.2.2 Bayesianism — Probabilities as subjective beliefs Another notion of probabilities is to think of them as “beliefs” inside one’s mind. The core of Bayesian methods is Bayes’ theorem which describes how prior belief is combined with observed data: \\[P(H|Data)=\\frac{P(Data|H)*P(H)}{P(Data)}\\] \\[Posterior=\\frac{Likelihood*Prior}{Marginal\\textrm{ } Likelihood},\\] The job of the “marginal Likelihood” in the denominator is to standardize the posterior and thus to ensure it sums up to one (integrates to one). Therefore, the key lesson of Bayes’ theorem is (McElreath 2015): \\[Posterior \\propto Likelihood * Prior\\] A parameter in Bayesian methods is conceptualized as a random variable with its own distribution (the posterior) that summarizes the current state of knowledge. The expected value of the posterior is the best guess about the true value of the parameter and its variability reflects the amount of uncertainty (Kline 2013). In Frequentist statistics, a parameter is seen as a constant that should be estimated with sample statistics (Kline 2013). References "],
["models.html", "6 Models ", " 6 Models "],
["likelihood-prior-posterior.html", "6.1 Likelihood, Prior, &amp; Posterior", " 6.1 Likelihood, Prior, &amp; Posterior Before the topic of is introduced, some conceptual notions are neccessary. 6.1.1 Probability density function vs. Likelihood function As already know from the “probability”-lecture, for a coin flip the probability of each outcome can be described with the , as there exist two discrete outcomes (head or tail) and a constant probability \\(\\theta\\): \\[p([X=x]|\\theta)=\\theta^{[x]}(1-\\theta)^{(1-[x])}\\] where \\(\\theta\\) is the probability of “head” for the coin flip; the bracket [ ] indicates that the particular parameter is treated as unknown. With the formula above the probability of \\(\\theta\\) is treated as “known”. Accordingly, the distribution of possible outcomes can be derived. But, as already seen in the introductory part, one might be interested in the value of \\(\\theta\\) by a given data set. Then \\(\\theta\\) is unknown and the data are observed. Treating \\(\\theta\\) as parameters instead of \\(x\\) leads to the — a mathematical formula that specifies the plausibility of the data. It states the probability of any possible observation: \\[p(X=x|[\\theta])=[\\theta]^x(1-[\\theta])^{(1-x)}\\] Please be aware that through exchanging the roles of \\(x\\) and \\(\\theta\\) in the second equation (likelihood function) this function is no longer a probability distribution and thus does not integrate to 1. 6.1.2 Prior &amp; Posterior The clearest difference between frequentist and Bayesian methods is the incorporation of prior information in the Bayesian framework. The posterior distribution results by combining the likelihood with the prior information. When an uninformative (e.g. uniform) prior is used then the posterior is completely dependent on the data. The influence of the prior on the posterior depends on their relative weighting. Remember that the posterior is the conditional distribution of the parameter given the data. The mathematical procedure behind it depicts . Consider for example a Beta distribution with the parameters a and b: \\(\\beta \\sim Beta(a,b)\\). Assuming a coin flip experiment, the parameters of a Beta distribution can be interpreted as \\(a=\\)no. of heads and $b = $no. of tails, thus, \\(n=a+b\\). Consequently, Beta(1,1) has a lower weight and thus influence on the posterior than Beta(5,5). # simulate coin flip data set sample.space &lt;- c(1, 0) theta &lt;- 0.5 # probability of a success (here: head) X &lt;- 30 # number of trials in the experiment n &lt;- 10 # number of observations k &lt;- 0 # number of heads [initialization] ## repeat experiment N-times for (i in 1: n) { k[i] &lt;- sum(sample(sample.space, size = X, replace = TRUE, prob = c(theta, 1 - theta))) } ## show results in a tibble coin.flip1 &lt;- tibble(&quot;n&quot; = seq(from=1, to=n, by=1), &quot;k&quot; = k, &quot;x&quot; = X ) #show the different prior distributions par(mfrow=c(1,2)) Beta1 &lt;- rbeta(n=1e5, shape1=1, shape2=1) Beta2 &lt;- rbeta(n=1e5, shape1=10, shape2=10) plot(density(Beta1), ylim=c(0,4), main=&quot;Prior 1: Beta(1,1)&quot;, xlab=expression(theta)) plot(density(Beta2), ylim=c(0,4), main=&quot;Prior 2: Beta(10,10)&quot;, xlab=expression(theta)) #show posterior distributions weighted by priors prior1 &lt;- set_prior(&quot;beta(1,1)&quot;, class = &quot;Intercept&quot;) prior2 &lt;- set_prior(&quot;beta(10,10)&quot;, class = &quot;Intercept&quot;) model1 &lt;- brm(data=coin.flip1, formula= k|trials(x) ~ 1, prior=prior1, family = binomial(link = &quot;logit&quot;)) ## ## SAMPLING FOR MODEL &#39;d233e2cb513249af99cc8901673a6dea&#39; NOW (CHAIN 1). ## Chain 1: Rejecting initial value: ## Chain 1: Error evaluating the log probability at the initial value. ## Chain 1: Exception: beta_lpdf: Random variable is 1.41169, but must be less than or equal to 1 (in &#39;model293d64f719cc_d233e2cb513249af99cc8901673a6dea&#39; at line 22) ## ## Chain 1: Rejecting initial value: ## Chain 1: Error evaluating the log probability at the initial value. ## Chain 1: Exception: beta_lpdf: Random variable is -0.477164, but must be &gt;= 0! (in &#39;model293d64f719cc_d233e2cb513249af99cc8901673a6dea&#39; at line 22) ## ## Chain 1: Rejecting initial value: ## Chain 1: Error evaluating the log probability at the initial value. ## Chain 1: Exception: beta_lpdf: Random variable is -0.507018, but must be &gt;= 0! (in &#39;model293d64f719cc_d233e2cb513249af99cc8901673a6dea&#39; at line 22) ## ## Chain 1: Rejecting initial value: ## Chain 1: Error evaluating the log probability at the initial value. ## Chain 1: Exception: beta_lpdf: Random variable is -0.0731695, but must be &gt;= 0! (in &#39;model293d64f719cc_d233e2cb513249af99cc8901673a6dea&#39; at line 22) ## ## Chain 1: Rejecting initial value: ## Chain 1: Error evaluating the log probability at the initial value. ## Chain 1: Exception: beta_lpdf: Random variable is -0.28327, but must be &gt;= 0! (in &#39;model293d64f719cc_d233e2cb513249af99cc8901673a6dea&#39; at line 22) ## ## Chain 1: Rejecting initial value: ## Chain 1: Error evaluating the log probability at the initial value. ## Chain 1: Exception: beta_lpdf: Random variable is -1.09299, but must be &gt;= 0! (in &#39;model293d64f719cc_d233e2cb513249af99cc8901673a6dea&#39; at line 22) ## ## Chain 1: ## Chain 1: Gradient evaluation took 1.1e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.096342 seconds (Warm-up) ## Chain 1: 0.062512 seconds (Sampling) ## Chain 1: 0.158854 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;d233e2cb513249af99cc8901673a6dea&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 1e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.079588 seconds (Warm-up) ## Chain 2: 0.062837 seconds (Sampling) ## Chain 2: 0.142425 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;d233e2cb513249af99cc8901673a6dea&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 8e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.074325 seconds (Warm-up) ## Chain 3: 0.071151 seconds (Sampling) ## Chain 3: 0.145476 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;d233e2cb513249af99cc8901673a6dea&#39; NOW (CHAIN 4). ## Chain 4: Rejecting initial value: ## Chain 4: Error evaluating the log probability at the initial value. ## Chain 4: Exception: beta_lpdf: Random variable is -0.513716, but must be &gt;= 0! (in &#39;model293d64f719cc_d233e2cb513249af99cc8901673a6dea&#39; at line 22) ## ## Chain 4: ## Chain 4: Gradient evaluation took 1.1e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.081146 seconds (Warm-up) ## Chain 4: 0.071126 seconds (Sampling) ## Chain 4: 0.152272 seconds (Total) ## Chain 4: samples.model1 &lt;- posterior_samples(model1) model2 &lt;- brm(data=coin.flip1, formula= k|trials(x) ~ 1, prior=prior2, family = binomial(link = &quot;logit&quot;)) ## ## SAMPLING FOR MODEL &#39;4d8687896c7ea933ac28802adabeca38&#39; NOW (CHAIN 1). ## Chain 1: Rejecting initial value: ## Chain 1: Error evaluating the log probability at the initial value. ## Chain 1: Exception: beta_lpdf: Random variable is 1.41134, but must be less than or equal to 1 (in &#39;model293d4e737cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22) ## ## Chain 1: Rejecting initial value: ## Chain 1: Error evaluating the log probability at the initial value. ## Chain 1: Exception: beta_lpdf: Random variable is -0.0575102, but must be &gt;= 0! (in &#39;model293d4e737cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22) ## ## Chain 1: Rejecting initial value: ## Chain 1: Error evaluating the log probability at the initial value. ## Chain 1: Exception: beta_lpdf: Random variable is -0.675824, but must be &gt;= 0! (in &#39;model293d4e737cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22) ## ## Chain 1: ## Chain 1: Gradient evaluation took 1.4e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.021875 seconds (Warm-up) ## Chain 1: 0.022912 seconds (Sampling) ## Chain 1: 0.044787 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;4d8687896c7ea933ac28802adabeca38&#39; NOW (CHAIN 2). ## Chain 2: Rejecting initial value: ## Chain 2: Error evaluating the log probability at the initial value. ## Chain 2: Exception: beta_lpdf: Random variable is -1.57662, but must be &gt;= 0! (in &#39;model293d4e737cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22) ## ## Chain 2: Rejecting initial value: ## Chain 2: Error evaluating the log probability at the initial value. ## Chain 2: Exception: beta_lpdf: Random variable is 1.01808, but must be less than or equal to 1 (in &#39;model293d4e737cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22) ## ## Chain 2: Rejecting initial value: ## Chain 2: Error evaluating the log probability at the initial value. ## Chain 2: Exception: beta_lpdf: Random variable is 1.64284, but must be less than or equal to 1 (in &#39;model293d4e737cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22) ## ## Chain 2: ## Chain 2: Gradient evaluation took 9e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.021949 seconds (Warm-up) ## Chain 2: 0.023011 seconds (Sampling) ## Chain 2: 0.04496 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;4d8687896c7ea933ac28802adabeca38&#39; NOW (CHAIN 3). ## Chain 3: Rejecting initial value: ## Chain 3: Error evaluating the log probability at the initial value. ## Chain 3: Exception: beta_lpdf: Random variable is -1.67186, but must be &gt;= 0! (in &#39;model293d4e737cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22) ## ## Chain 3: Rejecting initial value: ## Chain 3: Error evaluating the log probability at the initial value. ## Chain 3: Exception: beta_lpdf: Random variable is -0.544132, but must be &gt;= 0! (in &#39;model293d4e737cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22) ## ## Chain 3: Rejecting initial value: ## Chain 3: Error evaluating the log probability at the initial value. ## Chain 3: Exception: beta_lpdf: Random variable is -0.396642, but must be &gt;= 0! (in &#39;model293d4e737cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22) ## ## Chain 3: Rejecting initial value: ## Chain 3: Error evaluating the log probability at the initial value. ## Chain 3: Exception: beta_lpdf: Random variable is 1.35674, but must be less than or equal to 1 (in &#39;model293d4e737cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22) ## ## Chain 3: Rejecting initial value: ## Chain 3: Error evaluating the log probability at the initial value. ## Chain 3: Exception: beta_lpdf: Random variable is 1.29124, but must be less than or equal to 1 (in &#39;model293d4e737cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22) ## ## Chain 3: Rejecting initial value: ## Chain 3: Error evaluating the log probability at the initial value. ## Chain 3: Exception: beta_lpdf: Random variable is -0.944933, but must be &gt;= 0! (in &#39;model293d4e737cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22) ## ## Chain 3: Rejecting initial value: ## Chain 3: Error evaluating the log probability at the initial value. ## Chain 3: Exception: beta_lpdf: Random variable is 1.10175, but must be less than or equal to 1 (in &#39;model293d4e737cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22) ## ## Chain 3: Rejecting initial value: ## Chain 3: Error evaluating the log probability at the initial value. ## Chain 3: Exception: beta_lpdf: Random variable is 1.1372, but must be less than or equal to 1 (in &#39;model293d4e737cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22) ## ## Chain 3: Rejecting initial value: ## Chain 3: Error evaluating the log probability at the initial value. ## Chain 3: Exception: beta_lpdf: Random variable is 1.94278, but must be less than or equal to 1 (in &#39;model293d4e737cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22) ## ## Chain 3: ## Chain 3: Gradient evaluation took 1e-05 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.021222 seconds (Warm-up) ## Chain 3: 0.02031 seconds (Sampling) ## Chain 3: 0.041532 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;4d8687896c7ea933ac28802adabeca38&#39; NOW (CHAIN 4). ## Chain 4: Rejecting initial value: ## Chain 4: Error evaluating the log probability at the initial value. ## Chain 4: Exception: beta_lpdf: Random variable is -1.84509, but must be &gt;= 0! (in &#39;model293d4e737cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22) ## ## Chain 4: Rejecting initial value: ## Chain 4: Error evaluating the log probability at the initial value. ## Chain 4: Exception: beta_lpdf: Random variable is -1.06841, but must be &gt;= 0! (in &#39;model293d4e737cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22) ## ## Chain 4: Rejecting initial value: ## Chain 4: Error evaluating the log probability at the initial value. ## Chain 4: Exception: beta_lpdf: Random variable is 1.04065, but must be less than or equal to 1 (in &#39;model293d4e737cf_4d8687896c7ea933ac28802adabeca38&#39; at line 22) ## ## Chain 4: ## Chain 4: Gradient evaluation took 7e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.021251 seconds (Warm-up) ## Chain 4: 0.017201 seconds (Sampling) ## Chain 4: 0.038452 seconds (Total) ## Chain 4: samples.model2 &lt;- posterior_samples(model2) par(mfrow=c(2,2)) Beta1 &lt;- rbeta(n=1e5, shape1=1, shape2=1) Beta2 &lt;- rbeta(n=1e5, shape1=10, shape2=10) plot(density(Beta1), ylim=c(0,4), main=&quot;Prior 1: Beta(1,1)&quot;, xlab=expression(theta), xlim=c(0,1)) plot(density(Beta2), ylim=c(0,4), main=&quot;Prior 2: Beta(10,10)&quot;, xlab=expression(theta), xlim=c(0,1)) plot(density(inv_logit_scaled(samples.model1$b_Intercept)), main=&quot;Posterior with prior Beta(1,1)&quot;, xlab=expression(theta), xlim=c(0.48,0.60)) plot(density(inv_logit_scaled(samples.model2$b_Intercept)), main=&quot;Posterior with prior Beta(10,10)&quot;, xlab=expression(theta), xlim=c(0.48,0.60)) A lot of effort has been done in the area of “Objective Baysian data analysis” in order to develop “uninformativ prior distributions”, because a lot of people feel uncomfortable using informative priors — seeing them as biased or unscientific. On the contrary, most people interpret results based on their prior experiences and in this light, prior information is just a way to quantify this (Dobson and Barnett 2008). It is “just” important that the definition of the prior distributions make sense (at every stage) in the model. 6.1.2.1 Exursos: Prior predictive distribution So far we have seen that the over parameters captures the initial assumptions or state of knowledge about the psychological variables they represent (Lee and Wagenmakers 2014), in the above example this variable is \\(\\theta\\). Considering these initial assumptions in terms of prior distributions allow to make predictions about what data we would expect given the model and current state of knowledge. This distribution is called . It is a distribution over data, and gives the relative probability of different observable outcomes before any data have been seen (Lee and Wagenmakers 2014). For the coin flip model we consider a flat prior distribution: Beta(1,1). The prior predictive distribution would therefore look like: ## prior predictive distribution ##coin flip example with prior: Beta(1,1) n &lt;- 1e5 p &lt;- rbeta(n=n,shape1 = 1,shape2 = 1) x &lt;- rbinom(n=n,size=30, prob=p) hist(x, freq = FALSE, main=&quot;Prior predictive distribution&quot;, ylim=c(0,0.15), xlab=&quot;k - number of heads&quot;) References "],
["modeling.html", "6.2 Modeling", " 6.2 Modeling 6.2.1 Introductory example As introductory example a coin flip experiment is considered. The question is if a particular coin is . In order to investigate this question a coin is flipped \\(x\\) times (=trials) and the number of success (i.e. number of “head”) \\(k\\) is recorded. This is repeated \\(n\\) times (=observations). #this code is just copy&amp;paste from above [it is included again for better comprehension] #simulate coin flip data set sample.space &lt;- c(0, 1) theta &lt;- 0.5 # probability of a success (here: head) X &lt;- 30 # number of trials in the experiment n &lt;- 10 # number of observations k &lt;- 0 # number of heads [initialization] ## repeat experiment N-times for (i in 1: n) { k[i] &lt;- sum(sample(sample.space, size = X, replace = TRUE, prob = c(theta, 1 - theta))) } ## show results in a tibble coin.flip &lt;- tibble(&quot;n&quot; = seq(from=1, to=n, by=1), &quot;k&quot; = k, &quot;x&quot; = X ) %&gt;% print() ## # A tibble: 10 x 3 ## n k x ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 10 30 ## 2 2 14 30 ## 3 3 11 30 ## 4 4 15 30 ## 5 5 16 30 ## 6 6 8 30 ## 7 7 13 30 ## 8 8 16 30 ## 9 9 15 30 ## 10 10 16 30 The above table shows the observed outcome, but how the underlying probability of coming up can be derived from that data set? 6.2.2 Steps of Data Analysis The approach described here is based on (McElreath 2015, @kruschke2015). Although the approach is introduced in a Bayesian context, it can be used as a general guideline (with some caveats): Identify the relevant variables according to the hypothesis (Measurement scales, predicted vs. predictor variables). Define the descriptive model for the relevant variables. likelihood distribution (distribution of each outcome variable that defines the plausibility of individual observations) parameters (define and name all parameters of the model in order to relate the likelihood to the predictor variable(s)) Bayesian context: Specify prior distribution(s). Further steps that will be subject of later chapters: Inference and interpretation of the results. Model checking (Is the defined model adequate?) In the following we are interested in the question if a certain coin is biased. First step is to identify the relevant variables. For the coin flip experiment a coin is flipped \\(n\\) times, whereby each observation consists of \\(x\\) trials. The variable \\(Y\\) is dichotomous with the possible outcomes “head” and “tail”. For each observation the outcome is recorded: “0” for coming up tail and “1” for coming up head. The data are summarized for each observation. The variable \\(k\\) indicates the number of heads coming up in \\(x\\) trials. In the second step a descriptive model for the identified variables has to be defined. An underlying probability \\(\\theta\\) is assumed, indicating the probability of heads coming up \\(p(y=1)\\). The probability that the outcome is head, given a value of parameter \\(\\theta\\), is the value of \\(\\theta\\) (Kruschke 2015, 109). Formally, this can be written as \\[p(y=1|\\theta)=\\theta\\] As only two outcomes of \\(Y\\) exists, the probability that the outcome is tail is the complementary probability \\(1-\\theta\\). Both probabilities can be combined in one probability expression: \\[Pr(Y|n,\\theta)=\\frac{n!}{y!(n-y)!}\\theta^{y}(1-\\theta)^{n-y}.\\] This probability distribution is called the Binomial distribution. The fracture at the beginning indicates how many ordered sequences of \\(n\\) outcomes a count \\(y\\) have, therefore the important conceptional part is the latter one. # Plot probability distribution: What would be the expected observed number # of &quot;head&quot; given the underlying prob. theta? par(mfrow = c(2, 2)) ## theta=0.2 hist( rbinom(n = 1e6, size = 30, prob = 0.2), xlab = &quot;k&quot;, main = &quot;Binomial(1e6,0.2)&quot;, xlim = c(0, 30), freq = FALSE ) ## theta=0.5 hist( rbinom(n = 1e6, size = 30, prob = 0.5), xlab = &quot;k&quot;, main = &quot;Binomial(1e6,0.5)&quot;, xlim = c(0, 30), freq = FALSE ) ## theta=0.8 hist( rbinom(n = 1e6, size = 30, prob = 0.8), xlab = &quot;k&quot;, main = &quot;Binomial(1e6,0.8)&quot;, xlim = c(0, 30), freq = FALSE ) When the coin is flipped only once, then the probabilty can be written as: \\[Pr(Y|\\theta)=\\theta^{y}(1-\\theta)^{1-y}.\\] This special variant of the Binomial distribution is the so-called Bernoulli distribution. To see the connection to the first considerations: When the outcome “head” is observed the equation reduces to \\(Pr(y=1|\\theta)=\\theta\\) and when the outcome “tail” is observed the equation results in \\(Pr(y=0|\\theta)=(1-\\theta).\\) Accordingly, for the introductory example it can be noted that the coin flip variable \\(Y\\) Binomial distribution. (Note: For Bayes’ rule the is needed. Remember, the likelihood function treats \\(\\theta\\) as unknow and the data as known. This role of parameter is exchanged in a probability distribution.) # calculate the Liklihood function binomial.likelihood &lt;- function(n, k, theta){theta^k*(1-theta)^(n-k)} theta &lt;- seq(from=0, to=1, by=0.01) # Plot likelihood: What would be the expected underlying prob. theta given # observed number of &quot;head&quot; in 100 observations? par(mfrow=c(2,2)) plot(theta, binomial.likelihood(100,20,theta), xlab=expression(theta), ylab=&quot;likelihood&quot;, type=&quot;l&quot;) plot(theta,binomial.likelihood(100,50,theta), xlab=expression(theta), ylab=&quot;likelihood&quot;, type=&quot;l&quot;) plot(theta,binomial.likelihood(100,80,theta), xlab=expression(theta), ylab=&quot;likelihood&quot;, type=&quot;l&quot;) The third step is solely a , that is the incorporation of prior knowledge. What do we believe about the coin bias \\(\\theta\\) before seeing the data? Assuming that no expectation about \\(\\theta\\) exists a priori, indicating that all values of \\(\\theta\\) between 0 and 1 are equally probable. This can be modeled by a uniform distribution or as already visualized as Beta distribution with parameters a=1 and b=1 (see following figure). # Modelling prior knowledge &quot;ignorance&quot; par(mfrow = c(1, 2)) ## simulated a uniform(0,1) distribution rethinking::dens(runif(n=1e6,min=0, max=1), ylim = c(0,1.5), xlab=expression(theta), main=&quot;Uniform(0,1)&quot;) ## simulates a beta(1,1) distribution rethinking::dens(rbeta(n=1e6,shape1=1,shape2=1), ylim = c(0,1.5), xlab=expression(theta), main=&quot;Beta(1,1)&quot;) So far, the coin flip model is define conceptionally. In the following some notational considerations have to be made. 6.2.3 Notation 6.2.3.1 Textual notation In the textual notation, first the prior assumptions (if the Bayesian perspective is taken) are described. For the coin flip example this is: \\[\\theta \\sim Beta(1,1).\\] The symbol “\\(\\sim\\)” means “is distributed as”, thus, the above equation says before seeing the data all possible values of \\(\\theta\\) between 0 and 1 are assumed to be equally likely. Subsequently, the descriptive model for the data has to be defined. As already described in the section above, it is assumed that the observed data (upcoming of heads \\(k\\)) are distributed as Binomial distribution with given \\(n\\) (number of observations) and unknown \\(\\theta\\). This relation is denoted symbollically as \\[k\\sim Binomial(\\theta|n).\\] To summarize the current model (whereby the prior knowledge is only considered from a Bayesian perspective): \\[\\theta \\sim Beta(1,1),\\] \\[k\\sim Binomial(\\theta|n).\\] 6.2.3.2 Graphical notation When models get very complex and incorporate many parameters it can be difficult to tease out all relations between the model components. In such a situation a graphical notation of a model might be helpful. In the following the convention described in Wagenmakers and Lee’s (2014) is used: The graph structure is used to indicate dependencies between the variables, with children depending on their parents (Lee and Wagenmakers 2014). General conventions: Nodes - problem relevant variables, shaded nodes - observed variables, unshaded nodes - unobserved variables, circular nodes - continuous variables, square nodes - discrete variables, single line - stochastic dependency, and double line - deterministic dependency. For the introductory example this indicates: relevant variables: number of trials (\\(n\\)), number of success (\\(k\\)) and probability for a success (\\(\\theta\\)), observed variables: \\(n\\) and \\(k\\), unobserved variables: \\(\\theta\\), continuous variable: \\(\\theta\\), discrete variables: \\(n\\) and \\(k\\). In the next step the dependencies have to be determined: The number of success \\(k\\) depends on the probability of a success \\(\\theta\\) as well as on the number of trials \\(n\\). Finally, the graphical structure together with the textual notation can be represented: Graphical notation Beta-Binomial Modell - One group 6.2.4 An outlook: Hierarchical models Often data can be considered as part of an overall structure. Single observations can be modelled belonging into different groups. These groups in turn are part of a superordinate group etc. Such information are presented in a model in form of a hierarchy. For example, consider again the coin flip experiment. The outcome of is influenced by the probability \\(\\theta\\). Further, \\(\\theta\\) is assumed to be distributed as Beta(1,1). Remember that the parameter a and b of a Beta-distribution can be considered in this context as: \\(a=\\)number of heads and \\(b=\\) number of tails, consequently, \\(n=a+b\\). 6.2.4.1 Reparameterization of a Beta distribution Probability distributions can be described by their and (or dispersion). The of a Beta distribution is defined as: \\(\\omega=\\frac{a-1}{a+b-2}\\), and the concentration as: \\(\\kappa=a+b.\\) The nice thing is, that the definition of the as well as of the consists solely of the parameters a and b. Therefore, it is possible to re-express the parameters of a Beta density in terms of \\(\\omega\\) and \\(\\kappa\\), such that: \\[Beta(a,b)=Beta\\left(\\omega(\\kappa -2)+1, (1-\\omega)(\\kappa -2)+1\\right).\\] Why this is useful? And what is its value in connection with hierarchical modeling? Return back to the coin flip experiment. So far, the parameters of the prior on \\(\\theta\\) are fixed: \\(a=1\\) and \\(b=1\\). Assume that we get further information: The manifacturing process of the coins has a bias near \\(\\omega\\) (example taken from (Kruschke 2015)). But how to incorporate this additional knowledge in the model? At this point, the hierarchy and the reparameterization come into play. Hierarchy because a further assumption is placed on top of the existing model and reparameterization, because we want to express the prior in terms of the mode \\(\\omega\\). Such that the model can be assumed as follows: Graphical notation hierarchical Beta-Binomial Modell - One group Now, the parameters of the hyperpriors (Gamma and Beta) are fixed, but they can be treated as parameters as well..as such hierarchical models can be created with any degree of complexity: Graphical notation hierarchical Beta-Binomial Modell - One group References "],
["further-examples.html", "6.3 Further examples", " 6.3 Further examples 6.3.1 Difference between two groups In the introductory example we asked for the underlying probility \\(\\theta\\) of a single coin that was flipped repeatedley. Consider now, that a second coin \\(y_2\\) is introduced. One question that arises might be for example: #simulate flipps of two coins sample.space &lt;- c(0,1) ##First coin: theta1 &lt;- 0.5 # probability of a success (here: head) X1 &lt;- 30 # number of trials in the experiment n1 &lt;- 100 # number of observations k1 &lt;- 0 # number of heads [initialization] for (i in 1: n1) { k1[i] &lt;- sum(sample(sample.space, size = X1, replace = TRUE, prob = c(theta1, 1 - theta1))) } ##Second coin: theta2 &lt;- 0.7 # probability of a success (here: head) X2 &lt;- 30 # number of trials in the experiment n2 &lt;- 100 # number of observations k2 &lt;- 0 # number of heads [initialization] ## repeat experiment N-times for (i in 1: n2) { k2[i] &lt;- sum(sample(sample.space, size = X2, replace = TRUE, prob = c(theta2, 1 - theta2))) } ## show results in a tibble coin.flip2 &lt;- tibble(&quot;coin&quot; = c(replicate(n1,&quot;coin1&quot;), replicate(n2,&quot;coin2&quot;)), &quot;n&quot; = c(seq(from=1, to=n1, by=1), seq(from=1, to=n2, by=1)), &quot;k&quot; = c(k1,k2), &quot;x&quot; = c(replicate(n1,X1), replicate(n2,X2)) ) %&gt;% print() ## # A tibble: 200 x 4 ## coin n k x ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 coin1 1 15 30 ## 2 coin1 2 16 30 ## 3 coin1 3 17 30 ## 4 coin1 4 17 30 ## 5 coin1 5 15 30 ## 6 coin1 6 14 30 ## 7 coin1 7 14 30 ## 8 coin1 8 14 30 ## 9 coin1 9 12 30 ## 10 coin1 10 21 30 ## # … with 190 more rows #Plotting the observed results ggplot(data=coin.flip2,mapping = aes(x=k, fill=coin ))+ geom_histogram() 6.3.1.1 Conceptual steps for modeling We suppose that the underlying probabilities of the two coins correspond to different latent variables \\(\\theta_1\\) and \\(\\theta_2\\). First step is again the identification of the relevant variables according to the research question. As already indicated for the “one coin” example we have: the observed number of heads \\(k_1\\) and \\(k_2\\) (for each coin, respectively), which is influenced by the number of observations \\(n_1\\) and \\(n_2\\) and by the underlying probabilities \\(\\theta_1\\) and \\(\\theta_2\\). Furthermore, from a conceptional perspective, we are interested in the difference between the coin biases. Therefore a further variable will be introduced \\(\\delta\\), defined by: \\[\\delta =\\theta_1 - \\theta_2.\\] The distributional assumptions, according to the second and third step, can be adopted from the “one coin” example, such that the graphical notation (including the textual notation) can be denoted as follows: 6.3.1.2 Notation Beta-Binomial Model - Two Groups Graphical notation Beta-Binomial Modell - Two groups 6.3.2 Simple linear regression with one metric predictor The following example originates from a data set in which speed of cars and the distance taken to stop was recorded. It is a simple data set good for introducing the basic ideas for simple linear regression. #The &quot;cars&quot; data set data(cars) #take a look at the variables included in the data set str(cars) ## &#39;data.frame&#39;: 50 obs. of 2 variables: ## $ speed: num 4 4 7 7 8 9 10 10 10 11 ... ## $ dist : num 2 10 4 22 16 10 18 26 34 17 ... One possible question could be how much the stopping distance increases when the speed of a car increases. 6.3.2.1 Conceptual steps for modeling First step is to identify the relevant variables. In this case these are “speed” measured in mph and “distance” measured in ft, thus, both variables are metric variables. As distance will be predicted from speed. The is “distance” and the is “speed”. A scatter plot can visualize a possible relationship between both variables. plot(x=cars$speed,y=cars$dist, type=&quot;p&quot;, main=&quot;scatter plot of cars data set&quot;, ylab=&quot;distance in ft&quot;, xlab=&quot;speed in mph&quot;) Next step is to define a descriptive model of the data. According to the scatter plot it is not too absurd to think that distance might be proportional to speed. Therefore, a linear relationship between both variables can be assumed, where speed is used i order to predict distance. But how can the distribution of the predicted variable “distance” be described? The following plot shows in blue the density of the actual distance values. #density of distance values in blue #(in black simulation of a normal distribution) dens(cars$dist, col=&quot;blue&quot;, norm.comp = TRUE, main=&quot;Distribution of distance&quot;, xlab=&quot;distance in ft&quot;) Although the distribution of “distance” values is not identical to the corresponding normal distribution, it can be assumed that the values follow a normal distribution. The underlying consideration is that the distance values \\(y_i\\) are distributed randomly according to a normal distribution around the predicted value \\(\\hat{y}\\) and with a standard deviation denoted with \\(\\sigma\\). This can be denoted as: \\[y_i\\sim Normal(\\mu, \\sigma).\\] The index \\(i\\) indicates each element (i.e. car) of the list \\(y\\), which in turn is the list of distances. In the third step, a Bayesian perspective is taken the prior knowlege (before seeing the data) has to be defined. The parameters of the current model are the predicted value \\(\\mu\\) and the standard deviation \\(\\sigma\\). For the parameter \\(\\mu\\) a normal distribution can be assumend with parameters that reflect the estimated values from the sample. #descriptive statistics from the sample tibble(variables=c(&quot;speed&quot;, &quot;distance&quot;), mean=c(mean(cars$speed),mean(cars$dist)), sigma = c(sd(cars$speed), sd(cars$dist))) ## # A tibble: 2 x 3 ## variables mean sigma ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 speed 15.4 5.29 ## 2 distance 43.0 25.8 \\[\\mu\\sim Normal(43,26)\\] For the standard deviation \\(\\sigma\\) a uniform distribution is assumed: \\[\\sigma\\sim Uniform(0,40)\\] 6.3.2.2 Excursos: Identically and independently distributed () The short model description \\(y_i\\sim Normal(\\mu, \\sigma)\\) incorporates often already an assumption about the distribution of distance-values: They are . Often the abbreviation can be found for this assumption: \\[y_i\\overset{\\text{iid}}{\\sim} Normal(\\mu, \\sigma).\\] The abbreviation indicates that each value \\(y_i\\) has the same probability function, independent of the other \\(y\\) values and using the same parameters (McElreath 2015). This is hardly ever true (why hierarchical modeling is very attractive). For example, thinking about the cars in the current example data set. Some cars may be of different types or even the same type but different batches. But the question is: Is this underlying dependency relevant for the model? If yes, this information has to be added in the model (e.g. in form of a hierachical model). Janyes states it as follows: (Jaynes 2003, 339). But if one do not know any relevant underlying relationships the most conservative distribution to use is . Note, that the stated assumptions define how the model represents a problem and not how the world should be understood. For example, there might exist underlying correlations but on the overall distribution there influence tends towards zero. In such cases it remains usefull to assume iid (McElreath 2015). 6.3.3 Notation Simple Regression model Graphical notation Simplre Regression model References "],
["further-elaboration-on-modeling-in-anticipation-of-the-topic-estimation.html", "6.4 Further elaboration on modeling (in anticipation of the topic “estimation”)", " 6.4 Further elaboration on modeling (in anticipation of the topic “estimation”) 6.4.1 Beta-Binomial model - one group (revisited) Sofar the existence of the underlying probability \\(\\theta\\) for observing head as outcome of a coin flip has been discussed. But the estimation of \\(\\theta\\) has been ignored until yet. Although “estimation” will be topic of next chapter, it is helpful at this point to discuss the introduced models further. In order to estimate \\(\\theta\\) are needed. When it comes to estimation exactly this/these parameter(s) will be the result(s), therefore is is important to see already the connection to the models that were developed in this chapter. For the coin flip example the value of interest is the underlying probability, thus, only one parameter is needed:\\(\\beta_0\\). (Note: Latin letters are used when we refer to the sample, greek letters are used when we refer to the population.) How is \\(\\beta_0\\) linked to the latent variable \\(\\theta\\)? Considering for example the simplest case: a (see next plot left side). The problem which arises at this point is that \\(\\theta\\) represents a probability, and is therefore bounded to the range 0-1 (grey shaded area). #Different relationships between the parameter and expected value x &lt;- seq(from=-4, to=4, length.out = 100) y &lt;- x #linear relationship y.log &lt;- inv_logit_scaled(x) #logistic relationship par(mfrow = c(1, 2)) #set both plot beside each other plot(x, y, type=&quot;l&quot;, ylab=expression(theta), xlab=expression(beta[0])) rect(-5, 0, 5, 1, col = rgb(0.5, 0.5, 0.5, 1/4), border = NA) plot(x, y.log, type=&quot;l&quot;, ylab=expression(logit~(theta)), xlab=expression(beta[0])) rect(-5, 0, 5, 1, col = rgb(0.5, 0.5, 0.5, 1/4), border = NA) A mathematical transformation is needed such that the parameter \\(\\beta_0\\) can take any value while \\(\\theta\\) is bounded to the range 0-1. One transformation that offers exactly this possibility is the (see aboth plot right side) \\[logit(\\theta) = \\beta_0.\\] As the underlying assumption maps the parameter to the latent variable \\(\\theta\\) (and not the other way around) from a conceptional point of view the is more appropriate, which is the in this case: \\[\\theta = logistic(\\beta_0).\\] It is defined as \\[\\theta=\\frac{exp(\\beta_0)}{1+exp(\\beta_0)}.\\] Both expression, and link achieve mathematically the same result but it is conceptionally just a different matter of emphasis (Kruschke 2015). 6.4.1.1 Notation of beta-binomial model - one group (revisited) The current descriptive model incorporates the idea that parameter \\(\\beta_0\\) is estimated from the given sample. It defines the latent variable \\(\\theta\\). The parameter is maped to \\(\\theta\\) by a logistic link function. The underlying probability \\(\\theta\\) designates the observed number of upcoming heads. The number of upcoming heads in turn, is assumed to be distributed as Binomial distribution. 6.4.2 Beta-Binomial model - two groups (revisited) In the above model for two coins the latent variable \\(\\delta\\) was already introduced. It is defined by the difference between the underlying probabilities \\(\\theta_1-\\theta_2\\). Which parameters should be used in order to estimate the difference between both groups? As we will see, it turns out that the same mathematical form can be used, as one would use for simple linear regression: \\[\\theta_j=\\beta_0+\\beta_1*X_{Group_j},\\] \\[\\textrm{with } X_{Group_j}=\\begin{cases} 0, \\textrm{if coin 2,}\\\\ 1, \\textrm{if coin 1.} \\end{cases}\\] Considering coin 2, the above equation would result in \\[\\theta_2 = \\beta_0,\\] which is the and indicates the proportion of head coming up for coin 2. Considering by contrast coin 1, then the equation would result in: \\[\\theta_1 = \\beta_0 + \\beta_1.\\] The proportion of coming up head for coin 1 has to be calcuated by summing up the \\(\\beta_0\\) and the \\(\\beta_1\\). Taken togehter: What is the interpretation of the slope \\(\\beta_1\\)? The difference \\(\\delta=\\theta_1-\\theta2\\) is \\[\\theta_1 - \\theta_2 = (\\beta_0+\\beta_1)-\\beta_0=\\beta_1=\\delta,\\] the slope \\(\\beta_1\\), thus, we can see that this parameterization enables us to estimate the difference between two groups. When in comes to estimation and interpretation the results will be the intercept \\(b_0\\) and the slope \\(b_1\\). 6.4.3 Simple linear regression model (revisited) References "],
["parameter-inference.html", "7 Parameter inference", " 7 Parameter inference MLE vs posterior confidence intervals credible intervals briefly: algorithms for MLE &amp; Bayesian inference "],
["hypothesis-testing.html", "8 Hypothesis Testing", " 8 Hypothesis Testing binomial test t-test ANOVA linear regression "],
["model-comparison.html", "9 Model Comparison", " 9 Model Comparison AIC likelihood ratio test Bayes factor "],
["bayesian-hypothesis-testing.html", "10 Bayesian hypothesis testing", " 10 Bayesian hypothesis testing testing via Bayesian posterior inference testing via model comparison "],
["model-criticism.html", "11 Model criticism", " 11 Model criticism prior and posterior predictives visual predictive checks prior/posterior predictive \\(p\\)-values "],
["simple-linear-regression.html", "12 Simple linear regression", " 12 Simple linear regression “multiple” = “more than one predictor” interactions collinearity categorical predictors relation to t-test and ANOVA different coding shemes robust regression "],
["logistic-regression.html", "13 Logistic regression", " 13 Logistic regression to do "],
["multinomial-regression.html", "14 Multinomial regression", " 14 Multinomial regression todo "],
["ordinal-regression.html", "15 Ordinal regression", " 15 Ordinal regression todo "],
["hierarchical-regression.html", "16 Hierarchical regression", " 16 Hierarchical regression todo "],
["common-probability-distributions.html", "A Common probability distributions ", " A Common probability distributions "],
["selected-continuous-distributions-of-random-variables.html", "A.1 Selected continuous distributions of random variables", " A.1 Selected continuous distributions of random variables A.1.1 Normal distribution One of the most important distribution families is the gaussian or normal family because it fits many natural phenomena. Furthermore the sampling distributions of many estimators depend on the normal distribution. On the one hand because they are derived from normally distributed random variables or on the other hand because they can be asymptotically approximated by a normal distribution for large samples (Central limit theorem). Distributions of the normal family are symmetric with range \\((-\\infty,+\\infty)\\) and have two parameters \\(\\mu\\) and \\(\\sigma\\) that are referred to, respectively, as the mean and the standard deviation of the normal random variable. These parameters are examples of location and scale parameters. The normal distribution is located at \\(\\mu\\) and its width is scaled by choice of \\(\\sigma\\). The distribution is symmetric with most observations lying aroung the central peak \\(\\mu\\) and more extreme values are further away depending on \\(\\sigma\\). \\[X\\sim Normal(\\mu,\\sigma^2)\\] Fig.~ shows the probability density function of three normal distributed random variables with different parameters. Fig.~ shows the corresponding cumulative function of the three normal distributions. rv_normal &lt;- tibble( x = seq(from = -15, to = 15, by = .01), y1 = dnorm(x), y2 = dnorm(x, mean = 2, sd = 2), y3 = dnorm(x, mean = -2, sd = 3) ) %&gt;% pivot_longer(cols = starts_with(&quot;y&quot;), names_to = &quot;parameter&quot;, values_to = &quot;y&quot;) %&gt;% mutate( parameter = case_when(parameter == &quot;y1&quot; ~ &quot;(0,1)&quot;, parameter == &quot;y2&quot; ~ &quot;(2,2)&quot;, parameter == &quot;y3&quot; ~ &quot;(-2,3)&quot;) ) ggplot(rv_normal, aes(x, y, color = parameter)) + geom_line(size = 2) + labs(color = &quot;X ~ Normal&quot;, y = &quot;Density&quot;) Figure 16.1: Examples of probability density function of normal distributions. rv_normal %&gt;% group_by(parameter) %&gt;% mutate( cum_y = cumsum(y)/sum(y) ) %&gt;% ungroup() %&gt;% ggplot(aes(x, cum_y, color = parameter)) + geom_line(size = 2) + labs(color = &quot;X ~ Normal&quot;, y = &quot;y&quot;) Figure 16.2: Examples of the cumulative distribution function of normal distributions corresponding to the previous probability density functions. A special case of normal distributed random variables is the standard normal distributed variable with \\(\\mu=0\\) and \\(\\sigma=1\\): \\(Y\\sim Normal(0,1)\\). Each normal distribution can be converted into a standard normal distribution by z-standardization (see equation below). The advantage of standardization is that values from different scales can be compared, because they become scale independent by z-transformation. Probability density function \\[f(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-0.5\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right)\\] Cumulative distribution function \\[F(x)=\\int_{-\\inf}^{x}f(t)dt\\] Expected value \\(E(X)=\\mu\\) Variance \\(Var(X)=\\sigma^2\\) Z-transformation \\(Z=\\frac{X-\\mu}{\\sigma}\\) **Deviation and *Coverage** The normal distribution is often associated with the . The values refer to the probability of a random data point landing within , or standard deviations of the mean (Fig.~ depicts these three intervals). For example, about 68% of values drawn from a normal distribution are within one standard deviation \\(\\sigma\\) away from the mean \\(\\mu\\). \\(P(\\mu-\\sigma \\leq X \\leq \\mu+\\sigma) = 0.6827\\) \\(P(\\mu-2\\sigma \\leq X \\leq \\mu+2\\sigma) = 0.9545\\) \\(P(\\mu-3\\sigma \\leq X \\leq \\mu+3\\sigma) = 0.9973\\) # plot normal distribution with intervals ggplot(NULL, aes(x = c(-10, 10))) + # plot area under the curve stat_function(fun = dnorm, args = list(mean = 0, sd = 2), geom = &quot;area&quot;, fill = project_colors[1], xlim = c(-6, 6)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 2), geom = &quot;area&quot;, fill = project_colors[2], xlim = c(-4, 4)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 2), geom = &quot;area&quot;, fill = project_colors[3], xlim = c(-2, 2)) + # plot the curve stat_function(fun = dnorm, args = list(mean = 0, sd = 2), geom = &quot;line&quot;, xlim = c(-10, 10), size = 2) + # scale x-axis xlim(-10, 10) + # label x-axis xlab(&quot;X&quot;) + # label ticks of x-axis scale_x_continuous(breaks = c(-6,-4,-2,0,2,4,6), labels = c(expression(-3~sigma),expression(-2~sigma), expression(-sigma),&quot;0&quot;,expression(sigma), expression(2~sigma),expression(3~sigma))) Figure 16.3: Coverage of normal distribution Linear transformations If \\(X\\sim Normal(\\mu, \\sigma^2)\\) is linear transformed by \\(Y=a*X+b\\), then the new random variable is again normal distributed with \\(Y \\sim Normal(a\\mu+b,a^2\\sigma^2)\\). Are \\(X\\sim Normal(\\mu_x, \\sigma^2)\\) and \\(Y\\sim Normal(\\mu_y, \\sigma^2)\\) normal distributed and independent, then their sum is again normal distributed with \\(X+Y \\sim Normal(\\mu_x+\\mu_y, \\sigma_x^2+\\sigma_y^2)\\). A.1.2 Chi-squared distribution The \\(\\chi^2\\)-distribution is widely used in hypothesis testing in inferential statistics, because many test statistics are approximately distributed as \\(\\chi^2\\)-distribution. The \\(\\chi^2\\)-distribution is directly related to the standard normal distribution: The sum of \\(n\\) independent and standard normal distributed random variables \\(X_1,X_2,...,X_n\\) is distributed according to a \\(\\chi^2\\) distribution with \\(n\\) : \\[Y=X_1^2+X_2^2+...+X_n^2.\\] The \\(\\chi^2\\) distribution is a skew probability distribution with range \\([0,+\\infty)\\) and only one parameter: \\(n\\), the degrees of freedom: (If \\(n=1\\), then \\((0,+\\infty)\\).) \\[X\\sim \\chi^2(n).\\] Fig.~ shows the probability density function of three chi-squared distributed random variables with different values for the parameter. Notice, that with increasing degrees of freedom the chi-squared distribution approximates the normal distribution. For \\(n \\geq 30\\) the chi-squared distribution can be approximated by a normal distribution. Fig.~ shows the corresponding cumulative function of the three chi-squared density distributions. rv_chisq &lt;- tibble( x = seq(from = 0, to = 20, by = .01), y1 = dchisq(x, df = 2), y2 = dchisq(x, df = 4), y3 = dchisq(x, df = 9) ) %&gt;% pivot_longer(cols = starts_with(&quot;y&quot;), names_to = &quot;parameter&quot;, values_to = &quot;y&quot;) %&gt;% mutate( parameter = case_when(parameter == &quot;y1&quot; ~ &quot;(2)&quot;, parameter == &quot;y2&quot; ~ &quot;(4)&quot;, parameter == &quot;y3&quot; ~ &quot;(9)&quot;) ) # dist plot ggplot(rv_chisq, aes(x, y, color = parameter)) + geom_line(size = 2) + labs(color = &quot;X ~ Chi-Squared&quot;, y = &quot;Density&quot;) Figure 16.4: Examples of probability density function of chi-squared distributions. rv_chisq %&gt;% group_by(parameter) %&gt;% mutate( cum_y = cumsum(y)/sum(y) ) %&gt;% ungroup() %&gt;% ggplot(aes(x, cum_y, color = parameter)) + geom_line(size = 2) + labs(color = &quot;X ~ Chi-Squared&quot;, y = &quot;y&quot;) Figure 16.5: Examples of the cumulative distribution function of chi-squared distributions corresponding to the previous probability density functions. Probability density function \\[f(x)=\\begin{cases}\\frac{x^{\\frac{n}{2}-1}e^{-\\frac{x}{2}}}{2^{\\frac{n}{2}}\\Gamma (\\frac{n}{2})} &amp;\\textrm{ for }x&gt;0,\\\\ 0 &amp;\\textrm{ otherwise.}\\end{cases}\\] Where \\(\\Gamma (\\frac{n}{2})\\) denotes the Gamma function. Cumulative distribution function \\[F(x)=\\frac{\\gamma (\\frac{n}{2},\\frac{x}{2})}{\\Gamma \\frac{n}{2},}\\] with \\(\\gamma(s,t)\\) being the lower incomplete gamma function: \\[\\gamma(s,t)=\\int_0^t t^{s-1}e^{-t} dt.\\] Expected value \\(E(X)=n\\) Variance \\(Var(X)=2n\\) Transformations The sum of two \\(\\chi^2\\)-distributed random variables \\(X \\sim \\chi^2(m)\\) and \\(Y \\sim \\chi^2(n)\\) is again a \\(chi^2\\)-distributed random variable \\(X+Y=\\chi^2(m+n)\\). A.1.3 F distribution The F distribution, named after R.A. Fisher, is used in particular in regression and variance analysis. It is defined by the ratio of two \\(chi^2\\)-distributed random variables \\(X\\sim \\chi^2(m)\\) and \\(Y\\sim \\chi^2(n)\\), each divided by its degree of freedom: \\[F=\\frac{\\frac{X}{m}}{\\frac{Y}{n}}.\\] The F distribution is a continuous skew probability distribution with range \\((0,+\\infty)\\) and two parameters \\(m\\) and \\(n\\), corresponding to the degrees of freedom of the two \\(chi^2\\)-distributed random variables: \\[X \\sim F(m,n).\\] Fig.~ shows the probability density function of three F distributed random variables with different parameter values. For a small number of degrees of freedom the density distribution is skewed to the left side. When the number increases, the density distribution gets more and more symmetric. Fig.~ shows the corresponding cumulative function of the three F density distributions. rv_F &lt;- tibble( x = seq(from = 0, to = 7, by = .01), y1 = df(x, df1 = 2, df2 = 4), y2 = df(x, df1 = 4, df2 = 6), y3 = df(x, df1 = 12, df2 = 12) ) %&gt;% pivot_longer(cols = starts_with(&quot;y&quot;), names_to = &quot;parameter&quot;, values_to = &quot;y&quot;) %&gt;% mutate( parameter = case_when(parameter == &quot;y1&quot; ~ &quot;(2,4)&quot;, parameter == &quot;y2&quot; ~ &quot;(4,6)&quot;, parameter == &quot;y3&quot; ~ &quot;(12,12)&quot;) ) # dist plot ggplot(rv_F, aes(x, y, color = parameter)) + geom_line(size = 2) + labs(color = &quot;X ~ F&quot;, y = &quot;Density&quot;) Figure 16.6: Examples of probability density function of F distributions. rv_F %&gt;% group_by(parameter) %&gt;% mutate( cum_y = cumsum(y)/sum(y) ) %&gt;% ungroup() %&gt;% ggplot(aes(x, cum_y, color = parameter)) + geom_line(size = 2) + labs(color = &quot;X ~ F&quot;, y = &quot;y&quot;) Figure 16.7: Examples of the cumulative distribution function of F distributions corresponding to the previous probability density functions. Probability density function \\[F(x)=m^{\\frac{m}{2}}n^{\\frac{n}{2}} \\cdot \\frac{\\Gamma (\\frac{m+n}{2})}{\\Gamma (\\frac{m}{2})\\Gamma (\\frac{n}{2})} \\cdot \\frac{x^{\\frac{m}{2}-1}}{(mx+n)^{\\frac{m+n}{2}}} \\textrm{ for } x&gt;0.\\] Where \\(\\Gamma(x)\\) denotes the gamma function. Cumulative distribution function \\[F(x)=I\\left(\\frac{m \\cdot x}{m \\cdot x+n},\\frac{m}{2},\\frac{n}{2}\\right),\\] with \\(I(z,a,b)\\) being the regularized incomplete beta function: \\[I(z,a,b)=\\frac{1}{B(a,b)} \\cdot \\int_0^z t^{a-1}(1-t)^{b-1} dt.\\] Expected value \\(E(X) = \\frac{n}{n-2}\\) (for \\(n \\geq 3\\)) Variance \\(Var(X) = \\frac{2n^2(n+m-2)}{m(n-4)(n-2)^2}\\) (for \\(n \\geq 5\\)) A.1.4 Student t-distribution The t or student-t distribution was discovered by William S. Gosset in 1908 (Vallverdú 2016), who published his work under the pseudonym “student”. He worked at the Guinness factory and had to deal with the problem of small sample sizes. This challenge resulted finally in the t distribution. Accordingly, this distribution is used in particular when the sample size is small and the variance unknown, which is often the case in reality. Its shape ressembles the normal bell shape and has a peak at zero, but the t distribution is a bit lower and wider (bigger tails) than the normal distribution. The t distribution consists of a standard normal distributed random variable \\(X\\sim Normal(0,1)\\) and a \\(\\chi^2\\)-distributed random variable \\(Y\\sim \\chi^2(n)\\) (X and Y are independent): \\[T = \\frac{X}{\\sqrt{\\frac{Y}{n}}}.\\] The t distribution has range \\((-\\infty,+\\infty)\\) and one parameter \\(n\\), the degrees of freedom. The degrees of freedom can be calculated by the sample size \\(n\\) minus one: \\[X \\sim t(n).\\] Fig.~ shows the probability density function of three t distributed random variables with different parameters. Notice that for small degrees of freedom \\(n\\), the t-distribution has bigger tails. This is because the t distribution was specially designed to provide more conservative test results when analyzing small samples. When the degrees of freedom increases, the t distribution approaches a normal distribution. For \\(n \\geq 30\\) this approximation is quite good. Fig.~ shows the corresponding cumulative function of the three t density distributions. rv_student &lt;- tibble( x = seq(from = -6, to = 6, by = .01), y1 = dt(x, df = 1), y2 = dt(x, df = 2), y3 = dt(x, df = 10) ) %&gt;% pivot_longer(cols = starts_with(&quot;y&quot;), names_to = &quot;parameter&quot;, values_to = &quot;y&quot;) %&gt;% mutate( parameter = case_when(parameter == &quot;y1&quot; ~ &quot;(1)&quot;, parameter == &quot;y2&quot; ~ &quot;(2)&quot;, parameter == &quot;y3&quot; ~ &quot;(10)&quot;) ) # dist plot ggplot(rv_student, aes(x, y, color = parameter)) + geom_line(size = 2) + labs(color = &quot;X ~ t&quot;, y = &quot;Density&quot;) Figure 16.8: Examples of probability density function of t distributions. rv_student %&gt;% group_by(parameter) %&gt;% mutate( cum_y = cumsum(y)/sum(y) ) %&gt;% ungroup() %&gt;% ggplot(aes(x, cum_y, color = parameter)) + geom_line(size = 2) + labs(color = &quot;X ~ t&quot;, y = &quot;y&quot;) Figure 16.9: Examples of the cumulative distribution function of t distributions corresponding to the previous probability density functions. Probability density function \\[ f(x)=\\frac{\\Gamma(\\frac{n+1}{2})}{\\sqrt{n\\pi} \\cdot \\Gamma(\\frac{n}{2})}\\left(1+\\frac{x^2}{n}\\right)^{-\\frac{n+1}{2}},\\] with \\(\\Gamma(x)\\) denoting the gamma function. Cumulative distribution function \\[F(x)=I\\left(\\frac{x+\\sqrt{x^2+n}}{2\\sqrt{x^2+n}},\\frac{n}{2},\\frac{n}{2}\\right),\\] where \\(I(z,a,b)\\) denotes the regularized incomplete beta function: \\[I(z,a,b)=\\frac{1}{B(a,b)} \\cdot \\int_0^z t^{a-1}(1-t)^{b-1} dt.\\] Expected value \\(E(X) = 0\\) Variance \\(Var(X) = \\frac{n}{n-2}\\) (for \\(n \\geq 30\\)) A.1.5 Beta distribution The beta distribution creates a continuous distribution of numbers between 0 and 1, therefore this distribution is useful if the uncertain quantity is bounded by 0 and 1 (or 100%), is continuous, and has a single mode. In Bayesian Data Analysis the beta distribution has a special standing as prior distribution for a bernoulli or binomial (see discrete distributions) likelihood. The reason for this is that a combination of a beta prior and a bernoulli (or binomial) liklihood results in a posterior distribution with the same form as the beta distribution. Such priors are referred to as conjugate priors. A beta distribution has two parameters \\(a\\) and \\(b\\): \\[X \\sim Beta(a,b).\\] The two parameters can be interpreted as the number of observations made, such that: \\(n=a+b\\). If \\(a\\) and \\(b\\) get bigger, the beta distribution gets narrower. If only \\(a\\) gets bigger the distribution moves rightward and if only \\(b\\) gets bigger the distribution moves leftward. Thus, the parameters define the shape of the distribution, therefore they are also called shape parameters. A Beta(1,1) is equivalent to a uniform distribution. Fig.~ shows the probability density function of four beta distributed random variables with different parameter values. Fig.~ shows the corresponding cumulative functions. rv_beta &lt;- tibble( x = seq(from = 0, to = 1, by = .01), y1 = dbeta(x, shape1 = 1, shape2 = 1), y2 = dbeta(x, shape1 = 4, shape2 = 4), y3 = dbeta(x, shape1 = 4, shape2 = 2), y4 = dbeta(x, shape1 = 2, shape2 = 4) ) %&gt;% pivot_longer(cols = starts_with(&quot;y&quot;), names_to = &quot;parameter&quot;, values_to = &quot;y&quot;) %&gt;% mutate( parameter = case_when(parameter == &quot;y1&quot; ~ &quot;(1,1)&quot;, parameter == &quot;y2&quot; ~ &quot;(4,4)&quot;, parameter == &quot;y3&quot; ~ &quot;(4,2)&quot;, parameter == &quot;y4&quot; ~ &quot;(2,4)&quot;) ) # dist plot ggplot(rv_beta, aes(x, y, color = parameter)) + geom_line(size = 2) + labs(color = &quot;X ~ Beta&quot;, y = &quot;Density&quot;) Figure 16.10: Examples of probability density function of beta distributions. rv_beta %&gt;% group_by(parameter) %&gt;% mutate( cum_y = cumsum(y)/sum(y) ) %&gt;% ungroup() %&gt;% ggplot(aes(x, cum_y, color = parameter)) + geom_line(size = 2) + labs(color = &quot;X ~ Beta&quot;, y = &quot;y&quot;) Figure 16.11: Examples of the cumulative distribution function of beta distributions corresponding to the previous probability density functions. Probability density function \\[f(x)=\\frac{\\theta^{(a-1)} (1-\\theta)^{(b-1)}}{B(a,b)},\\] where \\(B(a,b)\\) is the beta function: \\[B(a,b)=\\int^1_0 \\theta^{(a-1)} (1-\\theta)^{(b-1)}d\\theta.\\] Cumulative distribution function \\[F(x)=\\frac{B(x;a,b)}{B(a,b)},\\] where \\(B(x;a,b)\\) is the incomplete beta function: \\[B(x;a,b)=\\int^x_0 t^{(a-1)} (1-t)^{(b-1)} dt,\\] and \\(B(a,b)\\) the (complete) beta function \\[B(a,b)=\\int^1_0 \\theta^{(a-1)} (1-\\theta)^{(b-1)}d\\theta.\\] Expected value Mean: \\(E(X)=\\frac{a}{a+b}\\) Mode: \\(\\omega=\\frac{(a-1)}{a+b-2}\\) Variance Variance: \\(Var(X)=\\frac{ab}{(a+b)^2(a+b+1)}\\) Concentration: \\(\\kappa=a+b\\) (related to variance such that, the bigger \\(a\\) and \\(b\\) are, the narrower the distribution) Reparameterization of the beta distribution Sometimes it is helpful (and more intuitive) to write the beta distribution in terms of its mode \\(\\omega\\) and concentration \\(\\kappa\\) instead of \\(a\\) and \\(b\\): \\[Beta(a,b)=Beta(\\omega(\\kappa-2)+1, (1-\\omega)(\\kappa-2)+1), \\textrm{ for } \\kappa &gt; 2.\\] A.1.6 Uniform distribution The (continuous) uniform distribution takes values within a specified range \\(a\\) and \\(b\\) that have constant probability. Sometimes the distribution is also called rectangular distribution, due to its shape of a rectangle. The uniform distribution is in particular common for random number generation. In Bayesian Data Analysis it is often used as prior distribution to express ignorance. This can be thought in the following way: When different events are possible but no (reliable) information exists about their probability of occurence, the most conservative (and also intuitive) choice would be to assign probability such that all events are equally likely to occur. The uniform distribution model this intuition, it generates a completely random number in some interval \\([a,b)\\). The distribution is specified by two parameters: the end points \\(a\\) (minimum) and \\(b\\) (maximum): \\[X \\sim Unif(a,b).\\] When \\(a=0\\) and \\(b=1\\) the distribution is referred to as standard uniform distribution. Fig.~ shows the probability density function of two uniform distributed random variables with different parameter values. Fig.~ shows the corresponding cumulative functions. rv_unif &lt;- tibble( x = seq(from = -.1, to = 4.2, by = .01), y1 = dunif(x), y2 = dunif(x, min = 2, max = 4) ) %&gt;% pivot_longer(cols = starts_with(&quot;y&quot;), names_to = &quot;parameter&quot;, values_to = &quot;y&quot;) %&gt;% mutate( parameter = ifelse(parameter == &quot;y1&quot;, &quot;(0,1)&quot;, &quot;(2,4)&quot;) ) # dist plot ggplot(rv_unif, aes(x, y, color = parameter)) + geom_line(size = 2) + labs(color = &quot;X ~ Uniform&quot;, y = &quot;Density&quot;) Figure 16.12: Examples of probability density function of uniform distributions. rv_unif %&gt;% group_by(parameter) %&gt;% mutate( cum_y = cumsum(y)/sum(y) ) %&gt;% ungroup() %&gt;% ggplot(aes(x, cum_y, color = parameter)) + geom_line(size = 2) + labs(color = &quot;X ~ Uniform&quot;, y = &quot;y&quot;) Figure 16.13: Examples of the cumulative distribution function of uniform distributions corresponding to the previous probability density functions. Probability density function \\[f(x)=\\begin{cases} \\frac{1}{b-a} &amp;\\textrm{ for } x \\in [a,b],\\\\0 &amp;\\textrm{ otherwise.}\\end{cases}\\] Cumulative distribution function \\[F(x)=\\begin{cases}0 &amp; \\textrm{ for } x&lt;a,\\\\\\frac{x-a}{b-a} &amp;\\textrm{ for } a\\leq x &lt; b,\\\\ 1 &amp;\\textrm{ for }x \\geq b. \\end{cases}\\] Expected value \\(E(X)=\\frac{a+b}{2}\\) Variance \\(Var(X)=\\frac{(b-a)^2}{12}\\) References "],
["selected-discrete-distributions-of-random-variables.html", "A.2 Selected discrete distributions of random variables", " A.2 Selected discrete distributions of random variables A.2.1 Binomial distribution The binomial distribution is a useful model for binary decisions where the outcome is a choice between two alternatives (e.g. Yes/No, Left/Right, Present/Absent, Head/Tail, …). The two outcomes are coded as \\(0\\) (failure) and \\(1\\) (success). Consequently, let the probability of occurence of the outcome “success” be \\(p\\), then the probability of occurence of “failure” is \\(1-p\\). Consider a coin-flip experiment, with the outcomes “head” and “tail”. If we flip a coin repeatedly, e.g. \\(30\\) times, the successive trials are independent of each other and the probability \\(p\\) is constant, then the resulting binomial distribution is a discrete random variable with outcomes \\(\\{0,1,2,...,30\\}\\). The binomial distribution has two parameters “size” and “prob”, often denoted as \\(n\\) and \\(p\\), respectively. The “size” refers to the number of trials and “prob” to the probability of success: \\[X \\sim Binomial(n,p).\\] Fig.~ shows the probability mass function of three binomial distributed random variables with different parameter values. As stated above, \\(p\\) refers to the probability of success. The higher this probability the more often we will observe the outcome coded with “1”. Therefore the distribution tends toward the right side and vice-versa. The distribution gets more symmetrical if the parameter \\(p\\) approximates 0.5. Fig.~ shows the corresponding cumulative functions. # how many trials trials = 30 rv_binom &lt;- tibble( x = seq(0, trials), y1 = dbinom(x, size = trials, p = 0.2), y2 = dbinom(x, size = trials, p = 0.5), y3 = dbinom(x, size = trials, p = 0.8) ) %&gt;% pivot_longer(cols = starts_with(&quot;y&quot;), names_to = &quot;parameter&quot;, values_to = &quot;y&quot;) %&gt;% mutate( parameter = case_when(parameter == &quot;y1&quot; ~ &quot;(n,0.2)&quot;, parameter == &quot;y2&quot; ~ &quot;(n,0.5)&quot;, parameter == &quot;y3&quot; ~ &quot;(n,0.8)&quot;) ) # dist plot ggplot(rv_binom, aes(x, y, fill = parameter)) + geom_col(position = &quot;identity&quot;, alpha = 0.8) + labs(fill = &quot;X ~ Binomial&quot;, y = &quot;Probability&quot;) Figure 16.14: Examples of probability mass function of Binomial distributions. rv_binom %&gt;% group_by(parameter) %&gt;% mutate( cum_y = cumsum(y)/sum(y) ) %&gt;% ungroup() %&gt;% ggplot(aes(x, cum_y, color = parameter)) + geom_step(size = 2) + labs(color = &quot;X ~ Binomial&quot;, y = &quot;y&quot;) Figure 16.15: Examples of the cumulative distribution function of Binomial distributions corresponding to the previous probability mass functions. Probability mass function \\[f(x)=\\binom{n}{x}p^x(1-p)^{n-x},\\] where \\(\\binom{n}{x}\\) is the binomial coefficient. Cumulative function \\[F(x)=\\sum_{k=0}^{x}\\binom{n}{k}p^k(1-p)^{n-k}\\] Expected value \\(E(X)=n \\cdot p\\) Variance \\(Var(X)=n \\cdot p \\cdot (1-p)\\) A.2.2 Bernoulli distribution The Bernoulli distribution is a special case of the binomial distribution with \\(size = 1\\), therefore the outcome of a bernoulli random variable is either 0 or 1. Apart from that the same information holds as for the binomial distribution. As the “size” parameter is now negligible, the bernoulli distribution has only one parameter, the probability of success \\(p\\): \\[X \\sim Bern(p).\\] Fig.~ shows the probability mass function of three bernoulli distributed random variables with different parameters. Fig.~ shows the corresponding cumulative distributions. rv_bern &lt;- tibble( x = seq(from = 0, to = 1), y1 = dbern(x, prob = 0.2), y2 = dbern(x, prob = 0.5), y3 = dbern(x, prob = 0.8) ) %&gt;% pivot_longer(cols = starts_with(&quot;y&quot;), names_to = &quot;parameter&quot;, values_to = &quot;y&quot;) %&gt;% mutate( parameter = case_when(parameter == &quot;y1&quot; ~ &quot;(0.2)&quot;, parameter == &quot;y2&quot; ~ &quot;(0.5)&quot;, parameter == &quot;y3&quot; ~ &quot;(0.8)&quot;) ) # dist plot ggplot(rv_bern, aes(x, y, fill = parameter)) + geom_col(position = &quot;dodge&quot;, color = &quot;white&quot;) + labs(fill = &quot;X ~ Bernoulli&quot;, y = &quot;Probability&quot;) + scale_x_continuous(breaks = c(0.0,1.0), labels = c(&quot;0&quot;,&quot;1&quot;), limits = c(-0.5,1.5)) Figure 16.16: Examples of probability mass function of Bernoulli distributions. rv_bern %&gt;% group_by(parameter) %&gt;% mutate( cum_y = cumsum(y)/sum(y), cum_y2 = cumsum(y)/sum(y) ) %&gt;% add_column( x2 = c(1,1,1,1.5,1.5,1.5) ) %&gt;% ungroup() %&gt;% ggplot(aes(x, cum_y, color = parameter)) + geom_segment(aes(xend = x2, yend = cum_y2), size = 1.5, linetype = &quot;dashed&quot;) + geom_segment(aes(x = -0.5, y = 0,xend = 0.0, yend = 0), size = 1.5, linetype = &quot;dashed&quot;) + geom_point(aes(x, cum_y), size = 4) + labs(color = &quot;X ~ Bernoulli&quot;, y = &quot;y&quot;) + scale_x_continuous(breaks = c(0.0,1.0), labels = c(&quot;0&quot;,&quot;1&quot;), limits = c(-0.5,1.5)) Figure 16.17: Examples of the cumulative distribution function of Bernoulli distributions corresponding to the previous probability mass functions. Probability mass function \\[f(x)=\\begin{cases} p &amp;\\textrm{ if } x=1,\\\\ 1-p &amp;\\textrm{ if } x=0.\\end{cases}\\] Cumulative function \\[F(x)=\\begin{cases} 0 &amp;\\textrm{ if } x &lt; 0, \\\\ 1-p &amp;\\textrm{ if } 0 \\leq x &lt;1,\\\\1 &amp;\\textrm{ if } x \\geq 1.\\end{cases}\\] Expected value \\(E(X)=p\\) Variance \\(Var(X)=p \\cdot (1-p)\\) A.2.3 Beta-Binomial distribution The beta-binomial distribution, as the name already indicates, is a mixture of a binomial and beta distribution. Remember, a binomial distribution is useful to model a binary choice with outcomes “0” and “1”. The binomial distribution has two parameters \\(p\\), the probability of success (“1”), and \\(n\\), the number of trials. Furthermore we assume that the successive trials are independent and \\(p\\) is constant. In a beta-binomial distribution \\(p\\) is not anymore assumed to be constant (or fixed) but changes from trial to trial. Thus, a further assumption about the distribution of \\(p\\) is made and here the beta distribution comes into play: the probability \\(p\\) is assumed to be randomly drawn from a beta distribution with parameters \\(a\\) and \\(b\\). Therefore, the beta-binomial distribution has three parameters \\(n\\), \\(a\\) and \\(b\\): \\[X \\sim BetaBinom(n,a,b).\\] For large values of a and b the distribution approaches a binomial distribution. When \\(a=1\\) and \\(b=1\\) the distribution equals a discrete uniform distribution from 0 to \\(n\\). When \\(n = 1\\), the distribution equals a bernoulli distribution. Fig.~ shows the probability mass function of three beta-binomial distributed random variables with different parameter values. Fig.~ shows the corresponding cumulative distributions. # how many trials trials = 30 rv_betabinom &lt;- tibble( x = seq(from = 0, to = trials), y1 = dbbinom(x, size = trials, alpha = 4, beta = 4), y2 = dbbinom(x, size = trials, alpha = 2, beta = 4), y3 = dbbinom(x, size = trials, alpha = 1, beta = 1) ) %&gt;% pivot_longer(cols = starts_with(&quot;y&quot;), names_to = &quot;parameter&quot;, values_to = &quot;y&quot;) %&gt;% mutate( parameter = case_when(parameter == &quot;y1&quot; ~ &quot;(n,4,4)&quot;, parameter == &quot;y2&quot; ~ &quot;(n,2,4)&quot;, parameter == &quot;y3&quot; ~ &quot;(n,1,1)&quot;) ) # dist plot ggplot(rv_betabinom, aes(x, y, fill = parameter)) + geom_col(position = &quot;identity&quot;, alpha = 0.7) + labs(fill = &quot;X ~ Beta-Binomial&quot;, y = &quot;Probability&quot;) Figure 16.18: Examples of probability mass function of Beta-Binomial distributions. rv_betabinom %&gt;% group_by(parameter) %&gt;% mutate( cum_y = cumsum(y)/sum(y) ) %&gt;% ungroup() %&gt;% ggplot(aes(x, cum_y, color = parameter)) + geom_step(size = 2) + labs(color = &quot;X ~ Beta-Binomial&quot;, y = &quot;y&quot;) Figure 16.19: Examples of the cumulative distribution function of Beta-Binomial distributions corresponding to the previous probability mass functions. Probability mass function \\[f(x)=\\binom{n}{x} \\frac{B(a+x,b+n-x)}{B(a,b)},\\] where \\(\\binom{n}{x}\\) is the binomial coefficient and \\(B(x)\\) the beta function (see beta distribution). Cumulative function \\[F(x)=\\begin{cases} 0 &amp;\\textrm{ if } x&lt;0,\\\\ \\binom{n}{x} \\frac{B(a+x,b+n-x)}{B(a,b)} {}_3F_2(n,a,b) &amp;\\textrm{ if } 0 \\leq x &lt; n,\\\\ 1 &amp;\\textrm{ if } x \\geq n. \\end{cases}\\] Where \\({}_3F_2(n,a,b)\\) is the generalized hypergeometric function. Expected value \\(E(X)=n \\frac{a}{a+b}\\) Variance \\(Var(X)=n \\frac{ab}{(a+b)^2} \\frac{a+b+n}{a+b+1}\\) A.2.4 Poisson distribution A poisson distributed random variable represents the number of successes occurring in a given time interval. It gives the probability of a given number of events happening in a fixed interval of time. The poisson distribution is a limiting case of the binomial distribution when the number of trials becomes very large and the probability of success is small. For example the number of car accidents in Osnabrueck in the next month, the number of typing errors on a page, the number of interruptions generated by a CPU during T seconds, etc. Events described by a poisson distribution must fullfill the following conditions: they occur in non-overlapping intervals, they can not occur simultaneously and each event occurs at a constant rate. The poisson distribution has one parameter, the rate \\(\\lambda\\), sometimes also referred to as intensity: \\[X \\sim Po(\\lambda).\\] The parameter \\(\\lambda\\) can be thought of as the expected number of events in the time interval. Consequently, changing the rate parameter changes the probability of seeing different numbers of events in one interval. See Fig.~ for the probability mass function of three poisson distributed random variables with different parameter values. Notice, that the higher \\(\\lambda\\) the more symmetrical gets the distribution. In fact, the poisson distribution can be approximated by a normal distribution for a rate paramter \\(\\geq\\) 10. Fig.~ shows the corresponding cumulative distributions. rv_pois &lt;- tibble( x = seq(from = 0, to = 30, by = 1), y1 = dpois(x, lambda = 2), y2 = dpois(x, lambda = 8), y3 = dpois(x, lambda = 15) ) %&gt;% pivot_longer(cols = starts_with(&quot;y&quot;), names_to = &quot;parameter&quot;, values_to = &quot;y&quot;) %&gt;% mutate( parameter = case_when(parameter == &quot;y1&quot; ~ &quot;(2)&quot;, parameter == &quot;y2&quot; ~ &quot;(8)&quot;, parameter == &quot;y3&quot; ~ &quot;(15)&quot;) ) # dist plot ggplot(rv_pois, aes(x, y, fill = parameter)) + geom_col(alpha = 0.7, position = &quot;identity&quot;) + labs(fill = &quot;X ~ Poisson&quot;, y = &quot;Density&quot;) Figure 16.20: Examples of probability mass function of Poisson distributions. # cumdist plot rv_pois %&gt;% group_by(parameter) %&gt;% mutate( cum_y = cumsum(y)/sum(y) ) %&gt;% ungroup() %&gt;% ggplot(aes(x, cum_y, color = parameter)) + geom_step(size = 2) + labs(color = &quot;X ~ Poisson&quot;, y = &quot;y&quot;) Figure 16.21: Examples of the cumulative distribution function of Poisson distributions corresponding to the previous probability mass functions. Probability mass function \\[f(x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}\\] Cumulative function \\[F(x)=\\sum_{k=0}^{x}\\frac{\\lambda^k}{k!}e^{-\\lambda}\\] Expected value \\(E(X)= \\lambda\\) Variance \\(Var(X)=\\lambda\\) "],
["understanding-distributions-as-random-variables.html", "A.3 Understanding distributions as random variables", " A.3 Understanding distributions as random variables #initialize parameters a = 3 b = 1.5 n = 1e6 #create random variables ## normal distributed RVs X_norm &lt;- rnorm(n = n, mean = 1, sd = 2) Y_norm &lt;- rnorm(n = n, mean = 1.5, sd = 2.5) ## standard normal distributed RVs stdNormal1 &lt;- rnorm(n = n, mean = 0, sd = 1) stdNormal2 &lt;- rnorm(n = n, mean = 0, sd = 1) stdNormal3 &lt;- rnorm(n = n, mean = 0, sd = 1) ## chi-square distributed RV C &lt;- rchisq(n=n, df=3) ## student-t distributed RV student &lt;- rt(n=20, df=3) ## F distributed RV fisher &lt;- rf(n=n, df1=1, df2=3) ## create linear transformation of X X_lin &lt;- a*X_norm+b ## create RV A as addition of X + Y sum_xy &lt;- X_norm + Y_norm ## create chisquare distributed RV C2 &lt;- stdNormal2^2+stdNormal3^2 C3 &lt;- stdNormal1^2+stdNormal2^2+stdNormal3^2 ## create student-t distributed RV T1 &lt;- stdNormal1/sqrt(C2/1) ## create F distributed RV F1 &lt;- (C2/2)/(C3/1) ## normal ditributed RV B (equal to addition: X+Y) N_add &lt;- rnorm(n = n, mean = 1 + 1.5, sd = sqrt(2^2 + 2.5^2)) ## normal distributed RV V (equal to linear transformation: a*X+b) N_lin &lt;- rnorm(n = n, mean = a*1+b, sd = (a*2)) par(mfrow=c(1,2)) plot(density(X_norm), main=&quot;Normal RV: a*X+b&quot;, xlim=c(-20,20),xlab=&quot;RVs&quot;)+ points(density(N_lin), type = &quot;l&quot;, col=&quot;blue&quot;)+ plot(density(X_norm), main=&quot;Normal RV: X+Y&quot;, xlim=c(-20,20),xlab=&quot;RVs&quot;)+ points(density(Y_norm), type = &quot;l&quot;, lty=&quot;dotted&quot;)+ points(density(N_add), type = &quot;l&quot;, col=&quot;blue&quot;) ## integer(0) par(mfrow=c(1,2)) plot(density(stdNormal1), main=expression(&quot;X,Y,Z ~N(0,1);&quot;~X^2+Y^2+Z^2), xlim=c(-10,10),ylim=c(0,0.4),xlab=&quot;RVs&quot;)+ points(density(C3), type = &quot;l&quot;, col=&quot;blue&quot;) ## integer(0) plot(density(C), main=&quot;Chi-square distribution&quot;, xlim=c(0,15),ylim=c(0,0.4), xlab=&quot;RVs&quot;, col=&quot;red&quot;, lwd=2, lty=&quot;dashed&quot;)+ points(density(C3), type = &quot;l&quot;, col=&quot;blue&quot;) ## integer(0) par(mfrow=c(2,2)) plot(density(stdNormal1), main=&quot;Normal devided by Chi-square RVs&quot;, xlim=c(-10,10),ylim=c(0,0.4),xlab=&quot;RVs&quot;)+ points(density(C), type = &quot;l&quot;)+ points(density(T1),type=&quot;l&quot;, col=&quot;blue&quot;) ## integer(0) plot(density(T1), main=&quot;Student-t distribution&quot;, xlim=c(-10,10),ylim=c(0,0.4),xlab=&quot;RVs&quot;)+ points(density(student),type=&quot;l&quot;, col=&quot;red&quot;) ## integer(0) plot(density(stdNormal1), main=expression(&quot;X~N(0,1),&quot;~Y~&quot;~&quot;~chi^2~(2)~&quot;;&quot;~X/sqrt(Y/n)), xlim=c(-10,10),ylim=c(0,0.4),xlab=&quot;RVs&quot;)+ points(density(C2), type = &quot;l&quot;)+ points(density(T1), type = &quot;l&quot;, col=&quot;blue&quot;) ## integer(0) plot(density(stdNormal1), main=&quot;Student-t distribution&quot;, xlim=c(-10,10),ylim=c(0,0.4),xlab=&quot;RVs&quot;)+ points(density(student), type = &quot;l&quot;, col=&quot;red&quot;) ## integer(0) "],
["exponential-family-and-maximum-entropy.html", "B Exponential Family and Maximum Entropy ", " B Exponential Family and Maximum Entropy "],
["an-important-family-the-exponential-family.html", "B.1 An important family: The Exponential Family", " B.1 An important family: The Exponential Family Most common distributions used in statistical modeling are members of the exponential family. Among others: Poisson distribution, Bernoulli distribution, Normal distribution, Chi-Square distribution, and of course the Exponential distribution. In the upcoming section some of these distributions will be described in more detail. But what makes the exponential family so special? On the one hand, distributions of this family have some convenient mathematical properties which makes them attractive to use in statistical modeling. In particular for Bayesian Analysis: For example do all these distributions have a conjugate prior and the posterior distribution has a simple form. Furthermore the above example distributions are really just examples. The exponential family encompasses a wide class of distributions which makes it possible to model various cases. On the other hand, the use of distributions from the exponential family is also from a conceptional perspective attractive. Consider for example the following situation: Consider we want to infer a propability distribution subject to certain constraints. For example a coin flip experiment can have only a dichotomous outcome {0,1} and has a constant probability. Which distribution should be used in order to model this scenario? There are several possible distributions that can be used, according to which criteria should a distribution be selected? Often one attempts a conservative choice, that is to bring as little subjective information into a model as possible. Or in other terms, one goal could be to select the distribution, among all possible distributions, that is maximal ignorant and least biased given the constraints. Consequently, the question arises how “ignorance” can be measured and distributions compared according to their “information content”? This will be topic of the upcoming exursos, the key words here are “entropy”, which comes from information theory, and “Maximum Entropy Principal”. To briefly anticipate the connection between exponential family and maximum ignorance distributions: The maximum entropy principal starts with constraints that are imposed on a distribution and derives by maximizing entropy a probability density/mass function. Distributions belonging to the exponential family arise as solutions to the maximum entropy problem subject to linear constraints. In the upcoming section selected continous and discrete distributions will be described in more detail. Followed by a part which motivation is to strengthen the intuition about understanding distributions as random variables. "],
["excursos-information-entropy-and-maximum-entropy-principal.html", "B.2 Excursos: “Information Entropy” and “Maximum Entropy Principal”", " B.2 Excursos: “Information Entropy” and “Maximum Entropy Principal” B.2.1 Information Entropy Entropy is a measure of information content of an outcome of \\(X\\) such that less probable outcomes convey more information than more probable ones. Thus, entropy can be stated as a measure of uncertainty. When the goal is to find a distrbution that is as ignorant as possible, then, consequently, entropy should be maximal. Formally, entropy is defined as follows: If \\(X\\) is a discrete random variable with distribution \\(P(X=x_i)=p_i\\) then the entropy of \\(X\\) is \\[H(X)=-\\sum_{i} p_i \\log p_i.\\] If \\(X\\) is a continuous random variable with probability density \\(p(x)\\) then the differential entropy of \\(X\\) is \\[H(X)=-\\int_{-\\infty}^{+\\infty} p(x) \\log p(x) dx.\\] From which considerations is this entropy definition derived? There exist various approaches that finally come to the same answer: the above stated definition of entropy. However, the most cited derivation is Shannon’s theorem. Another and perhapse more intuitive derivation is Wallis derivation. Jaynes (2003) describes both approaches in detail. The following provides a short insight in both derivations and is taken from (Jaynes 2003). B.2.1.1 Shannon’s theorem Shannon’s approach starts by stating conditions that a measure of the \\(H_n\\) has to satisfy. It is possible to set up some kind of association between and real numbers \\(H_n\\) is a continous function of \\(p_i\\). Otherwise, an arbitrarily small change in the probability distribution would lead to a big change in the amount of uncertainty. \\(H_n\\) should correspond to common sense in that, when there are many possibilities, we are more uncertain than when there are few. This condition takes the form that in case the \\(p_i\\) are all equal, the quantity \\(h(n)\\) is a monotonic increasing function of \\(n\\). \\(H_n\\) is consistent in that, when there is more than one way of working out its value, we must get the same answer for few possible way. Under these assumptions the resulting unique measure of uncertainty of a probability distribution \\(p\\) turns out to be just the average log-probability: \\[H(p)=-\\sum_i p_i \\log(p_i).\\] (The interested reader can find a systematic derivation in (Jaynes 2003).) Accepting this interpretation of entropy, it follows that the distribution \\((p_1,...,p_n)\\) which maximizes the above equation, subject to constraints imposed by the available information, will represent the most description of what the model about the propositions \\((A_1,...,A_n)\\) (Jaynes 2003). The function \\(H\\) is called the , or the of the distribution \\(\\{p_i\\}\\). B.2.1.2 The Wallis derivation A second and perhaps more intuitive approach of deriving entropy was suggested by G. Wallis. The following description is taken from Jaynes (2003). We are given information \\(I\\), which is to be used in assigning probabilities \\(\\{p_1,...,p_m\\}\\) to \\(m\\) different probabilities. We have a total amount of probability \\[\\sum_{i=1}^{m} p_i =1\\] to allocate among them. The problem can be stated as follows. Choose some integer \\(n&gt;&gt;m\\), and imagine that we have \\(n\\) little of probabilities, each of magnitude \\(\\delta=\\frac{1}{n}\\), to distribute in an way we see fit. Suppose we were to scatter these quanta at random among the \\(m\\) choices (penny-pitch game into \\(m\\) equal boxes). If we simply toss these quanta of probability at random, so that each box has an equal probability of getting them, nobody can claim that any box is being unfairly favoured over any other. If we do this and the first box receives exactly \\(n_1\\) quanta, the second \\(n_2\\) quanta etc. we will say the random experiment has generated the probability assignment: \\[p_i=n_i\\delta=\\frac{n_i}{n}, \\textrm{ with } i=1,2,...,m.\\] The probability that this will happen is the multinomial distribution: \\[m^{-n} \\frac{n!}{n_1!\\cdot...\\cdot n_m!}.\\] Now imagine that we repeatedly scatter the \\(n\\) quanta at random among the \\(m\\) boxes. Each time we do this we examine the resulting probability assignment. If it happens to conform to the information \\(I\\), we accept it; otherwise we reject it and try again. We continue until some probability assignment \\(\\{p_1,...,p_m\\}\\) is accepted. What is the most likely probability distribution to result from this game? It is the one which maximizes \\[W=\\frac{n!}{n_1! \\cdot ... \\cdot n_m!}\\] subject whatever constraints are imposed by the information \\(I\\). We can refine this procedure by using smaller quanta, i.e. large \\(n\\). By using \\[n!\\sim \\sqrt{(2\\pi n)} \\left(\\frac{n}{e}\\right)^n,\\] and taking the logarithm from it: \\[\\log(n!) \\sim \\sqrt{(2\\pi n)}+n\\log\\left(\\frac{n}{e}\\right),\\] we have \\[\\log(n!) \\sim \\sqrt{(2\\pi n)}+n\\log(n) - n.\\] Taking furthermore, also the logarithm from \\(W\\) and substituting \\(\\log(n!)\\) by Sterlings approximation, finally gives the definition of information entropy, as derived by Shannon’s theorem: \\[\\frac{1}{n} \\log(W) \\rightarrow -\\sum_{i=1}^{m}p_i\\log(p_i)=H(p_1,...,p_m).\\] To sum it up: Entropy is a measure of uncertainty. The higher the entropy of a random variable \\(X\\) the more uncertainty it incorporates. When the goal is to find a maximal ignorance distribution, this goal can be consequently translated into a maximization problem: Find the distribution with maximal entropy subject to existing constraints. This will be topic of the next part of our excursos. B.2.2 Deriving Probability Distributions using the Maximum Entropy Principle The maximum entropy principle is a means of deriving probability distributions given certain constraints and the assumption of maximizing entropy. One technique for solving this maximization problem is the . B.2.2.1 Lagrangian multiplier technique Given a mutivariable function \\(f(x,y,...)\\) and constraints of the form \\(g(x,y,...)=c\\), where \\(g\\) is another multivariable function with the same input space as \\(f\\) and \\(c\\) is a constant. In order to minimize (or maximize) the function \\(f\\) consider the following steps, assuming \\(f\\) to be \\(f(x)\\): Introduce a new variable \\(\\lambda\\), called , and define a new function \\(\\mathcal{L}\\) with the form: \\[\\mathcal{L}(x,\\lambda)=f(x)+\\lambda (g(x)-c).\\] Set the derivative of the function \\(\\mathcal{L}\\) equal to the zero: \\[\\mathcal{L&#39;}(x,\\lambda)=0,\\] in order to find the critical points of \\(\\mathcal{L}\\). Consider each resulting solution within the limits of the made constraints and derive the resulting distribution \\(f\\), which gives the minimum (or maximum) one is searching for. For more details see (Academy 2019) B.2.2.2 Example 1: Derivation of maximum entropy pdf with no other constraints For more details see (Finlayson 2017, @keng2017) Suppose a random variable for which we have absolutely no information on its probability distribution, beside the fact that it should be a pdf and thus, integrate to 1. We ask for the following: (Reza 1994) We assume that the maximum ignorance distribution is the one with maximum entropy. It minimizes the prior information in a distribution and is therefore the most conservative choice. For the continuous case entropy, the measure of uncertainty, is defined as \\[H(x)=-\\int_{a}^{b}p(x) \\log(p(x))dx,\\] with subject to the mentioned constraint that the sum of all probabilities is one (as it is a pdf): \\[\\int_{a}^{b}p(x)dx =1.\\] Rewrite this into the form of equation gives \\[\\mathcal{L}=-\\int_{a}^{b}p(x) \\log(p(x))dx + \\lambda \\left(\\int_{a}^{b}p(x)dx-1 \\right).\\] The next step is to the Lagrangian function. To solve this, we have to use the (Keng 2017). First differentiating \\(\\mathcal{L}\\) with respect to \\(p(x)\\) \\[\\frac{\\partial \\mathcal{L}}{\\partial p(x)}=0,\\] \\[-1-\\log(p(x))+\\lambda=0,\\] \\[p(x)=e^{(\\lambda-1)}.\\] Second, the result of \\(p(x)\\) has to satisfy the stated constraint \\[\\int_{a}^{b} p(x)dx=1,\\] \\[\\int_{a}^{b} e^{1-\\lambda} dx=1.\\] Solving this equation with respect to \\(\\lambda\\) gives: \\[\\lambda=1-\\log\\left(\\frac{1}{b-a}\\right).\\] Taking both solutions together we get the following probability density function: \\[p(x)=e^{(1-\\lambda)}=e^{\\left(1-\\left(1-\\log\\left(\\frac{1}{b-a}\\right)\\right)\\right)},\\] \\[p(x)= \\frac{1}{b-a}.\\] And this is the on the interval \\([a,b]\\). Such that, the answer of the above question is: This should not be too unexpected. As it is quite intuitive that a uniform distribution is the maximal ignorance distribution (when no other constraints were made). The next example will be more exciting. B.2.2.3 Example 2: Derivation of maximum entropy pdf with given mean \\(\\mu\\) and variance \\(\\sigma^2\\) Suppose a random variable \\(X\\) with a preassigned standard deviation \\(\\sigma\\) and mean \\(\\mu\\). Again the question is: The Maximum Entropy is defined for the current case as \\[H(X)=-\\int_{-\\infty}^{\\infty} p(x) \\log p(x)dx,\\] is subject to the constraint that it should be a pdf \\[\\int_{-\\infty}^{\\infty} p(x)dx = 1,\\] and that \\(\\mu\\) and \\(\\sigma\\) are given (whereby only one constrained is needed, as the \\(\\mu\\) is already included in the definition of \\(\\sigma\\)): \\[\\int_{-\\infty}^{\\infty}(x-\\mu)^2 p(x) dx = \\sigma^2.\\] Accordingly to the above mentioned technique the formulas are summarized in form of the equation: \\[\\mathcal{L}= -\\int_{-\\infty}^{\\infty} p(x) \\log p(x)dx + \\lambda_0\\left(\\int_{-\\infty}^{\\infty} p(x)dx - 1 \\right) + \\lambda_1\\left(\\int_{-\\infty}^{\\infty}(x-\\mu)^2 p(x) dx - \\sigma^2 \\right).\\] Next, \\(\\mathcal{L}\\) will be partially differentiated with respect to \\(p(x)\\): \\[\\frac{\\partial \\mathcal{L}}{\\partial p(x)}=0,\\] \\[-(1+\\log p(x))+\\lambda_0+\\lambda_1 (x-\\mu)^2=0,\\] \\[p(x)=e^{\\lambda_0+\\lambda_1 (x-\\mu)^2-1}.\\] Further we have to make sure that the result holds for the stated constraints: \\[\\int_{-\\infty}^{\\infty} e^{\\lambda_0+\\lambda_1 (x-\\mu)^2-1}-1 dx = 1,\\] and \\[\\int_{-\\infty}^{\\infty}(x-\\mu)^2 e^{\\lambda_0+\\lambda_1 (x-\\mu)^2-1} dx = \\sigma^2.\\] For the first constraint we get \\[e^{\\lambda_0-1} \\sqrt{-\\frac{\\pi}{\\lambda_1}} = 1,\\] and for the second constraint \\[e^{\\lambda_0-1} = \\sqrt{\\frac{1}{2\\pi}} \\frac{1}{\\sigma},\\] Thus \\[\\lambda_1=\\frac{-1}{2\\sigma^2}\\] Taking all together we can write: \\[p(x)=e^{\\lambda_0+\\lambda_1 (x-\\mu)^2-1}=e^{\\lambda_0-1}e^{\\lambda_1 (x-\\mu)^2},\\] substituting the solutions for \\(e^{\\lambda_0-1}\\) and \\(\\lambda_1\\): \\[p(x)= \\sqrt{\\frac{1}{2\\pi}} \\frac{1}{\\sigma} e^{\\frac{-1}{2\\sigma^2}(x-\\mu)^2},\\] finally we can rearrange the terms a bit and get: \\[p(x)= \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp{\\left(\\frac{-1}{2}\\left(\\frac{(x-\\mu)^2}{\\sigma^2}\\right)\\right)},\\] the . To sum it up: If one is to infer a probability distribution given certain constraints, out of all distributions \\(\\{p_i\\}\\) compatible with them, one should pick the distribution \\(\\{p_i^*\\}\\) having the largest value of \\(H\\) (De Martino and De Martino 2018). In other terms, a Maximum Entropy distribution is completely undetermined by features that do not appear explicitly in the constraints subject to which it has been computed. An overview of Maximum Entropy distributions can be found on Wikipedia. References "],
["references.html", "References", " References "]
]
