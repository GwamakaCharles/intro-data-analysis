[
["index.html", "Introduction to Data Analysis Preface", " Introduction to Data Analysis Michael Franke last rendered at: 2020-01-30 06:49:14 Preface This book is the basic reading material for the course “Introduction to Data Analysis”, held at the Universtity of Osnabrück in the winter term of 2019, as part of the BSc Cognitive Science program. It introduces key concepts of data analysis from a frequentist and a Bayesian tradition. It uses R to handle, plot and analyze data. It relies on simulation to illustrate selected statistical concepts. "],
["general-introduction.html", "1 General Introduction", " 1 General Introduction This chapter lays out the learning goals of this book (Section 1.1) and describes how these goals are to be achieved (Section 1.2). Section 1.3 details which technical tools and methods are covered here, and which are not. There will be some information on the kinds of data sets we will use during the course in Section 1.4. Finally, Section 1.5 provides information about how to install the necessary tools for this course. "],
["Chap-01-00-intro-learning-goals.html", "1.1 Learning goals", " 1.1 Learning goals At the end of this course students should: feel confident to pick up any data set to explore in a hypothesis-driven manner have gained the competence to understand complex data sets, manipulate a data set, so as to plot aspects of it in ways that are useful for answering a given research question understand the general logic of statistical inference in frequentist and Bayesian approach be able to independently evaluate statistical analyses based on their adequacy for a given research question and data set be able to critically assess the adequacy of analyses commonly found in the literature Notice that this is, although a lot of hard work already, sill rather modest! It doesn’t actually say that we necessarily aim at the competence to do it or even to do it flawlessly! Our main goal is understanding, because that is the foundation of practical success and the foundation of an ability to learn more in the future. We do not teach tricks! We do not share recipes! "],
["Chap-01-00-intro-course-structure.html", "1.2 Course structure", " 1.2 Course structure The course consists of fours parts. After giving a more detailed overview of the course, Part I introduces R the main programming language that we will use. Part II covers what is often called descriptive statistics. It also gives us room to learn more about R when we massage data into shape, compute summary statistics and plot various different data types in various different ways. Part III is the main theoretical part. It covers what is often called inferential statistics. Two aspects distinguish this course from the bulk of its cousins out there. First, we use a dual-pronged approach, i.e., we are going to introduce both the frequentist and the Bayesian approach to statistical inference side by side. The motivation for this is that seeing the contrast between the two approaches will aid our understanding of either one. Second, we will use a computational approach, i.e., we foster understanding of mathematical notions with computer simulations or other variants of helpful code. Part IV covers applications of what we have learned so far. It focuses on generalized linear models, a class of models that have become the new standard for analyses of experimental data in the social and psychological sciences, but are also very useful for data exploration in other domains (such as machine learning). There are also appendices with additional information: Further useful material (textbooks, manuals, etc.) is provided in Appendix A. Appendix ?? covers the most important probability distributions used in this book. An excursion providing more information about the important Exponential Family of probability distributions and the Maximum Entropy Principle is given in Appendix ??. The data sets which reoccur throughout the book as “running examples” are succinctly summarized in Appendix ??. "],
["Chap-01-00-intro-tools-methods.html", "1.3 Tools and topics covered (and not covered) here", " 1.3 Tools and topics covered (and not covered) here The main programming language used in this course is R (R Core Team 2018). We will make heavy use of the tidyverse package (Wickham 2017), which provides a unified set of functions and conventions that deviate (sometimes: substantially) from basic R. We will also be using the probabilistic programming language WebPPL (Goodman and Stuhlmüller 2014), but only “passively” in order to quickly obtain results from probabilistic calculations that we can experiment with directly in the browser. We will not learn to write WebPPL code from scratch. We will rely on the R package brms (Bürkner 2017) for running Bayesian generalized regression models, which itself relies on the probabilistic programming language Stan (Carpenter et al. 2017). We will, however, not learn about Stan in this course. Instead of Stan we will use the package greta (Golding 2019) to write our models and do inference with them. This is because, for current learning purposes, the language in which greta formulates its models is much closer to R and so, let’s hope, easier to learn. Section 1.5 gives information about how to install these, and other, tools necessary for this course. The main topics that this course will cover are: data preparation: how to clean up, and massage a data set into shape for plotting and analysis data visualization: how to select aspects of data to visualize in informative and useful ways statistical models: what that is, and why it’s beneficial to think in terms of models, not tests statistical inference: what that is, and how it’s done in frequentist and Bayesian approaches hypothesis testing: how Frequentists and Bayesians test scientific hypotheses generalized regression: how to apply GRMs to different types of data sets There is, obviously, a lot that we will not cover in this course. We will, for instance, not dwell at any length on the specifics of algorithms for computing statistical inferences or model fits. We will also deal with the history and the philosophy of science of statistics only to the extent that it helps understand the theoretical notions and practical habits that are important in the context of this course. We will also not do heavy math. Data analysis can be quite varied, because data itself can be quite varied. We try to sample some variation, but since this is an introductory course with lots of other ground to cover, we will be slightly conservative in the kind of data that we analyze. There will, for example, not be any pictures, sounds, dates or time points in any of the material covered here. There are at least two different motivations for data analysis, and it is important to keep them apart. This course focues on data analysis for explanation, i.e., routines that help us understand reality through the inspection and massaging of empirical data. We will only glance at the alternative approach, which is data analysis for prediction, i.e., using models to predict future observations, as commonly practiced in machine learning and its applications. In sloppy slogan form, this course treats data science for scientific knowledge gain, not the engineers’ applications. References "],
["Chap-01-00-intro-data-sets.html", "1.4 Data sets covered", " 1.4 Data sets covered We want to learn how to do data analysis. This is impossible without laying hands (keys?) on several data sets. But switching from one data set to another is mentally taxing. It is also difficult to focus and really care about any-old data set. This is why this course relies on a small selection of recurring data sets that are, hopefully, generally interesting for the target audience: students of cognitive science. Appendix ?? gives an overview of the most important, recurring data sets used in this course. Most of the data sets that we will use repeatedly in this class come from various psychological experiments. To make this even more emersive, these experiments are implemented as browser-based experiments, using _magpie. This makes it possible for students of this course to do the exact experiments whose data we are analyzing (and maybe generate some more intuitions, maybe generate some hypotheses) about the very data at hand. But it also makes it possible that we will analyze ourselves. That’s why part of the exercises for this course will run additional analyses on data collected from the aspiring data analysts themselves. If you want to become an analyst, you should also have undergone analysis yourself, so to speak. "],
["Chap-01-00-intro-installation.html", "1.5 Installation", " 1.5 Installation This course relies on a few different pieces of software. Primarily, we’ll be using R, but we’ll need installations of Python and C++ in the background. There are two options for installing. The simplest method, described in Section 1.5.1 is to install VirtualBox and use our provided Ubuntu virtual machine (link provided in class), which has the required software pre-installed and tested. When using this method, you will be working in an virtualized Linux environment. Alternatively, you can go through a manual installation tailored to your own OS, as described in Section 1.5.2. Manual installation is recommended if you do not wish to use a virtualized Linux environment. The VirtualBox method can be a fall-back option if manual installation fails. Both methods of installation are detailed below. Finally, Section 1.5.3 explains how to update the R package that wraps all R packages needed for this course and provides some extra conenience functions. 1.5.1 VirtualBox Setup 1.5.1.1 Step 1. Install VirtualBox Follow the instructions here to download and install for your platform. 1.5.1.2 Step 2. Download our Ubuntu image Download the provided VirtualBox Disk Image and move it into a folder such as “VMs”. The link for the VirtualBox Disk Image is provided on StudIP. 1.5.1.3 Step 3. Create a new virtual machine and add the downloaded image Open VirtualBox and click New Give your new virtual machine a name, e.g. “IDA2019” Change the Machine Folder to the folder where you put the disk image Change the Type to “Linux” and Version to “Ubuntu (64-bit)” and click Next to proceed to memory allocation Allocate about half of your available memory and click Next to proceed to hard disk selection Choose “Use an existing virtual hard disk file” and use the file selection icon to add our provided disk image Click Create 1.5.1.4 Step 4. Give your virtual machine more processing power Select your virtual machine on the right panel and click Settings on the top Navigate to System -&gt; Processor Increase the Processor(s) to about half of what your computer can provide 1.5.1.5 Step 5. Boot your virtual machine Select your virtual machine and click Start The username of the system is “user” and the password is “password” 1.5.1.6 Step 6. Install further packages The virtual machine includes most of the packages required, but not all. First, run the following commands in ‘Terminal’: sudo apt update sudo apt install r-cran-devtools r-cran-boot r-cran-extradistr r-cran-ggsignif r-cran-naniar Then, open RStudio and run the following command in the R console: devtools::install_github(&quot;n-kall/IDA2019-package&quot;) 1.5.1.7 Troubleshooting If the virtual machine does not boot, you may need to ensure that ‘virtualization’ is enabled in your computer’s BIOS. Talk to a tutor if you’re having difficulties doing so. 1.5.2 Manual installation If you don’t want to use the virtual machine, or it doesn’t work for you, the following six steps describe how to get the main components installed manually. Depending on your operating system (e.g. macOS, Linux, Windows), you might need to follow slightly different instructions, which are specified. Depending on the exact setup of your computer, the results may vary. The virtual machine has been tested with the required software, so if you can, we recommend using that. 1.5.2.1 Step 1. Install Python Windows and macOS: We recommend installing miniconda from here Linux: You can install miniconda or you can just use the preinstalled Python (which saves time and space). Make sure you have pip installed, e.g. for Ubuntu apt install python3-pip 1.5.2.2 Step 2. Install the required Python packages We have provided files that list the required Python packages. They can be installed automatically with the following commands in the terminal. For Anaconda: Download this enviroment file conda env create -f environment.yml For Linux users who are using pip: Download this requirements file pip3 install -r requirements.txt 1.5.2.3 Step 3. Install R Windows and macOS: Download and install R from here Linux users: We need to have at least version 3.5 of R. This may be available in your distribution’s repository. e.g. if you are using a recent version of Ubuntu (18.10 or later), you can install R with apt install r-base. Otherwise, follow these instructions for your version. 1.5.2.4 Step 4. Install RStudio All platforms: Download and install the latest version of RStudio from here 1.5.2.5 Step 5. Install a C++ toolchain For the Stan language, which will be interfaced through an R package called brms, you’ll need a working C++ compiler. Windows: Download and install the latest version of RTools from here macOS: Download and install the latest version of RTools for macOS from here Note: you may need to register for an Apple Developer account (free of charge). Linux (e.g. Ubuntu): You can install a compiler and toolchain with apt install build-essential. 1.5.2.6 Step 6. Install R packages We have created an R package that, when installed, will prompt the installation of the packages we will use in this course. In this step, you’ll install this package (and automatically install its dependencies). For Windows and macOS: Open RStudio and run the following two commands in the console. install.packages(&quot;devtools&quot;) devtools::install_github(&quot;n-kall/IDA2019-package&quot;) For Linux (e.g. Ubuntu): It’s much faster (and less error-prone) if you install devtools from the app repository via terminal: apt install r-cran-devtools and continue to the second line in the R console. But this will only work if you are using a recent version of Ubuntu (18.10 or later). If you had to manually update your R to version in step 3, you’ll need to install the following from terminal: apt install libssl-dev libxml2-dev libcurl4-openssl-dev and then install via the the R console: install.packages(&quot;devtools&quot;) devtools::install_github(&quot;n-kall/IDA2019-package&quot;) 1.5.3 Updating the course package Occasionally, we might have to add packages or functionality as we go through this course. In that case, you will have to update the package IDA2019-package that ships all of this for this course. To update, use: devtools::install_github(&quot;n-kall/IDA2019-package&quot;) "],
["Chap-01-01-R.html", "2 Basics of R", " 2 Basics of R R is a specialized programming language for data science. Though old, it is heavily supported by an active community. New tools for data handling, visualization, and statistical analysis are provided in the form of packages.1 While other programming languages specialized for scientific computing, like Python of Julia, also lend themselves beautifully for data analysis, the choice of R in this course is motivated because R’s raison d’être is data analysis. Some of the R packages that this course will use provide cutting-edge methods which are not as conveniently available in other programming languages (yet). In a manner of speaking, there are two flavors of R. We should distinguish base R from the tidyverse. Base R is what you have when you do not load any packages. We enter the tidyverse by loading the package tidyverse (see below for information on how to do that). The tidyverse consists of several components (which are actually stand-alone packages that can be loaded separately if needed) all of which supply extra functionality for data analysis, based on a unifying philosophy and representation format. While eventually interchangable, the look-and-feel of base R and the tidyverse is quite different. Figure 2.1 lists a selection of packages from the tidyverse in relation to their role at different stages of the process of data analysis. The image is taken from this introduction to the tidyverse. Figure 2.1: Overview of selected packages from the tidyverse. The course will also introduce Rmarkdown in Section 2.6. Rmarkdown is a nice way of documenting your data analyes in a reproducible form. Participants will use Rmarkdown to prepare their homework assignments. Make sure to have completely installed everything of relevance for this course, as described in Section 1.5. Unless you have strong opinions or an unassailable favorite, we recommend trying RStudio as an IDE for R. The official documentation for base R is An Introduction to R. The standard reference for using the tidyverse is R for Data Science (R4DS). There are some very useful cheat sheets which you should definitely check out! There are pointers to further material in Appendix ??. The learning goals for this chapter are: become familiar with R, its syntax and basic notions become familiar with the key functionality from the tidyverse understand and write simple R scripts be able to write documents in Rmarkdown Packages live in the official package repository CRAN, or are supplied in less standardized forms, e.g., via open repositories, such as GitHub.↩ "],
["ch1-first-steps.html", "2.1 First steps", " 2.1 First steps R is an interpreted language. This means that you do not have to compile it. You can just evaluate it line by line, in a so-called session. The session stores the current values of all variables. If you do not want to retype, you can store your code in a script.2 Try this out by either typing r to open an R session in a terminal or load RStudio.3 You can immediately calculate stuff: 6 * 7 ## [1] 42 2.1.1 Functions R has many built-in functions. The most common situation is that the function is called by its name using prefix notation, followed by round brackets which enclose the function’s arguments (separated by commata if multiple). For example, the function round takes a number and, per default, returns the closest integer: # the function `round` takes a number as argument and # returns the closest integer (default) round(0.6) ## [1] 1 Actually, round allows several arguments. It takes as input the number x to be rounded, and another integer number digits which gives the number of digits after the comma to which x should be rounded. We can then specify these arguments in a function call of round by providing the named arguments. # rounds the number `x` to the number `digits` of digits round(x = 0.138, digits = 2) ## [1] 0.14 When providing all arguments with names, the order of arguments does not matter. When providing at least one non-named argument, all non-named arguments have to be presented in the right order (as expected by the function; to find out what that is use help, as explained below in 2.1.6) after subtracting the named arguments from the ordered list of arguments. round(x = 0.138, digits = 2) # works as intended round(digits = 2, x = 0.138) # works as intended round(0.138, digits = 2) # works as intended round(0.138, 2) # works as intended round(x = 0.138, 2) # works as intended round(digits = 2, 0.138) # works as intended round(2, x = 0.138) # works as intended round(2, 0.138) # does not work as intended (returns 2) Functions can have default values for some or all of their arguments. In the case of round the default is digits = 0. There is obviously no default for x in the function round. Some functions can take an arbitrary number of arguments. The function sum, which sums up numbers is a point in case. # adds all of its arguments together sum(1,2,3) ## [1] 6 Selected functions can also be called in infix notation. This applies to frequently recurring operations, such as mathematical operations or logical comparisons. # both of these calls sum 1, 2, and 3 together sum(1,2,3) # prefix notation 1 + 2 + 3 # prefix notation Section 2.3 will list some of the most important built-in functions. It will also explain how to define your own functions. 2.1.2 Variables You can assign values to variables using three assignment operators: -&gt;, &lt;- and =, like so: x &lt;- 6 # assigns 6 to variable x 7 -&gt; y # assigns 7 to variable y z = 3 # assigns 3 to variable z x * y / z # returns 6 * 7 / 3 = 14 ## [1] 14 Use of = is discouraged.4 It is good practice to use a consistent naming scheme for variables. This book uses snake_case_variable_names and tends towards using long_and_excessively_informative_names for important variables, and short variable names, like i, j or x, for local variables, indices etc. 2.1.3 Literate coding It is good practice to document code with short but informative comments. Comments in R are demarcated with #. x &lt;- 4711 # a nice number from Cologne Since everything on a line after an occurrence of # is treated as a comment, it is possible to break long function calls across several lines, and to add comments to each line: round( # call the function `round` x = 0.138, # number to be rounded digits = 2 # number of after-comma digits to round to ) In RStudio, you can use Command+Shift+C (on Mac) and Ctrl+Shift+C (on Windows/Linux) to comment or uncomment code, and you can use comments to structure your scripts. Any comment followed by ---- is treated as a (foldable) section. # SECTION: variable assignments ---- x &lt;- 6 y &lt;- 7 # SECTION: some calculations ---- x * y 2.1.4 Objects Strictly speaking, all entities in R are objects but that is not always apparent or important for everyday practical purposes see the manual for more information. R supports an object-oriented programming style, but we will not make (explicit) use of this functionality. In fact, this course heavily uses and encourages a functional programming style (see Section 2.4). Some functions (e.g., optimizers or fitting functions for statistical models) return objects, however, and we will use this output in various ways. For example, if we run a linear regression model on some data set, the output is an object. # you do not need to understand this code model_fit = lm(formula = speed~dist, data = cars) # just notice that the function `lm` returns an object is.object(model_fit) ## [1] TRUE # printing an object on the screen usually gives you summary information print(model_fit) ## ## Call: ## lm(formula = speed ~ dist, data = cars) ## ## Coefficients: ## (Intercept) dist ## 8.2839 0.1656 2.1.5 Packages Much of R’s charm unfolds through the use of packages. CRAN has the official package repository. To install a new package from a CRAN mirror use the install.packages function. For example, to install the package devtools, you would use: install.packages(&quot;devtools&quot;) Once installed, you need to load your desired packages for each fresh session, using: library(devtools) Once loaded all functions, data etc. that ship with a package are available without additional reference to the package name. If you want to be careful or curteous to an admirer of your code, you can reference the package a function comes from explicitly. For example, the following code calls the function install_github from the package devtools explicitly (so that you would not need to load the package beforehand, for example): devtools::install_github(&quot;SOME-URL&quot;) Indeed, the install_github function allows you to install bleeding-edge packages from github. You can install all of the relevant packages using (after installing the devtools package, as described in Section 1.5: devtools::install_github(&quot;n-kall/IDA2019-package&quot;) After this installation, you can load all packages for this course simply by using: library(IDA2019) In RStudio, there is a special tab in the pane with informtion on “files”, “plots” etc. to show all installed packages. This also shows which packages are currently loaded. 2.1.6 Getting help If you encounter a function like lm that you do not know about, you can access its documentation with the help function or just typing ?lm. Ror example, the following call summons the documentation for lm, the first parts are shown in Figure 2.2. help(lm) knitr::include_graphics(&quot;visuals/R-doc-example.png&quot;) Figure 2.2: Excerpt from the documentation of the lm function. If you are looking for help on a more general topic, use the function help.search. It takes a regular expression as input and outputs a list of occurrences in the available documentation. A useful shortcut for help.search is just to type ?? followed by the (unquoted) string to search for. For example, calling either of the following lines might produce a display like in Figure 2.3. # two equivalent ways for obtaining help on search term &#39;linear&#39; help.search(&quot;linear&quot;) ??linear knitr::include_graphics(&quot;visuals/R-doc-search-example.png&quot;) Figure 2.3: Result of calling help.search for the term ‘linear’. The top entries in Figure 2.3 are vignettes. These are compact manuals or tutorial on particular topics or functions, and they are directly available in R. If you want to browse through the vignettes available on your machine (which depend on which packages you have installed), go ahead: browseVignettes() Line-by-line execution of code is useful for quick development and debugging. Make sure to learn about keyboard shortcuts to execute single lines or chunks of code in your favorite editor, e.g., check the RStudio Cheat Sheet for information on its keyboard shortcuts.↩ When starting a session in a terminal, you can exit a running R session by typing quit() or q().↩ You can produce &lt;- in RStudio with Option-- (on Mac) and Alt-- (on Windows/Linux). For other useful keyboard shortcuts, see here.↩ "],
["ch1-data-types.html", "2.2 Data types", " 2.2 Data types Let’s briefly go through the data types that are most important for our later purposes. We can assess the type of an object stored in variable x with the function typeof(x). typeof(3) # returns type &quot;double&quot; typeof(TRUE) # returns type &quot;logical&quot; typeof(cars) # returns &#39;list&#39; (includes data.frames, tibbles, objects, ...) typeof(&quot;huhu&quot;) # return &#39;character&quot; (= string) typeof(mean) # return &#39;closure&quot; (= function) typeof(c) # return &#39;builtin&quot; (= deep system internal stuff) typeof(round) # returns type &quot;special&quot; (= well, special stuff?) To learn more about an object, it can help to just print it out as a string: str(lm) ## function (formula, data, subset, weights, na.action, method = &quot;qr&quot;, model = TRUE, ## x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, contrasts = NULL, ## offset, ...) It is sometimes possible to cast objects of one type into another type XXX using functions as.XXX in base R or as_XXX in the tidyverse. # casting Boolean value `TRUE` into number format as.numeric(TRUE) # returns 1 ## [1] 1 R is essentially an array-based language. Arrays are arbitrary but finite dimensional matrices. We will discuss what is usually referred to as vectors (= one-dimensional arrays), matrices (= two-dimensional arrays) and arrays (= more-than-two-dimensional) in the following section on numeric information. But it is important to keep in mind that arrays can contain objects of other types than numeric information (as long as all objects in the array are of the same type). 2.2.1 Numeric vectors &amp; matrices 2.2.1.1 Numeric information Standard number format in R is double. typeof(3) ## [1] &quot;double&quot; We can also represent numbers as integers and complex. typeof(as.integer(3)) # returns &#39;integer&#39; ## [1] &quot;integer&quot; typeof(as.complex(3)) # returns &#39;complex&#39; ## [1] &quot;complex&quot; 2.2.1.2 Numeric vectors As a generally useful heuristic, expect every numerical information to be treated as a vector (or higher-order: matrix, array, … ; see below), and to expect any (basic, mathematical) operation in R to (most likely) apply to the whole vector, matrix, array, collection.5 This makes it possible to ask for the length of a variable to which we assing a single number, for instance: x &lt;- 7 length(x) ## [1] 1 We can even index such a variable: x &lt;- 7 x[1] # what is the entry in position 1 of the vector x? ## [1] 7 Or assign a new value to a hitherto unused index: x[3] &lt;- 6 # assign the value 6 to the 3rd entry of vector x x # notice that the 2nd entry is undefined, or &quot;NA&quot;, not available ## [1] 7 NA 6 Vectors in general can be declared with the built-in function c(). To memorize this, think of concatenation or combination. x &lt;- c(4, 7, 1, 1) # this is now a 4-place vector x ## [1] 4 7 1 1 There are also helpful functions to generate sequences of numbers: 1:10 # returns 1, 2, 3, ..., 10 seq(from = 1, to = 10, by = 1) # returns 1, 2, 3, ..., 10 seq(from = 1, to = 10, by = 0.5) # returns 1, 1.5, 2, ..., 9.5, 10 seq(from = 0, to = 1 , length.out = 11) # returns 0, 0.1, ..., 0.9, 1 Indexing in R starts with 1, not 0! x &lt;- c(4, 7, 1, 1) # this is now a 4-place vector x[2] ## [1] 7 And now we see what is meant above when we said that (almost) every mathematical operation can be expected to apply to a vector: x &lt;- c(4, 7, 1, 1) # 4-placed vector as before x + 1 ## [1] 5 8 2 2 2.2.1.3 Numeric matrices Matrices are declared with the function matrix. This function takes, for instance, a vector as an argument. x &lt;- c(4, 7, 1, 1) # 4-placed vector as before (m &lt;- matrix(x)) # cast x into matrix format ## [,1] ## [1,] 4 ## [2,] 7 ## [3,] 1 ## [4,] 1 Notice that the result is a matrix with a single column. This is important. R uses so-called column-major mode.6 This means that it will fill columns first. For example, a matrix with three columns based on a six-placed vector \\(1, 2, \\dots, 6\\) will be built by filling the first column from top to bottom, then the second column top to bottom, and so on.7 m &lt;- matrix(1:6, ncol = 3) m ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 In line with column-major mode, vectors are treated as column vectors in matrix operations: x = c(1,0,1) # 3-place vector m %*% x # dot product with previous matrix &#39;m&#39; ## [,1] ## [1,] 6 ## [2,] 8 As usual, and independently of column- or row-major mode, matrix indexing starts with the row index: m[1,] # produces first row of matrix &#39;m&#39; ## [1] 1 3 5 2.2.1.4 Arrays Arrays are simply higher-dimensional matrices. We will not make use of arrays in this course. 2.2.1.5 Names for vectors, matrices and arrays The positions in a vector can be given names. This is extremely useful for good “literate coding” and therefore highly recommended. The names of vector x’s positions are retrieved and set by the names function: students &lt;- c(&quot;Jax&quot;, &quot;Jamie&quot;, &quot;Jason&quot;) # names of students grades &lt;- c(1.3, 2.7, 2.0) # a vector of grades names(grades) # retrieve names: with no nanmes so far ## NULL names(grades) &lt;- students # assign names names(grades) # retrieve names again: names assigned ## [1] &quot;Jax&quot; &quot;Jamie&quot; &quot;Jason&quot; grades # output shows names ## Jax Jamie Jason ## 1.3 2.7 2.0 We can also set the names of a vector directly during construction:8 # names of students (this is a character vector, see below) students &lt;- c(&quot;Jax&quot;, &quot;Jamie&quot;, &quot;Jason&quot;) # constructing a vector with names directly assigned grades &lt;- c(1.3, 2.7, 2.0, names = students) Names for matrices are retrieved or set with functions rownames and colnames. # declare matrix m &lt;- matrix(1:6, ncol = 3) # assign row and column names, using function # `str_c` which is described below rownames(m) &lt;- str_c(&quot;row&quot;, 1:nrow(m), sep = &quot;_&quot;) colnames(m) &lt;- str_c(&quot;col&quot;, 1:ncol(m), sep = &quot;_&quot;) m ## col_1 col_2 col_3 ## row_1 1 3 5 ## row_2 2 4 6 2.2.2 Booleans There are built-in names for Boolean values “true” and “false”, predictably named TRUE and FALSE. Equivalent shortcuts are T and F. If we attempt to do math with Boolean vectors, the outcome is what any reasonable logician would expect: x &lt;- c(T,F,T) 1 - x ## [1] 0 1 0 x + 3 ## [1] 4 3 4 Boolean vectors can be used as index sets to extract elements from other vectors. # vector 1, 2, ..., 5 number_vector &lt;- 1:5 # index of odd numbers set to `TRUE` boolean_vector &lt;- c(T,F,T,F,T) # returns the elemnts from number vector, for which # the corresponding element in the Boolean vector is true number_vector[boolean_vector] ## [1] 1 3 5 2.2.3 Special values There are a couple of keywords reserved in R for special kinds of objects: NA: “not availables”; represents missing values in data NaN: “not a number”; e.g., division zero by zero Inf or -Inf: infinity and negative infinity; returned when number is too big or devision by zero NULL: the NULL object; often returned when function is undefined for input 2.2.4 Characters (= strings) Strings are called characters in R. We will be stubborn and call them strings for most of the time here. We can assign a string value to a variable by putting the string in double quotes: x &lt;- &quot;huhu&quot; typeof(x) ## [1] &quot;character&quot; We can create vectors of characters in the obvious way: chr_vector &lt;- c(&quot;huhu&quot;, &quot;hello&quot;, &quot;huhu&quot;, &quot;ciao&quot;) chr_vector ## [1] &quot;huhu&quot; &quot;hello&quot; &quot;huhu&quot; &quot;ciao&quot; The package stringr from the tidyverse also provides very useful and, in comparison to base R, more uniform functions for string manipulation. The cheat sheet for the stingr package is highly recommended for a quick overview. Below are some examples. Function str_c concatenates strings: str_c(&quot;Hello&quot;, &quot;Hi&quot;, &quot;Hey&quot;, sep = &quot;! &quot;) ## [1] &quot;Hello! Hi! Hey&quot; We can find the indeces of matches in a character vector with str_which: chr_vector &lt;- c(&quot;huhu&quot;, &quot;hello&quot;, &quot;huhu&quot;, &quot;ciao&quot;) str_which(chr_vector, &quot;hu&quot;) ## [1] 1 3 Similarly, str_detect gives a Boolean vector of matching: chr_vector &lt;- c(&quot;huhu&quot;, &quot;hello&quot;, &quot;huhu&quot;, &quot;ciao&quot;) str_detect(chr_vector, &quot;hu&quot;) ## [1] TRUE FALSE TRUE FALSE If we want to get the strings matching a pattern, we can use str_subset: chr_vector &lt;- c(&quot;huhu&quot;, &quot;hello&quot;, &quot;huhu&quot;, &quot;ciao&quot;) str_subset(chr_vector, &quot;hu&quot;) ## [1] &quot;huhu&quot; &quot;huhu&quot; Replacing all matches with another string works with str_replace_all: chr_vector &lt;- c(&quot;huhu&quot;, &quot;hello&quot;, &quot;huhu&quot;, &quot;ciao&quot;) str_replace_all(chr_vector, &quot;h&quot;, &quot;B&quot;) ## [1] &quot;BuBu&quot; &quot;Bello&quot; &quot;BuBu&quot; &quot;ciao&quot; For data preparation we often need to split strings by a particular character. For instance, a set of reaction times could be separated by a character line “|”. We can split this string representation to get individual measurements like so: # three measures of reaction time in a single string reaction_times &lt;- &quot;123|234|345&quot; # notice that we need to doubly (!) escape character | # notice also that the results is a list (see below) str_split(reaction_times, &quot;\\\\|&quot;, n = 3) ## [[1]] ## [1] &quot;123&quot; &quot;234&quot; &quot;345&quot; 2.2.5 Factors Factors are special vectors, which treat its elements as ordered or unorderd categories. This is useful for representing data from experiments, e.g., of categorical or ordinal variables (see Chapter ??). To create a factor, we can use the function factor. The following code creates an unorderd factor: chr_vector &lt;- c(&quot;huhu&quot;, &quot;hello&quot;, &quot;huhu&quot;, &quot;ciao&quot;) factor(chr_vector) ## [1] huhu hello huhu ciao ## Levels: ciao hello huhu Ordered factors also register the order of the categories: chr_vector &lt;- c(&quot;huhu&quot;, &quot;hello&quot;, &quot;huhu&quot;, &quot;ciao&quot;) factor( chr_vector, # the vector to treat as factor ordered = T, # make sure its treated as ordered factor levels = c(&quot;huhu&quot;, &quot;ciao&quot;, &quot;hello&quot;) # specify order of levels ) ## [1] huhu hello huhu ciao ## Levels: huhu &lt; ciao &lt; hello We will see that ordered factors are important, for example, in plotting when they determine the order in which different parts of data are arranged on the screen. They are also important for statistical analysis, because they help determine how categories are compared to one another. Factors are trickier to work with than mere lists, because they are rigid about the represented factor levels. Adding an item that does not belong to any of a factor’s levels, leads to trouble: chr_vector &lt;- c(&quot;huhu&quot;, &quot;hello&quot;, &quot;huhu&quot;, &quot;ciao&quot;) my_factor &lt;- factor( chr_vector, # the vector to treat as factor ordered = T, # make sure its treated as ordered factor levels = c(&quot;huhu&quot;, &quot;ciao&quot;, &quot;hello&quot;) # specify order of levels ) my_factor[5] &lt;- &quot;huhu&quot; # adding a &quot;known category&quot; is okay my_factor[6] &lt;- &quot;moin&quot; # adding an &quot;unknown category&quot; does not work my_factor ## [1] huhu hello huhu ciao huhu &lt;NA&gt; ## Levels: huhu &lt; ciao &lt; hello The forcats package from the tidyverse helps dealing with factors. You should check the Cheat Sheet for more helpful functionality. Here is an example of how to expand the levels of a factor: chr_vector &lt;- c(&quot;huhu&quot;, &quot;hello&quot;, &quot;huhu&quot;, &quot;ciao&quot;) my_factor &lt;- factor( chr_vector, # the vector to treat as factor ordered = T, # make sure its treated as ordered factor levels = c(&quot;huhu&quot;, &quot;ciao&quot;, &quot;hello&quot;) # specify order of levels ) my_factor[5] &lt;- &quot;huhu&quot; # adding a &quot;known category&quot; is okay my_factor &lt;- fct_expand(my_factor, &quot;moin&quot;) # add new category my_factor[6] &lt;- &quot;moin&quot; # adding new item now works my_factor ## [1] huhu hello huhu ciao huhu moin ## Levels: huhu &lt; ciao &lt; hello &lt; moin It is sometimes useful (especially for plotting) to flexibly reorder the levels of an ordered factor. Here are some useful functions from the forcats package: my_factor # original factor ## [1] huhu hello huhu ciao huhu moin ## Levels: huhu &lt; ciao &lt; hello &lt; moin fct_rev(my_factor) # reverse level order ## [1] huhu hello huhu ciao huhu moin ## Levels: moin &lt; hello &lt; ciao &lt; huhu fct_relevel( # manually supply new level order my_factor, c(&quot;hello&quot;, &quot;ciao&quot;, &quot;huhu&quot;) ) ## [1] huhu hello huhu ciao huhu moin ## Levels: hello &lt; ciao &lt; huhu &lt; moin 2.2.6 Lists, data frames &amp; tibbles Lists are key-value pairs. They are created with the built-in function list. The difference between a list and a named vector is that in the latter all elements must be of the same type. In a list, the elements can be of arbitraty type. They can also be vectors or even lists themselves. For example: my_list &lt;- list( single_number = 42, chr_vector = c(&quot;huhu&quot;, &quot;ciao&quot;), nested_list = list(x = 1, y = 2, z = 3) ) my_list ## $single_number ## [1] 42 ## ## $chr_vector ## [1] &quot;huhu&quot; &quot;ciao&quot; ## ## $nested_list ## $nested_list$x ## [1] 1 ## ## $nested_list$y ## [1] 2 ## ## $nested_list$z ## [1] 3 To access a list element by its name (=key), we can use the $ sign followed by the unquoted name, double square brackets [[ &quot;name&quot; ]] with the quoted name inside, or indices in double brackets, like so: # all of these return the same list element my_list$chr_vector ## [1] &quot;huhu&quot; &quot;ciao&quot; my_list[[&quot;chr_vector&quot;]] ## [1] &quot;huhu&quot; &quot;ciao&quot; my_list[[2]] ## [1] &quot;huhu&quot; &quot;ciao&quot; Lists are very important in R because almost all structured data that belongs together is stored as lists. Objects are special kinds of lists. Data is stored in special kinds of lists, so-called data frames or so-called tibbles. A data frame is base R’s standard format to store data in. A data frame is a list of vectors of equal length. Data sets are instantiated with the function data.frame: # fake experimental data exp_data &lt;- data.frame( trial = 1:5, condition = factor( c(&quot;C1&quot;, &quot;C2&quot;, &quot;C1&quot;, &quot;C3&quot;, &quot;C2&quot;), ordered = T ), response = c(121, 133, 119, 102, 156) ) exp_data ## trial condition response ## 1 1 C1 121 ## 2 2 C2 133 ## 3 3 C1 119 ## 4 4 C3 102 ## 5 5 C2 156 We can access columns of a data frame, just like we access elements in a list. Additionally, we can also use index notation, like in a matrix: # gives the value of the cell in row 2, column 3 exp_data[2,3] # return 133 ## [1] 133 In RStudio, you can inspect data in data frames (and tibbles (see below)) with the function View. Tibbles are the tidyverse counterpart of data frames. We can cast a data frame into a tibble, using as_tibble. as_tibble(exp_data) ## # A tibble: 5 x 3 ## trial condition response ## &lt;int&gt; &lt;ord&gt; &lt;dbl&gt; ## 1 1 C1 121 ## 2 2 C2 133 ## 3 3 C1 119 ## 4 4 C3 102 ## 5 5 C2 156 But we can also create a tibble directly with the keyword tibble. Indeed, creation of tibbles is conveniently more flexible than the creation of data frames: the former allow dynamic look-up of previously defined elements. my_tibble &lt;- tibble(x = 1:10, y = x^2) # dynamic construction possible my_dataframe &lt;- data.frame(x = 1:10, y = x^2) # ERROR :/ Another important difference between data frames and tibbles concerns default treatment of character (=string) vectors. When reading in data from a CSV file as a data frame (using function read.csv) each character vector is treated as a factor per default. But when using read_csv to read CSV data into a tibble character vector are not treated as factors. If you are familiar with Python’s scipy and numpy packages, this is R’s default mode of treating numerical information.↩ Python, on the other hand, uses the reverse row-major mode.↩ It is in this sense that the “first index moves fastest” in column-major mode, which is another frequently given explanation of column-major mode.↩ Notice that we can create strings (actually called ‘characters’ in R) with double quotes↩ "],
["Chap-01-01-functions.html", "2.3 Functions", " 2.3 Functions 2.3.1 Some important built-in functions Many helpful functions are defined in base R or supplied by packages. We recommend browsing the Cheat Sheets every now and then to pick up more useful stuff for your inventory. Here are some functions that are very basic and generally useful. 2.3.1.1 Standard logic &amp;: “and” |: “or” !: “not” negate(): a pipe-friendly ! (see Section 2.5 for more on piping) all(): returns true of a vector if all elements are T any(): returns true of a vector if at least on element is T 2.3.1.2 Comparisons &lt;: smaller &gt;: greater ==: equal (you can also use near()instead of == e.g. near(3/3,1)returns TRUE) &gt;=: greater or equal &lt;=: less or equal !=: not equal 2.3.1.3 Set theory %in%: wheter an element is in a vector union(x,y): union of x and y intersect(x,y): intersection of x and y setdiff(x,y): all elements in x that are not in y 2.3.1.4 Sampling and combinatorics runif(): random number from unit interval [0;1] sample(x, size, replace): take size samples from x (with replacement if replace is T) choose(n,k): number of subsets of size n out of a set of size k (binomial coefficient) 2.3.2 Defining your own functions If you find yourself in a situation in which you would like to copy-paste some code, possibly with minor amendments, this usually means that you should wrap some recurring operations into a custom-defined function. There are two ways of defining your own functions: as a named function, or an anonymous function. 2.3.2.1 Named functions The special operator supplied by base R to create new functions is the keyword function. Here is an example of defining a new function with two input variables x and y that returns a computation based on these numbers. We assign this newly created function to the variable cool_function, so that we can use this name to call the function later. Notice that the use of the return keyword is optional here. If it is left out, the evaluation of the last line is returned. # define a new function # takes two numbers x &amp; y as argument # return x * y + 1 cool_function &lt;- function(x, y) { return(x * y + 1) } # apply `cool_function` to some numbers: cool_function(3,3) # return 10 cool_function(1,1) # return 2 cool_function(1:2,1) # returns vector [2,3] cool_function(1) # throws error: &#39;argument &quot;y&quot; is missing, with no default&#39; cool_function() # throws error: &#39;argument &quot;x&quot; is missing, with no default&#39; We can give default values for the parameters passed to a function: # same function as before but with # default values for each argument cool_function_2 &lt;- function(x = 2, y = 3) { return(x * y + 1) } # apply `cool_function_2` to some numbers: cool_function_2(3,3) # return 10 cool_function_2(1,1) # return 2 cool_function_2(1:2,1) # returns vector [2,3] cool_function_2(1) # returns 4 (= 1 * 3 + 1) cool_function_2() # returns 7 (= 2 * 3 + 1) 2.3.2.2 Anonymous functions Notice that we can feed functions as parameters to other functions. This is an important ingredient of a functional-stlye of programming, and something that we will rely on heavily in this course (see Section 2.4). When supplying a function as an argument to another function, we might not want to name the function that is passed. Here’s a (stupid, but hopefully illustrating) example: # define a function that takes a function as argument new_applier_function &lt;- function(input, function_to_apply) { return(function_to_apply(input)) } # sum vector with built-in &amp; named function new_applier_function( input = 1:2, # input vector function_to_apply = sum # built-in &amp; named function to apply ) # returns 3 # sum vector with anonymous function new_applier_function( input = 1:2, # input vector function_to_apply = function(input) { return(input[1] + input[2]) } ) # returns 3 as well "],
["ch-01-01-loops-and-maps.html", "2.4 Loops and maps", " 2.4 Loops and maps For iteratively performing computation steps, R has a special syntax for for loops. Here is an example of a (stupid, but illustrative) example of a for loop in R: # fix a vector to transform input_vector &lt;- 1:6 # create output vector for memory allocation output_vector &lt;- integer(length(input_vector)) # iterate over length of input for (i in 1:length(input_vector)) { # multiply by 10 if even if (input_vector[i] %% 2 == 0) { output_vector[i] = input_vector[i] * 10 } else { output_vector[i] = input_vector[i] } } output_vector ## [1] 1 20 3 40 5 60 R also provides functional iterators (e.g., apply), but we will use the functional iterators from the purrr package. The main functional operator from purrr is map which takes a vector and a function, applies the function to each element in the vector and returns a list with the outcome. There are also versions of map, written as map_dbl (double), map_lgl (logical) or map_df (data frame), which return a vector of doubles, Booleans or a data frame. Here is a first example of how this code looks in a functional style using the functional iterator map_dbl: map_dbl( input_vector, function(i) { if (input_vector[i] %% 2 == 0) { return (input_vector[i] * 10 ) } else { return (input_vector[i]) } } ) ## [1] 1 20 3 40 5 60 We can write this even shorter, using purrr’s short-hand notation for functions: map_dbl( input_vector, ~ ifelse( .x %% 2 == 0, .x * 10, .x) ) ## [1] 1 20 3 40 5 60 The trailing ~ indicates that we define an anonymous function. It therefore replaces the usual function(...) call which indicates which arguments the anonymous function expects. To make up for this, after the ~ we can use .x for the first (and only) argument of our anonymous function. To apply a function to more than one input vector, element per element, we can use pmap and its derivatives, like pmap_dbl etc. pmap takes a list of vectors and a function. In short-hand notation we can define an anonymous function with ~ and integers like ..1, ..2 etc, for the first, second … argument. For example: x &lt;- 1:3 y &lt;- 4:6 z &lt;- 7:9 pmap_dbl( list(x, y, z), ~ ..1 - ..2 + ..3 ) ## [1] 4 5 6 "],
["Chap-01-01-piping.html", "2.5 Piping", " 2.5 Piping When we use a functional style of programming, piping is your best friend. Consider the standard example of applying functions in what linguists would call “center-embedding”. We start with the input (written inside the inner-most bracketing), then apply the first function round, then the second mean, writing each next function call “around” the previous. # define input input_vector &lt;- c(0.4, 0.5, 0.6) # first round, then take mean mean(round(input_vector)) ## [1] 0.3333333 Things quickly get out of hand when more commands are nested. A common practice is to store intermediate results of computations in new variables which are only used to pass the result into the next step. # define input input_vector &lt;- c(0.4, 0.5, 0.6) # rounded input rounded_input &lt;- round(input_vector) # mean of rounded input mean(rounded_input) ## [1] 0.3333333 Piping let’s you pass the result of a previous function call into the next. The magrittr package supplies a special infix operator %&gt;% for piping.9 The pipe %&gt;% essentially takes what results from evaluating the expression on its left-hand side and inputs it as the first argument in the function on its right-hand side. So x %&gt;% f is equivalent to f(x). Or, to continue the example from above, we can now write: input_vector %&gt;% round %&gt;% mean ## [1] 0.3333333 The functions defined as part of the tidyverse are all constructed in such a way that the first argument is the most likely input you would like to pipe into them. But if you want to pipe the left-hand side into another argument slot than the first, you can do that by using the . notation to mark the slot where the left-hand side should be piped into: y %&gt;% f(x, .) is equivalent to f(x,y). The pipe symbol %&gt;% can be inserted in RStudio with Ctrl+Shift+M (Win/Linux) or Cmd+Shift+M (Mac).↩ "],
["ch-01-01-Rmarkdown.html", "2.6 Rmarkdown", " 2.6 Rmarkdown Howework assignments will be issued, filled and submitted in Rmarkdown. To get familiar with Rmarkdown, please follow this tutorial. "],
["Chap-04-01-simple-linear-regression.html", "3 Simple linear regression", " 3 Simple linear regression This chapter introduces the basics of (simple) linear regression modeling with one explanatory variable. It covers ordinary least-squares regression, a frequentist maximum-likelihood approach, as well as a Bayesian approach. It addresses how hypotheses about the values of a regression model’s parameters (so-called coefficients) can be adressed in a frequentist and a Bayesian approach. "],
["data-set-murder-data.html", "3.1 Data set: murder data", " 3.1 Data set: murder data As a running example we use data on murder rates in cities of different population size, also containing further socio-economic information. murder_data &lt;- read_csv(url(&#39;https://raw.githubusercontent.com/michael-franke/intro-data-analysis/master/data_sets/murder_rates.csv&#39;)) %&gt;% rename(murder_rate = annual_murder_rate_per_million_inhabitants, low_income = percentage_low_income, unemployment = percentage_unemployment) %&gt;% select(murder_rate, low_income, unemployment, population) We take a look at the data: murder_data ## # A tibble: 20 x 4 ## murder_rate low_income unemployment population ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 11.2 16.5 6.2 587000 ## 2 13.4 20.5 6.4 643000 ## 3 40.7 26.3 9.3 635000 ## 4 5.3 16.5 5.3 692000 ## 5 24.8 19.2 7.3 1248000 ## 6 12.7 16.5 5.9 643000 ## 7 20.9 20.2 6.4 1964000 ## 8 35.7 21.3 7.6 1531000 ## 9 8.7 17.2 4.9 713000 ## 10 9.6 14.3 6.4 749000 ## 11 14.5 18.1 6 7895000 ## 12 26.9 23.1 7.4 762000 ## 13 15.7 19.1 5.8 2793000 ## 14 36.2 24.7 8.6 741000 ## 15 18.1 18.6 6.5 625000 ## 16 28.9 24.9 8.3 854000 ## 17 14.9 17.9 6.7 716000 ## 18 25.8 22.4 8.6 921000 ## 19 21.7 20.2 8.4 595000 ## 20 25.7 16.9 6.7 3353000 Each row in this data set shows data from a city. The information in the columns is: murder_rate: annual murder rate per million inhabitants low_income: percentage of inhabitants with a low income (however that is defined) unemployment: percentage of unemployed inhabitants population: number of inhabitants of a city Here’s a nice way of plotting each variable against each other: GGally::ggpairs(murder_data, title = &quot;Murder rate data&quot;) The diagonal of this graph shows the density curve of the data in each column. Scatter plots below the diagonal show pairs of values from two columns plotted against each other. The information above the diagonal gives the correlation score of each pair of variables. "],
["what-is-a-simple-linear-regression.html", "3.2 What is a (simple) linear regression?", " 3.2 What is a (simple) linear regression? We are interested in explaining or predicting the murder rates in a city. Suppose we have nothing to explain it with, i.e., we only have a vector of murder rates. Let’s plot the murder rate for every city (just numbered consecutively): 3.2.1 Prediction without any further information Suppose we knew all observed murder rates. If we then wanted to predict the murder rate of a random city , but had no further information about that city, our best guess would be the mean of the observed murder rates, because this is what minimizes (on average) the distance to the observed murder rates. The plot below visualizes the prediction we make by this naive approach. The black dots show the data points, the red line shows the prediction we make (the mean murder rate), the small hollow dots show the specific predictions for each observed value \\(x_i\\) and the gray lines show the distance between our prediction and the actual data observation. The mean distance could be captured in terms of the total sum of squares like this, where \\(y\\) is the \\(n\\)-placed vector observed murder rates and \\(\\bar{y}\\) is its mean: \\[ \\text{TSS} = \\sum_{i=1}^n (y_i - \\bar{y})^2 \\] In the case at hand, that is: y &lt;- murder_data %&gt;% pull(murder_rate) n &lt;- length(y) tss_simple &lt;- sum((y - mean(y))^2) tss_simple ## [1] 1855.202 3.2.2 Prediction with knowledge of unemployment rate We might not be very content with this prediction error. Suppose we could use some piece of information about the random city whose murder rate we are trying to predict. E.g., we might happen to know the value of the variable unemployment. How could that help us make a better prediction? There does seem to be some useful information in the unemployment rate, which may lead to better predictions of the murder rate. We see this in a scatter plot: Let us assume, for the sake of current illustration, that we expect a very particular functional relationship between the variables murder_rate and unemployment. For some reason or other, we hypothesize that even with 0% unemployment, the murder rate would be positive, namely at 4 murders per million inhabitants. We further hypothesize that with each increase of 1% in the unemployment percentage, the murder rate per million increases by 2. The functional relationship between dependent variable \\(y\\) (= murder rate) and predictor variable \\(x\\) (= unemployment) would then be expressible as a linear function (the hat on variable \\(y\\) indicates that these are not data observations but predictions): \\[ \\hat{y}_i = 2x_i + 4 \\] Here is a graphical representation of this functional relationship. Again, the black dots show the data points, the red line the linear function \\(f(x) = 2x +4\\), the small hollow dots show the specific predictions for each observed value \\(x_i\\) and the gray lines show the distance between our prediction and the actual data observation. (Notice that there are data points for which the unemployment rate is the same, but we observed different murder rates.) We can again quantify our prediction error in terms of a sum of squares like we did before. For the case of a prediction vector \\(\\hat{y}\\), the quantity in question is called residual sum of squares. \\[ \\text{RSS} = \\sum_{i=1}^n (y_i - \\hat{y})^2 \\] Here is how we can calculate RSS in R: y &lt;- murder_data %&gt;% pull(murder_rate) x &lt;- murder_data %&gt;% pull(unemployment) predicted_y &lt;- 2 * x + 4 n &lt;- length(y) rss_guesswork &lt;- sum((y - predicted_y)^2) rss_guesswork ## [1] 1327.74 Compared to the previous prediction, which was based on the mean \\(\\bar{y}\\) only, this linear function reduces the prediction error (measured here geometrically in terms of a sum of squares). 3.2.3 Simple linear regression: general problem formulation Suppose we have \\(k\\) predictor variables \\(x_1, \\dots , x_k\\) and dependent variable \\(y\\). We consider the simple linear relation (where the hat on top of vector \\(y\\) symbolizes that this is a vector of predictions): \\[ \\hat{y}_i = \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_k x_{ki}\\] The parameters \\(\\beta_0, \\beta_1, \\dots, \\beta_k\\) of this equation are called regression coefficients. In particular, \\(\\beta_0\\) is called the regression intercept and \\(\\beta_1, \\dots, \\beta_k\\) are regression slope coefficients. Based on the predictions of a parameter vector \\(\\langle \\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_k\\rangle\\), we consider the residual sum of squares as a measure of prediction error: \\[\\text{RSS}_{\\langle {\\beta}_0, {\\beta}_1, \\dots, {\\beta}_k\\rangle} = \\sum_{i = 1}^k (y_i - \\hat{y}_i)^2 \\] We would like to find the best parameter values (denoted traditionally by a hat on the parameter’s variable: \\(\\hat{\\beta}_i\\)) in the sense of minimizing the residual sum of squares: \\[ \\langle \\hat{\\beta}_0, \\hat{\\beta}_1, \\dots , \\hat{\\beta}_k\\rangle = \\arg \\min_{\\langle \\beta_0, \\beta_1, \\dots, \\beta_k\\rangle} \\text{RSS}_{\\langle {\\beta}_0, {\\beta}_1, \\dots, {\\beta}_k\\rangle} \\] For instance, the example started above, where we regressed murder_rate against unemployment has two regression coefficients: an intercept term and a slope for unemployment. The optimal solution for these (see next section) delivers the regression line in the graph below: The total sum of squares for the best fitting parameters is: ## [1] 467.6023 The next section will explain how we find best-fitting parameter values in the sense above. "],
["ordinary-least-squares-regression.html", "3.3 Ordinary least-squares regression", " 3.3 Ordinary least-squares regression This section looks at different ways of finding values for regression coefficients that minimize the residual sum of squares. 3.3.1 Finding optimal parameters with optim We can use the optim function to find the best-fitting parameter values for a simple linear regression. Here is an example based on the murder data. # data to be explained / predicted y &lt;- murder_data %&gt;% pull(murder_rate) # data to use for prediction / explanation x &lt;- murder_data %&gt;% pull(unemployment) # function to calculate residual sum of squares get_rss = function(y, x, beta_0, beta_1) { yPred = beta_0 + x * beta_1 sum((y-yPred)^2) } # finding best-fitting values for TSS fit_rss = optim(par = c(0, 1), fn = function(par) { get_rss(y, x, par[1], par[2]) } ) # output the results message( &quot;Best fitting parameter values:&quot;, &quot;\\n\\tIntercept: &quot;, fit_rss$par[1] %&gt;% signif(5), &quot;\\n\\tSlope: &quot;, fit_rss$par[2] %&gt;% signif(5), &quot;\\nRSS for best fit: &quot;, fit_rss$value %&gt;% signif(5) ) ## Best fitting parameter values: ## Intercept: -28.528 ## Slope: 7.0795 ## RSS for best fit: 467.6 3.3.2 Fitting OLS regression lines with lm R also has a built-in function lm which fits (simple) linear regression models via RSS minimization. Here is how you call this function for the running example: # fit an OLS regression fit_lm &lt;- lm( # the formula argument specifies dependent and independent variables formula = murder_rate ~ unemployment, # we also need to say where the data (columns) should come from data = murder_data ) # output the fitted object fit_lm ## ## Call: ## lm(formula = murder_rate ~ unemployment, data = murder_data) ## ## Coefficients: ## (Intercept) unemployment ## -28.53 7.08 The output of the fitted object shows the best-fitting values (compare them to what we obtained by hand). It also shows the function call by which this fit was obtained. There is more information in the object fit_lm and we will return to this later when we consider hypothesis testing on regression coefficients. But it might be interesting to take a quick preview already: summary(fit_lm) ## ## Call: ## lm(formula = murder_rate ~ unemployment, data = murder_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.2415 -3.7728 0.5795 3.2207 10.4221 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -28.5267 6.8137 -4.187 0.000554 *** ## unemployment 7.0796 0.9687 7.309 8.66e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.097 on 18 degrees of freedom ## Multiple R-squared: 0.748, Adjusted R-squared: 0.7339 ## F-statistic: 53.41 on 1 and 18 DF, p-value: 8.663e-07 3.3.3 Finding optimal parameter values with math It is also possible to determine the OLS-fits by a mathematical derivation. Theorem 3.1 (OLS solution) For a simple linear regression model with just one predictor, the solution for: \\[\\arg \\min_{\\langle \\beta_0, \\beta_1\\rangle} \\sum_{i = 1}^k (y_i - (\\beta_0 + \\beta_1 x_{i}))^2\\] is given by: \\[ \\begin{aligned} \\hat{\\beta_1} &amp;= \\frac{Cov(x,y)}{Var(x)} &amp; \\hat{\\beta_0} &amp;= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\end{aligned} \\] Proof. TBD   Let’s use these formulas to calculate regression coefficients for the running example as well: tibble( beta_1 = cov(x,y) / var(x), beta_0 = mean(y) - beta_1 * mean(x) ) ## # A tibble: 1 x 2 ## beta_1 beta_0 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 7.08 -28.5 A similar result exists also for regression with more than one predictor variable. This explains why the computation of best-fitting regression coefficients with a built-in function like lm is lightning fast (as compared to using optim or a Bayesian approach which relies on MCMC sampling). "],
["a-maximum-likelihood-approach.html", "3.4 A maximum-likelihood approach", " 3.4 A maximum-likelihood approach In order to be able to extend regression modeling to predictor variables other than metric variables (so-called generalized linear regression models), the geometric approach needs to be abandoned in favor of a likelihood-based approach. There are two equivalent formulation of a (simple) linear regression model, using a likelihood-based approach. The first is more explicit, showing clearly that the model assumes that for each observation \\(y_i\\), the model assumes an error term \\(\\epsilon_i\\), which is an iid sample from a Normal distribution. (Notice that the likelihood-based model assumes an additional parameter \\(\\sigma\\), the standard deviation of the error terms.) \\[ \\mathbf{\\text{likelihood-based regression}} \\\\ \\mathbf{\\text{[explicit version]}}\\\\ \\begin{align*} y_{\\text{pred}} &amp; = \\beta_0 + \\beta_1 x \\\\ y_i &amp; = y_{\\text{pred}} + \\epsilon_i \\\\ \\epsilon_i &amp; \\sim \\text{Normal}(0, \\sigma) \\\\ \\end{align*} \\] The second, equivalent version of this writes this more compactly, suppressing the explicit mentioning of iid error terms: \\[ \\mathbf{\\text{likelihood-based regression}} \\\\ \\mathbf{\\text{[compact version]}} \\\\ \\begin{align*} y_{\\text{pred}} &amp; = \\beta_0 + \\beta_1 x \\\\ y &amp; \\sim \\text{Normal}(\\mu = y_{\\text{pred}}, \\sigma) \\end{align*} \\] We can use optim to find maximum likelihood estimates: # data to be explained / predicted y &lt;- murder_data %&gt;% pull(murder_rate) # data to use for prediction / explanation x &lt;- murder_data %&gt;% pull(unemployment) # function to calculate negative log-likelihood get_nll = function(y, x, beta_0, beta_1, sd) { if (sd &lt;= 0) {return( Inf )} yPred = beta_0 + x * beta_1 nll = -dnorm(y, mean=yPred, sd=sd, log = T) sum(nll) } # finding MLE fit_lh = optim(par = c(0, 1, 1), fn = function(par) { get_nll(y, x, par[1], par[2], par[3]) } ) # output the results message( &quot;Best fitting parameter values:&quot;, &quot;\\n\\tIntercept: &quot;, fit_lh$par[1] %&gt;% signif(5), &quot;\\n\\tSlope: &quot;, fit_lh$par[2] %&gt;% signif(5), &quot;\\nNegative log-likelihood for best fit: &quot;, fit_lh$value %&gt;% signif(5) ) ## Best fitting parameter values: ## Intercept: -28.517 ## Slope: 7.0783 ## Negative log-likelihood for best fit: 59.898 It is no coincidence that these fitted values are (modulo number imprecision) the same as for the geometric OLS approach. Theorem 3.2 (MLE solution) For a simple linear regression model with just one predictor, the solution for: \\[\\arg \\max_{\\langle \\beta_0, \\beta_1, \\sigma \\rangle} \\prod_{i = 1}^k \\text{Normal}(\\mu = \\beta_0 + \\beta_1 x_{i}, \\sigma)\\] is the same as for the OLS approach: \\[ \\begin{aligned} \\hat{\\beta_1} &amp;= \\frac{Cov(x,y)}{Var(x)} &amp; \\hat{\\beta_0} &amp;= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\end{aligned} \\] Proof. TBD   The equivalence also extends to cases with more than one explanatory variable. R also has a built-in way of approaching simple linear regression wiith a maximum-likelihood approach, namely by using the function glm (generalized linear model). Notice that the output looks slightly different from that of lm. fit_glm &lt;- glm(murder_rate ~ unemployment, data = murder_data) fit_glm ## ## Call: glm(formula = murder_rate ~ unemployment, data = murder_data) ## ## Coefficients: ## (Intercept) unemployment ## -28.53 7.08 ## ## Degrees of Freedom: 19 Total (i.e. Null); 18 Residual ## Null Deviance: 1855 ## Residual Deviance: 467.6 AIC: 125.8 We might also risk a peek at the summary of fit_glm: summary(fit_glm) ## ## Call: ## glm(formula = murder_rate ~ unemployment, data = murder_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -9.2415 -3.7728 0.5795 3.2207 10.4221 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -28.5267 6.8137 -4.187 0.000554 *** ## unemployment 7.0796 0.9687 7.309 8.66e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 25.9779) ## ## Null deviance: 1855.2 on 19 degrees of freedom ## Residual deviance: 467.6 on 18 degrees of freedom ## AIC: 125.8 ## ## Number of Fisher Scoring iterations: 2 "],
["a-bayesian-approach.html", "3.5 A Bayesian approach", " 3.5 A Bayesian approach A Bayesian model for (simple) linear regression looks very much like the previous likelihood-based model, just that it also adds prior information. We have already seen a Bayesian linear regression model in Chapter ??. It is repeated here in Figure 3.1. Figure 3.1: Bayesian Simple Linear Regression Model (repeated from before). 3.5.1 Implementation in greta Here is an implementation of a Bayesian regression model for the running example murder data using greta: # data to be explained / predicted y &lt;- murder_data %&gt;% pull(murder_rate) # data to use for prediction / explanation x &lt;- murder_data %&gt;% pull(unemployment) y_greta &lt;- as_data(y) x_greta &lt;- as_data(x) # latent variables and priors intercept &lt;- student(df= 1, mu = 0, sigma = 10) slope &lt;- student(df= 1, mu = 0, sigma = 10) sigma &lt;- normal(0, 5, truncation = c(0, Inf)) # derived latent variable (linear model) y_pred &lt;- intercept + slope * x_greta # likelihood distribution(y) &lt;- normal(y_pred, sigma) # finalize model, register which parameters to monitor murder_model &lt;- model(intercept, slope, sigma) We can draw samples from the posterior distribution as usual: # draw samples draws_murder_data &lt;- greta::mcmc( murder_model, n_samples = 2000, chains = 4, warmup = 1000 ) # cast results (type &#39;mcmc.list&#39;) into tidy tibble tidy_draws_murder_data &lt;- ggmcmc::ggs(draws_murder_data) Here is a plot of the posterior: # plot posterior tidy_draws_murder_data %&gt;% ggplot(aes(x = value)) + geom_density(fill = &quot;lightgray&quot;, alpha = 0.5) + facet_wrap(~ Parameter, scales = &quot;free&quot;) 3.5.2 Using the brms package Instead of hand-coding each Bayesian regression model, we can also use the brms package (Bürkner 2017). The main funtion of this package is brm (short for Bayesian regression model). It behaves very similarly to the glm function we saw above.10 Here is an example for the current case study: fit_brms_murder &lt;- brm( # specify what to explain in terms of what # using the formula syntax formula = murder_rate ~ unemployment, # which data to use data = murder_data ) The function brm returns a model-fit object, similar to glm. We can inspect it to get more information, using the summary function: summary(fit_brms_murder) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: murder_rate ~ unemployment ## Data: murder_data (Number of observations: 20) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -28.48 7.32 -42.05 -13.79 1.00 3014 2362 ## unemployment 7.07 1.04 4.97 9.04 1.00 2978 2451 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 5.42 0.96 3.88 7.63 1.00 2664 2196 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). This output tells us about which model we fitted, states some properties of the MCMC sampling routine used to obtain samples from the posterior distribution, and it also gives us information about the posteriors under the heading “Population-Level Effects” we get summary statistics of the set of posterior samples for each regression coefficient, including the mean (“Estimate”), and the 95% interquantile range (“l-95%” is the lower bound and “u-95%” is the upper bound).11 References "],
["testing-coefficients.html", "3.6 Testing coefficients", " 3.6 Testing coefficients 3.6.1 Bayesian approach # get means and 95% HDI Bayes_estimates &lt;- tidy_draws_murder_data %&gt;% group_by(Parameter) %&gt;% summarise( &#39;|95%&#39; = HDInterval::hdi(value)[1], mean = mean(value), &#39;95|%&#39; = HDInterval::hdi(value)[2] ) Bayes_estimates ## # A tibble: 3 x 4 ## Parameter `|95%` mean `95|%` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 intercept -39.1 -24.6 -9.51 ## 2 sigma 3.78 5.36 7.20 ## 3 slope 4.34 6.53 8.53 3.6.2 Frequentist approach Figure 3.2 shows a frequentist model for testing the hypotheses that the regression coefficients are equal to zero. Figure 3.2: Frequentist model for testing whether regression coefficients are plausibly equal to zero. We use this model to compute the test statics for the obsered data: # observed data y_obs &lt;- murder_data %&gt;% pull(murder_rate) x_obs &lt;- murder_data %&gt;% pull(unemployment) n_obs &lt;- length(y_obs) # best-fitting coefficients beta_1_hat &lt;- cov(x_obs, y_obs) / var(x_obs) beta_0_hat &lt;- mean(y_obs) - beta_1_hat * mean(x_obs) # calculating t-scores MSE &lt;- sum((y_obs - beta_0_hat - beta_1_hat * x_obs)^2) / (n_obs-2) S_xx &lt;- sum((x_obs - mean(x_obs))^2) SE_beta_1_hat &lt;- sqrt(MSE / S_xx) SE_beta_0_hat &lt;- sqrt(MSE * (1/n_obs + mean(x_obs)^2 / S_xx)) t_slope = (beta_1_hat) / SE_beta_1_hat t_intercept = beta_0_hat / SE_beta_0_hat tibble(t_slope, t_intercept) ## # A tibble: 1 x 2 ## t_slope t_intercept ## &lt;dbl&gt; &lt;dbl&gt; ## 1 7.31 -4.19 Calculate \\(p\\)-values (two sided!) for both of these values: p_value_intercept = pt(t_intercept, df = n_obs -2) + 1-pt(-t_intercept, df = n_obs -2) p_value_slope = pt(-t_slope, df = n_obs -2) + 1-pt(t_slope, df = n_obs -2) tibble(p_value_intercept, p_value_slope) ## # A tibble: 1 x 2 ## p_value_intercept p_value_slope ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.000554 0.000000866 We compare the manual calculation to the that of the built-in functions lm and glm: summary(lm(murder_rate ~ unemployment, data = murder_data)) ## ## Call: ## lm(formula = murder_rate ~ unemployment, data = murder_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.2415 -3.7728 0.5795 3.2207 10.4221 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -28.5267 6.8137 -4.187 0.000554 *** ## unemployment 7.0796 0.9687 7.309 8.66e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.097 on 18 degrees of freedom ## Multiple R-squared: 0.748, Adjusted R-squared: 0.7339 ## F-statistic: 53.41 on 1 and 18 DF, p-value: 8.663e-07 summary(glm(murder_rate ~ unemployment, data = murder_data)) ## ## Call: ## glm(formula = murder_rate ~ unemployment, data = murder_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -9.2415 -3.7728 0.5795 3.2207 10.4221 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -28.5267 6.8137 -4.187 0.000554 *** ## unemployment 7.0796 0.9687 7.309 8.66e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 25.9779) ## ## Null deviance: 1855.2 on 19 degrees of freedom ## Residual deviance: 467.6 on 18 degrees of freedom ## AIC: 125.8 ## ## Number of Fisher Scoring iterations: 2 "],
["Chap-04-02-beyond-simple-regression.html", "4 Beyond simple linear regression", " 4 Beyond simple linear regression This chapter showcases how linear regression models can be applied flexibly to a variety of data analysis problems. categorical predictors two-groups (eco-sensitivity data) more groups (mental chronometry) interactions (Winter data) metric and categorical predictors combined avocado data logistic regression KoF data "],
["two-categorical-predictors.html", "4.1 Two categorical predictors", " 4.1 Two categorical predictors Let’s revisit the (fictitious) eco-sensitivity data from Chapter ??: x_A &lt;- c( 104, 105, 100, 91, 105, 118, 164, 168, 111, 107, 136, 149, 104, 114, 107, 95, 83, 114, 171, 176, 117, 107, 108, 107, 119, 126, 105, 119, 107, 131 ) x_B &lt;- c( 133, 115, 84, 79, 127, 103, 109, 128, 127, 107, 94, 95, 90, 118, 124, 108, 87, 111, 96, 89, 106, 121, 99, 86, 115, 136, 114 ) Remember that we are interested in the question whether there is a difference in means of group A and group B. Previously we used a \\(t\\)-test for this two-group comparison (frequentist or Bayesian). But we can also cast this as a regression problem. Here is how. Let’s first squeeze the data into a tibble: eco_sensitivity_data &lt;- tibble( group = c(rep(&quot;A&quot;, length(x_A)), rep(&quot;B&quot;, length(x_B))), measurement = c(x_A, x_B) ) eco_sensitivity_data ## # A tibble: 57 x 2 ## group measurement ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 104 ## 2 A 105 ## 3 A 100 ## 4 A 91 ## 5 A 105 ## 6 A 118 ## 7 A 164 ## 8 A 168 ## 9 A 111 ## 10 A 107 ## # … with 47 more rows Notice that this tibble contains the data in a tidy format, i.e., each row contains a tuple of associated measurements. We want to explain or predict the variable measurement in terms of the variable group.We can then run a regression model with forumla measurement ~ group. Here’s such a model using the Bayesian approach: fit_brms_eco_sensitivity &lt;- brm( # specify what to explain in terms of what # using the formula syntax formula = measurement ~ group, # which data to use data = eco_sensitivity_data ) Let’s inspect the summary information for the posterior samples: # just showing information for the so-called &#39;fixed effects&#39; summary(fit_brms_eco_sensitivity)$fixed ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 118.68533 3.755572 111.42867 125.8595713 1.001581 3956 3160 ## groupB -11.37756 5.576870 -22.47929 -0.6354608 1.001299 3845 2694 Compare this with the summary of the posterior estimates we obtained from the Bayesian \\(t\\)-test model for this data which we implemented in greta and ran in Chapter ??. Bayes_estimates_eco ## # A tibble: 1 x 4 ## Parameter `|95%` mean `95|%` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 delta -21.0 -10.9 -0.384 The mean of the \\(\\delta\\) parameter looks suscpiciously similar to the ominous groupB parameter shown in the output of the bmrs model fit. The 95% HDIs of the estimated posterior for the \\(\\delta\\) parameter also look suspiciously like the values of the 95% interquantile range for the groupB parameter. This is no conincidence! In fact, the regression model that bmrs calculates here is essentially the same as the \\(t\\)-test model we implemented in greta by hand except for slight different in the choice of the priors and inessential differences in mathematical formulation of the group mean comparison. show plots of posterior side-by-side spell out models, show graphs The upshot is, that we can conceive of a \\(t\\)-test as a special case of a linear regression model! –&gt; "],
["app-90-further-material.html", "A Further useful material ", " A Further useful material "],
["material-on-introduction-to-probability.html", "A.1 Material on Introduction to Probability:", " A.1 Material on Introduction to Probability: “Introduction to Probability” by J.K. Blitzstein and J. Hwang (Blitzstein and Hwang 2014) “Probability Theory: The Logic of Science” by E.T. Jaynes (Jaynes 2003) References "],
["material-on-bayesian-data-analysis.html", "A.2 Material on Bayesian Data Analysis:", " A.2 Material on Bayesian Data Analysis: “Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan” by J. Kruschke (Kruschke 2015) “Baysian Data Analysis” by A. Gelman et al. (Gelman et al. 2013) “Statistical Rethinking: A Bayesian Course with Examples in R and Stan” by R. McElreath (McElreath 2015) webbook based on McElreath’s book: Statistical Rethinking with brms, ggplot2, and the tidyverse by Solomon Kurz References "],
["material-on-frequentist-statistics.html", "A.3 Material on frequentist statistics:", " A.3 Material on frequentist statistics: “Statistics for LinguistsL: An introduction using R”, by B. Winter (Winter 2019-) References "],
["material-on-r-tidyverse-etc-.html", "A.4 Material on R, tidyverse, etc.:", " A.4 Material on R, tidyverse, etc.: official R manual: An Introduction to R “R for Data Science: Import, Tidy, Transform, Visualize, and Model Data” by H. Wickham and G. Grolemund (Wickham and Grolemund 2016) RStudio’s Cheat Sheets “Data Visualization” by K. Healy (Healy 2018) webbook Learning Statistics with R by Danielle Navarro webbook with focus on visualization: Data Science for Psychologists by Hansjörg Neth References "],
["further-information-for-rstudio.html", "A.5 Further information for RStudio", " A.5 Further information for RStudio Keyboard shortcuts for Windows and Mac in RStudio: “Tools -&gt; Keyboard Shortcuts Help” or also on the RStudio support site "],
["resources-on-webppl.html", "A.6 Resources on WebPPL", " A.6 Resources on WebPPL official website documentation short introduction tutorial Bayesian Data Analysis using Probabilistic Programs: Statistics as pottery by webbook on BDA with WebPPL by MH Tessler "],
["references.html", "References", " References "]
]
