<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>A Appendix: Common probability distributions | Introduction to Data Analysis</title>
  <meta name="description" content="Course Material for I2DA-2019 in Cognitive Science at University Osnabrück" />
  <meta name="generator" content="bookdown 0.14.1 and GitBook 2.6.7" />

  <meta property="og:title" content="A Appendix: Common probability distributions | Introduction to Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course Material for I2DA-2019 in Cognitive Science at University Osnabrück" />
  <meta name="github-repo" content="michael-franke/intro-data-analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Appendix: Common probability distributions | Introduction to Data Analysis" />
  
  <meta name="twitter:description" content="Course Material for I2DA-2019 in Cognitive Science at University Osnabrück" />
  

<meta name="author" content="" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="hierarchical-regression.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span. { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#testing-showcasing"><i class="fa fa-check"></i><b>0.1</b> Testing / Showcasing</a><ul>
<li class="chapter" data-level="0.1.1" data-path="index.html"><a href="index.html#quotes"><i class="fa fa-check"></i><b>0.1.1</b> Quotes</a></li>
<li class="chapter" data-level="0.1.2" data-path="index.html"><a href="index.html#infobox"><i class="fa fa-check"></i><b>0.1.2</b> Infobox</a></li>
<li class="chapter" data-level="0.1.3" data-path="index.html"><a href="index.html#plots"><i class="fa fa-check"></i><b>0.1.3</b> Plots</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>I Foundations</b></span></li>
<li class="chapter" data-level="1" data-path="general-introduction.html"><a href="general-introduction.html"><i class="fa fa-check"></i><b>1</b> General Introduction</a></li>
<li class="chapter" data-level="2" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>2</b> Data</a></li>
<li class="chapter" data-level="3" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html"><i class="fa fa-check"></i><b>3</b> Basics of Probability Theory</a><ul>
<li class="chapter" data-level="3.1" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#probability"><i class="fa fa-check"></i><b>3.1</b> Probability</a><ul>
<li class="chapter" data-level="3.1.1" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#outcomes-events-observations"><i class="fa fa-check"></i><b>3.1.1</b> Outcomes, events, observations</a></li>
<li class="chapter" data-level="3.1.2" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#probability-distributions"><i class="fa fa-check"></i><b>3.1.2</b> Probability distributions</a></li>
<li class="chapter" data-level="3.1.3" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#interpretations-of-probability"><i class="fa fa-check"></i><b>3.1.3</b> Interpretations of probability</a></li>
<li class="chapter" data-level="3.1.4" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#urns-and-frequencies"><i class="fa fa-check"></i><b>3.1.4</b> Urns and frequencies</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#structured-events-marginal-distributions"><i class="fa fa-check"></i><b>3.2</b> Structured events &amp; marginal distributions</a><ul>
<li class="chapter" data-level="3.2.1" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#probability-table-for-a-flip--draw-scenario"><i class="fa fa-check"></i><b>3.2.1</b> Probability table for a flip-&amp;-draw scenario</a></li>
<li class="chapter" data-level="3.2.2" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#structured-events-and-joint-probability-distributions"><i class="fa fa-check"></i><b>3.2.2</b> Structured events and joint-probability distributions</a></li>
<li class="chapter" data-level="3.2.3" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#marginalization"><i class="fa fa-check"></i><b>3.2.3</b> Marginalization</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#conditional-probability"><i class="fa fa-check"></i><b>3.3</b> Conditional probability</a><ul>
<li class="chapter" data-level="3.3.1" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#bayes-rule"><i class="fa fa-check"></i><b>3.3.1</b> Bayes rule</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#random-variables"><i class="fa fa-check"></i><b>3.4</b> Random variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#notation-terminology"><i class="fa fa-check"></i><b>3.4.1</b> Notation &amp; terminology</a></li>
<li class="chapter" data-level="3.4.2" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#cumulative-distribution-functions-mass-density"><i class="fa fa-check"></i><b>3.4.2</b> Cumulative distribution functions, mass &amp; density</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#expected-value-variance"><i class="fa fa-check"></i><b>3.5</b> Expected value &amp; variance</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>4</b> Models</a><ul>
<li class="chapter" data-level="4.1" data-path="models.html"><a href="models.html#overview"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="models.html"><a href="models.html#two-notions-of-probability-revisited"><i class="fa fa-check"></i><b>4.2</b> Two notions of probability (revisited)</a><ul>
<li class="chapter" data-level="4.2.1" data-path="models.html"><a href="models.html#frequentism-probabilities-as-properties-of-the-world"><i class="fa fa-check"></i><b>4.2.1</b> Frequentism — Probabilities as properties of the world</a></li>
<li class="chapter" data-level="4.2.2" data-path="models.html"><a href="models.html#bayesianism-probabilities-as-subjective-beliefs"><i class="fa fa-check"></i><b>4.2.2</b> Bayesianism — Probabilities as subjective beliefs</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="models.html"><a href="models.html#likelihood-prior-posterior"><i class="fa fa-check"></i><b>4.3</b> Likelihood, Prior, &amp; Posterior</a><ul>
<li class="chapter" data-level="4.3.1" data-path="models.html"><a href="models.html#probability-density-function-vs.likelihood-function"><i class="fa fa-check"></i><b>4.3.1</b> Probability density function vs. Likelihood function</a></li>
<li class="chapter" data-level="4.3.2" data-path="models.html"><a href="models.html#prior-posterior"><i class="fa fa-check"></i><b>4.3.2</b> Prior &amp; Posterior</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="models.html"><a href="models.html#modeling"><i class="fa fa-check"></i><b>4.4</b> Modeling</a><ul>
<li class="chapter" data-level="4.4.1" data-path="models.html"><a href="models.html#introductory-example"><i class="fa fa-check"></i><b>4.4.1</b> Introductory example</a></li>
<li class="chapter" data-level="4.4.2" data-path="models.html"><a href="models.html#steps-of-data-analysis"><i class="fa fa-check"></i><b>4.4.2</b> Steps of Data Analysis</a></li>
<li class="chapter" data-level="4.4.3" data-path="models.html"><a href="models.html#notation"><i class="fa fa-check"></i><b>4.4.3</b> Notation</a></li>
<li class="chapter" data-level="4.4.4" data-path="models.html"><a href="models.html#an-outlook-hierarchical-models"><i class="fa fa-check"></i><b>4.4.4</b> An outlook: Hierarchical models</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="models.html"><a href="models.html#further-examples"><i class="fa fa-check"></i><b>4.5</b> Further examples</a><ul>
<li class="chapter" data-level="4.5.1" data-path="models.html"><a href="models.html#difference-between-two-groups"><i class="fa fa-check"></i><b>4.5.1</b> Difference between two groups</a></li>
<li class="chapter" data-level="4.5.2" data-path="models.html"><a href="models.html#simple-linear-regression-with-one-metric-predictor"><i class="fa fa-check"></i><b>4.5.2</b> Simple linear regression with one metric predictor</a></li>
<li class="chapter" data-level="4.5.3" data-path="models.html"><a href="models.html#notation-simple-regression-model"><i class="fa fa-check"></i><b>4.5.3</b> Notation Simple Regression model</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="models.html"><a href="models.html#further-elaboration-on-modeling-in-anticipation-of-the-topic-estimation"><i class="fa fa-check"></i><b>4.6</b> Further elaboration on modeling (in anticipation of the topic “estimation”)</a><ul>
<li class="chapter" data-level="4.6.1" data-path="models.html"><a href="models.html#beta-binomial-model---one-group-revisited"><i class="fa fa-check"></i><b>4.6.1</b> Beta-Binomial model - one group (revisited)</a></li>
<li class="chapter" data-level="4.6.2" data-path="models.html"><a href="models.html#beta-binomial-model---two-groups-revisited"><i class="fa fa-check"></i><b>4.6.2</b> Beta-Binomial model - two groups (revisited)</a></li>
<li class="chapter" data-level="4.6.3" data-path="models.html"><a href="models.html#simple-linear-regression-model-revisited"><i class="fa fa-check"></i><b>4.6.3</b> Simple linear regression model (revisited)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>5</b> Inference</a></li>
<li class="chapter" data-level="6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>6</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="7" data-path="model-comparison.html"><a href="model-comparison.html"><i class="fa fa-check"></i><b>7</b> Model Comparison</a></li>
<li class="chapter" data-level="8" data-path="bayesian-hypothesis-testing.html"><a href="bayesian-hypothesis-testing.html"><i class="fa fa-check"></i><b>8</b> Bayesian hypothesis testing</a></li>
<li class="chapter" data-level="9" data-path="model-criticism.html"><a href="model-criticism.html"><i class="fa fa-check"></i><b>9</b> Model criticism</a></li>
<li class="part"><span><b>II Appliied (generalized) linear modeling</b></span></li>
<li class="chapter" data-level="10" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>10</b> Simple linear regression</a></li>
<li class="chapter" data-level="11" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>11</b> Logistic regression</a></li>
<li class="chapter" data-level="12" data-path="multinomial-regression.html"><a href="multinomial-regression.html"><i class="fa fa-check"></i><b>12</b> Multinomial regression</a></li>
<li class="chapter" data-level="13" data-path="ordinal-regression.html"><a href="ordinal-regression.html"><i class="fa fa-check"></i><b>13</b> Ordinal regression</a></li>
<li class="chapter" data-level="14" data-path="hierarchical-regression.html"><a href="hierarchical-regression.html"><i class="fa fa-check"></i><b>14</b> Hierarchical regression</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html"><i class="fa fa-check"></i><b>A</b> Appendix: Common probability distributions</a><ul>
<li class="chapter" data-level="A.1" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#an-important-family-the-exponential-family"><i class="fa fa-check"></i><b>A.1</b> An important family: The Exponential Family</a></li>
<li class="chapter" data-level="A.2" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#excursos-information-entropy-and-maximum-entropy-principal"><i class="fa fa-check"></i><b>A.2</b> Excursos: “Information Entropy” and “Maximum Entropy Principal”</a><ul>
<li class="chapter" data-level="A.2.1" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#information-entropy"><i class="fa fa-check"></i><b>A.2.1</b> Information Entropy</a></li>
<li class="chapter" data-level="A.2.2" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#deriving-probability-distributions-using-the-maximum-entropy-principle"><i class="fa fa-check"></i><b>A.2.2</b> Deriving Probability Distributions using the Maximum Entropy Principle</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#selected-continous-distributions-of-random-variables"><i class="fa fa-check"></i><b>A.3</b> Selected continous distributions of random variables</a><ul>
<li class="chapter" data-level="A.3.1" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#normal-distribution"><i class="fa fa-check"></i><b>A.3.1</b> Normal distribution</a></li>
<li class="chapter" data-level="A.3.2" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#chi-square-distribution"><i class="fa fa-check"></i><b>A.3.2</b> Chi-square distribution</a></li>
<li class="chapter" data-level="A.3.3" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#f-distribution"><i class="fa fa-check"></i><b>A.3.3</b> F distribution</a></li>
<li class="chapter" data-level="A.3.4" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#student-t-distribution-not-part-of-exponential-family"><i class="fa fa-check"></i><b>A.3.4</b> Student t-distribution (not part of exponential family)</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#selected-discrete-distributions-of-random-variables"><i class="fa fa-check"></i><b>A.4</b> Selected discrete distributions of random variables</a><ul>
<li class="chapter" data-level="A.4.1" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#binomial-distribution"><i class="fa fa-check"></i><b>A.4.1</b> Binomial distribution</a></li>
<li class="chapter" data-level="A.4.2" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#poisson-distribution"><i class="fa fa-check"></i><b>A.4.2</b> Poisson distribution</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="appendix-common-probability-distributions.html"><a href="appendix-common-probability-distributions.html#understanding-distributions-as-random-variables"><i class="fa fa-check"></i><b>A.5</b> Understanding distributions as random variables</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="appendix-common-probability-distributions" class="section level1">
<h1><span class="header-section-number">A</span> Appendix: Common probability distributions</h1>
<div id="an-important-family-the-exponential-family" class="section level2">
<h2><span class="header-section-number">A.1</span> An important family: The Exponential Family</h2>
<p>Most common distributions used in statistical modeling are members of the exponential family. Among others:</p>
<ul>
<li>Poisson distribution,</li>
<li>Bernoulli distribution,</li>
<li>Normal distribution,</li>
<li>Chi-Square distribution, and of course the</li>
<li>Exponential distribution.</li>
</ul>
<p>In the upcoming section some of these distributions will be described in more detail. But what makes the <em>exponential family</em> so <em>special</em>? On the one hand, distributions of this family have some convenient mathematical properties which makes them attractive to use in <em>statistical modeling</em>. In particular for Bayesian Analysis: For example do all these distributions have a <em>conjugate prior</em> and the posterior distribution has a simple form. Furthermore the above example distributions are really just examples. The exponential family encompasses a <em>wide class of distributions</em> which makes it possible to model various cases.</p>
<p>On the other hand, the use of distributions from the exponential family is also from a <em>conceptional perspective</em> attractive. Consider for example the following situation:</p>
<p>Consider we want to infer a propability distribution subject to certain constraints. For example a coin flip experiment can have only a dichotomous outcome {0,1} and has a constant probability. <em>Which distribution should be used in order to model this scenario?</em></p>
<p>There are several possible distributions that can be used, according to which <em>criteria</em> should a distribution be selected? Often one attempts a <em>conservative choice</em>, that is to bring as little subjective information into a model as possible. Or in other terms, one goal could be to select the distribution, among all possible distributions, that is <em>maximal ignorant</em> and least biased given the constraints.</p>
<p>Consequently, the question arises how <em>“ignorance” can be measured</em> and <em>distributions compared</em> according to their “information content”? This will be topic of the upcoming exursos, the key words here are <em>“entropy”</em>, which comes from information theory, and <em>“Maximum Entropy Principal”</em>.</p>
<p>To briefly anticipate the connection between exponential family and maximum ignorance distributions: The maximum entropy principal starts with constraints that are imposed on a distribution and derives by maximizing entropy a probability density/mass function. Distributions belonging to the exponential family arise as <em>solutions to the maximum entropy problem</em> subject to linear constraints.</p>
<p>In the upcoming section selected continous and discrete distributions will be described in more detail. Followed by a part which motivation is to strengthen the intuition about understanding <em>distributions as random variables</em>.</p>
</div>
<div id="excursos-information-entropy-and-maximum-entropy-principal" class="section level2">
<h2><span class="header-section-number">A.2</span> Excursos: “Information Entropy” and “Maximum Entropy Principal”</h2>
<div id="information-entropy" class="section level3">
<h3><span class="header-section-number">A.2.1</span> Information Entropy</h3>
<p><em>Entropy</em> is a measure of information content of an outcome of <span class="math inline">\(X\)</span> such that less probable outcomes convey more information than more probable ones. Thus, entropy can be stated as a <em>measure of uncertainty</em>. When the goal is to find a distrbution that is as ignorant as possible, then, consequently, entropy should be maximal. Formally, entropy is defined as follows:
If <span class="math inline">\(X\)</span> is a discrete random variable with distribution <span class="math inline">\(P(X=x_i)=p_i\)</span> then the entropy of <span class="math inline">\(X\)</span> is
<span class="math display">\[H(X)=-\sum_{i} p_i \log p_i.\]</span></p>
<p>If <span class="math inline">\(X\)</span> is a continuous random variable with probability density <span class="math inline">\(p(x)\)</span> then the differential entropy of <span class="math inline">\(X\)</span> is
<span class="math display">\[H(X)=-\int_{-\infty}^{+\infty} p(x) \log p(x) dx.\]</span></p>
<p>From which considerations is this <em>entropy</em> definition derived? There exist various approaches that finally come to the same answer: the above stated definition of entropy. However, the most cited derivation is Shannon’s theorem. Another and perhapse more intuitive derivation is Wallis derivation. Jaynes (2003) describes both approaches in detail. The following provides a short insight in both derivations and is taken from <span class="citation">(Jaynes <a href="#ref-jaynes2003">2003</a>)</span>.</p>
<div id="shannons-theorem" class="section level4">
<h4><span class="header-section-number">A.2.1.1</span> Shannon’s theorem</h4>
<p>Shannon’s approach starts by stating conditions that a measure of the  <span class="math inline">\(H_n\)</span> has to satisfy.</p>
<ol style="list-style-type: decimal">
<li>It is possible to set up some kind of association between  and real numbers</li>
<li><span class="math inline">\(H_n\)</span> is a continous function of <span class="math inline">\(p_i\)</span>. Otherwise, an arbitrarily small change in the probability distribution would lead to a big change in the amount of uncertainty.</li>
<li><span class="math inline">\(H_n\)</span> should correspond to common sense in that, when there are many possibilities, we are more uncertain than when there are few. This condition takes the form that in case the <span class="math inline">\(p_i\)</span> are all equal, the quantity <span class="math inline">\(h(n)\)</span> is a monotonic increasing function of <span class="math inline">\(n\)</span>.</li>
<li><span class="math inline">\(H_n\)</span> is consistent in that, when there is more than one way of working out its value, we must get the same answer for few possible way.</li>
</ol>
<p>Under these assumptions the resulting unique measure of uncertainty of a probability distribution <span class="math inline">\(p\)</span> turns out to be just the <em>average log-probability</em>:</p>
<p><span class="math display">\[H(p)=-\sum_i p_i \log(p_i).\]</span>
(The interested reader can find a systematic derivation in <span class="citation">(Jaynes <a href="#ref-jaynes2003">2003</a>)</span>.) Accepting this interpretation of entropy, it follows that the distribution <span class="math inline">\((p_1,...,p_n)\)</span> which maximizes the above equation, subject to constraints imposed by the available information, will represent the most  description of what the model  about the propositions <span class="math inline">\((A_1,...,A_n)\)</span> <span class="citation">(Jaynes <a href="#ref-jaynes2003">2003</a>)</span>.</p>
<p>The function <span class="math inline">\(H\)</span> is called the , or the  of the distribution <span class="math inline">\(\{p_i\}\)</span>.</p>
</div>
<div id="the-wallis-derivation" class="section level4">
<h4><span class="header-section-number">A.2.1.2</span> The Wallis derivation</h4>
<p>A second and perhaps more intuitive approach of deriving entropy was suggested by G. Wallis. The following description is taken from Jaynes (2003).</p>
<p>We are given information <span class="math inline">\(I\)</span>, which is to be used in assigning probabilities <span class="math inline">\(\{p_1,...,p_m\}\)</span> to <span class="math inline">\(m\)</span> different probabilities. We have a total amount of probability</p>
<p><span class="math display">\[\sum_{i=1}^{m} p_i =1\]</span></p>
<p>to allocate among them.</p>
<p>The problem can be stated as follows. Choose some integer <span class="math inline">\(n&gt;&gt;m\)</span>, and imagine that we have <span class="math inline">\(n\)</span> little  of probabilities, each of magnitude <span class="math inline">\(\delta=\frac{1}{n}\)</span>, to distribute in an way we see fit.</p>
<p>Suppose we were to scatter these quanta at random among the <span class="math inline">\(m\)</span> choices (penny-pitch game into <span class="math inline">\(m\)</span> equal boxes). If we simply toss these quanta of probability at random, so that each box has an equal probability of getting them, nobody can claim that any box is being unfairly favoured over any other.</p>
<p>If we do this and the first box receives exactly <span class="math inline">\(n_1\)</span> quanta, the second <span class="math inline">\(n_2\)</span> quanta etc. we will say the random experiment has generated the probability assignment:</p>
<p><span class="math display">\[p_i=n_i\delta=\frac{n_i}{n}, \textrm{ with } i=1,2,...,m.\]</span></p>
<p>The probability that this will happen is the multinomial distribution:</p>
<p><span class="math display">\[m^{-n} \frac{n!}{n_1!\cdot...\cdot n_m!}.\]</span></p>
<p>Now imagine that we repeatedly scatter the <span class="math inline">\(n\)</span> quanta at random among the <span class="math inline">\(m\)</span> boxes. Each time we do this we examine the resulting probability assignment. If it happens to conform to the information <span class="math inline">\(I\)</span>, we accept it; otherwise we reject it and try again. We continue until some probability assignment <span class="math inline">\(\{p_1,...,p_m\}\)</span> is accepted.</p>
<p>What is the most likely probability distribution to result from this game? It is the one which maximizes</p>
<p><span class="math display">\[W=\frac{n!}{n_1! \cdot ... \cdot n_m!}\]</span></p>
<p>subject whatever constraints are imposed by the information <span class="math inline">\(I\)</span>.</p>
<p>We can refine this procedure by using smaller quanta, i.e. large <span class="math inline">\(n\)</span>. By using </p>
<p><span class="math display">\[n!\sim \sqrt{(2\pi n)} \left(\frac{n}{e}\right)^n,\]</span>
and taking the logarithm from it:</p>
<p><span class="math display">\[\log(n!) \sim \sqrt{(2\pi n)}+n\log\left(\frac{n}{e}\right),\]</span>
we have</p>
<p><span class="math display">\[\log(n!) \sim \sqrt{(2\pi n)}+n\log(n) - n.\]</span></p>
<p>Taking furthermore, also the logarithm from <span class="math inline">\(W\)</span> and substituting <span class="math inline">\(\log(n!)\)</span> by Sterlings approximation, finally gives the definition of information entropy, as derived by Shannon’s theorem:</p>
<p><span class="math display">\[\frac{1}{n} \log(W) \rightarrow -\sum_{i=1}^{m}p_i\log(p_i)=H(p_1,...,p_m).\]</span></p>
<p><strong>To sum it up:</strong> Entropy is a measure of uncertainty. The higher the entropy of a random variable <span class="math inline">\(X\)</span> the more uncertainty it incorporates. When the goal is to find a maximal ignorance distribution, this goal can be consequently translated into a maximization problem: Find the distribution with maximal entropy subject to existing constraints. This will be topic of the next part of our excursos.</p>
</div>
</div>
<div id="deriving-probability-distributions-using-the-maximum-entropy-principle" class="section level3">
<h3><span class="header-section-number">A.2.2</span> Deriving Probability Distributions using the Maximum Entropy Principle</h3>
<p>The maximum entropy principle is a means of deriving probability distributions given certain constraints and the assumption of maximizing entropy. One technique for solving this maximization problem is the .</p>
<div id="lagrangian-multiplier-technique" class="section level4">
<h4><span class="header-section-number">A.2.2.1</span> Lagrangian multiplier technique</h4>
<p>Given a mutivariable function <span class="math inline">\(f(x,y,...)\)</span> and constraints of the form <span class="math inline">\(g(x,y,...)=c\)</span>, where <span class="math inline">\(g\)</span> is another multivariable function with the same input space as <span class="math inline">\(f\)</span> and <span class="math inline">\(c\)</span> is a constant.</p>
<p>In order to minimize (or maximize) the function <span class="math inline">\(f\)</span> consider the following steps, assuming <span class="math inline">\(f\)</span> to be <span class="math inline">\(f(x)\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Introduce a new variable <span class="math inline">\(\lambda\)</span>, called , and define a new function <span class="math inline">\(\mathcal{L}\)</span> with the form:</li>
</ol>
<p><span class="math display">\[\mathcal{L}(x,\lambda)=f(x)+\lambda (g(x)-c).\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Set the derivative of the function <span class="math inline">\(\mathcal{L}\)</span> equal to the zero:</li>
</ol>
<p><span class="math display">\[\mathcal{L&#39;}(x,\lambda)=0,\]</span></p>
<p>in order to find the critical points of <span class="math inline">\(\mathcal{L}\)</span>.</p>
<ol start="3" style="list-style-type: decimal">
<li>Consider each resulting solution within the limits of the made constraints and derive the resulting distribution <span class="math inline">\(f\)</span>, which gives the minimum (or maximum) one is searching for.</li>
</ol>
<p>For more details see <span class="citation">(Academy <a href="#ref-khanAcademy2019">2019</a>)</span></p>
</div>
<div id="example-1-derivation-of-maximum-entropy-pdf-with-no-other-constraints" class="section level4">
<h4><span class="header-section-number">A.2.2.2</span> Example 1: Derivation of maximum entropy pdf with no other constraints</h4>
<p>For more details see <span class="citation">(Finlayson <a href="#ref-finlayson2017">2017</a>, <span class="citation">@keng2017</span>)</span></p>
<p>Suppose a random variable for which we have absolutely no information on its probability distribution, beside the fact that it should be a pdf and thus, integrate to 1. We ask for the following:</p>
<p><span class="citation">(Reza <a href="#ref-reza1994">1994</a>)</span></p>
<p>We assume that the maximum ignorance distribution is the one with maximum entropy. It minimizes the prior information in a distribution and is therefore the most conservative choice.</p>
<p>For the continuous case entropy, the measure of uncertainty, is defined as</p>
<p><span class="math display">\[H(x)=-\int_{a}^{b}p(x) \log(p(x))dx,\]</span></p>
<p>with subject to the mentioned constraint that the sum of all probabilities is one (as it is a pdf):</p>
<p><span class="math display">\[\int_{a}^{b}p(x)dx =1.\]</span></p>
<p>Rewrite this into the form of  equation gives</p>
<p><span class="math display">\[\mathcal{L}=-\int_{a}^{b}p(x) \log(p(x))dx + \lambda \left(\int_{a}^{b}p(x)dx-1 \right).\]</span></p>
<p>The next step is to  the Lagrangian function. To solve this, we have to use the <span class="citation">(Keng <a href="#ref-keng2017">2017</a>)</span>.</p>
<p>First differentiating <span class="math inline">\(\mathcal{L}\)</span> with respect to <span class="math inline">\(p(x)\)</span></p>
<p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial p(x)}=0,\]</span>
<span class="math display">\[-1-\log(p(x))+\lambda=0,\]</span>
<span class="math display">\[p(x)=e^{(\lambda-1)}.\]</span></p>
<p>Second, the result of <span class="math inline">\(p(x)\)</span> has to satisfy the stated constraint</p>
<p><span class="math display">\[\int_{a}^{b} p(x)dx=1,\]</span></p>
<p><span class="math display">\[\int_{a}^{b} e^{1-\lambda} dx=1.\]</span></p>
<p>Solving this equation with respect to <span class="math inline">\(\lambda\)</span> gives:</p>
<p><span class="math display">\[\lambda=1-\log\left(\frac{1}{b-a}\right).\]</span></p>
<p>Taking both solutions together we get the following probability density function:</p>
<p><span class="math display">\[p(x)=e^{(1-\lambda)}=e^{\left(1-\left(1-\log\left(\frac{1}{b-a}\right)\right)\right)},\]</span></p>
<p><span class="math display">\[p(x)= \frac{1}{b-a}.\]</span></p>
<p>And this is the  on the interval <span class="math inline">\([a,b]\)</span>. Such that, the answer of the above question is:</p>

<p>This should not be too unexpected. As it is quite intuitive that a uniform distribution is the maximal ignorance distribution (when no other constraints were made). The next example will be more exciting.</p>
</div>
<div id="example-2-derivation-of-maximum-entropy-pdf-with-given-mean-mu-and-variance-sigma2" class="section level4">
<h4><span class="header-section-number">A.2.2.3</span> Example 2: Derivation of maximum entropy pdf with given mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span></h4>
<p>Suppose a random variable <span class="math inline">\(X\)</span> with a preassigned standard deviation <span class="math inline">\(\sigma\)</span> and mean <span class="math inline">\(\mu\)</span>. Again the question is: </p>
<p>The Maximum Entropy is defined for the current case as</p>
<p><span class="math display">\[H(X)=-\int_{-\infty}^{\infty} p(x) \log p(x)dx,\]</span></p>
<p>is subject to the constraint that it should be a pdf</p>
<p><span class="math display">\[\int_{-\infty}^{\infty} p(x)dx = 1,\]</span>
and that <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are given (whereby only one constrained is needed, as the <span class="math inline">\(\mu\)</span> is already included in the definition of <span class="math inline">\(\sigma\)</span>):</p>
<p><span class="math display">\[\int_{-\infty}^{\infty}(x-\mu)^2 p(x) dx = \sigma^2.\]</span></p>
<p>Accordingly to the above mentioned technique the formulas are summarized in form of the  equation:</p>
<p><span class="math display">\[\mathcal{L}= -\int_{-\infty}^{\infty} p(x) \log p(x)dx + \lambda_0\left(\int_{-\infty}^{\infty} p(x)dx - 1 \right) + \lambda_1\left(\int_{-\infty}^{\infty}(x-\mu)^2 p(x) dx - \sigma^2 \right).\]</span></p>
<p>Next, <span class="math inline">\(\mathcal{L}\)</span> will be partially differentiated with respect to <span class="math inline">\(p(x)\)</span>:</p>
<p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial p(x)}=0,\]</span>
<span class="math display">\[-(1+\log p(x))+\lambda_0+\lambda_1 (x-\mu)^2=0,\]</span></p>
<p><span class="math display">\[p(x)=e^{\lambda_0+\lambda_1 (x-\mu)^2-1}.\]</span></p>
<p>Further we have to make sure that the result holds for the stated constraints:</p>
<p><span class="math display">\[\int_{-\infty}^{\infty} e^{\lambda_0+\lambda_1 (x-\mu)^2-1}-1 dx = 1,\]</span></p>
<p>and</p>
<p><span class="math display">\[\int_{-\infty}^{\infty}(x-\mu)^2 e^{\lambda_0+\lambda_1 (x-\mu)^2-1} dx = \sigma^2.\]</span></p>
<p>For the first constraint we get</p>
<p><span class="math display">\[e^{\lambda_0-1} \sqrt{-\frac{\pi}{\lambda_1}} = 1,\]</span></p>
<p>and for the second constraint</p>
<p><span class="math display">\[e^{\lambda_0-1} = \sqrt{\frac{1}{2\pi}} \frac{1}{\sigma},\]</span></p>
<p>Thus</p>
<p><span class="math display">\[\lambda_1=\frac{-1}{2\sigma^2}\]</span></p>
<p>Taking all together we can write:</p>
<p><span class="math display">\[p(x)=e^{\lambda_0+\lambda_1 (x-\mu)^2-1}=e^{\lambda_0-1}e^{\lambda_1 (x-\mu)^2},\]</span></p>
<p>substituting the solutions for <span class="math inline">\(e^{\lambda_0-1}\)</span> and <span class="math inline">\(\lambda_1\)</span>:</p>
<p><span class="math display">\[p(x)= \sqrt{\frac{1}{2\pi}} \frac{1}{\sigma} e^{\frac{-1}{2\sigma^2}(x-\mu)^2},\]</span></p>
<p>finally we can rearrange the terms a bit and get:</p>
<p><span class="math display">\[p(x)= \frac{1}{\sigma\sqrt{2\pi}}\exp{\left(\frac{-1}{2}\left(\frac{(x-\mu)^2}{\sigma^2}\right)\right)},\]</span></p>
<p>the .</p>
<p><strong>To sum it up:</strong></p>
<p>If one is to infer a probability distribution given certain constraints, out of all distributions <span class="math inline">\(\{p_i\}\)</span> compatible with them, one should pick the distribution <span class="math inline">\(\{p_i^*\}\)</span> having the largest value of <span class="math inline">\(H\)</span> <span class="citation">(De Martino and De Martino <a href="#ref-deMartino2018">2018</a>)</span>. In other terms, a Maximum Entropy distribution is completely undetermined by features that do not appear explicitly in the constraints subject to which it has been computed.</p>
<p>An <strong>overview of Maximum Entropy distributions</strong> can be found on <a href="https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution">Wikipedia</a>.</p>
<!-- Common examples of non-exponential families arising from exponential ones are the Student's t-distribution, beta-binomial distribution -->
<!-- ![Alt](https://github.com/michael-franke/intro-data-analysis/tree/master/chapters/images/distributions.png "distributions") -->
</div>
</div>
</div>
<div id="selected-continous-distributions-of-random-variables" class="section level2">
<h2><span class="header-section-number">A.3</span> Selected continous distributions of random variables</h2>
<div id="normal-distribution" class="section level3">
<h3><span class="header-section-number">A.3.1</span> Normal distribution</h3>
<p><span class="math display">\[X\sim Normal(\mu,\sigma^2)\]</span></p>
<p>[usage of Normal distribution -&gt; normal by addition, multiplication, log-multiplication]</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb27-1" data-line-number="1">n &lt;-<span class="st"> </span><span class="fl">1e6</span></a>
<a class="sourceLine" id="cb27-2" data-line-number="2"></a>
<a class="sourceLine" id="cb27-3" data-line-number="3">rv.normal.intro &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</a>
<a class="sourceLine" id="cb27-4" data-line-number="4">  <span class="dt">X =</span> <span class="kw">rnorm</span>(n, <span class="dt">mean=</span><span class="op">-</span><span class="dv">2</span>, <span class="dt">sd=</span><span class="dv">3</span>),</a>
<a class="sourceLine" id="cb27-5" data-line-number="5">  <span class="dt">Y =</span> <span class="kw">rnorm</span>(n, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>),</a>
<a class="sourceLine" id="cb27-6" data-line-number="6">  <span class="dt">Z =</span> <span class="kw">rnorm</span>(n, <span class="dt">mean=</span><span class="dv">2</span>, <span class="dt">sd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb27-7" data-line-number="7">)</a>
<a class="sourceLine" id="cb27-8" data-line-number="8"></a>
<a class="sourceLine" id="cb27-9" data-line-number="9"><span class="kw">ggplot</span>(<span class="dt">data=</span>rv.normal.intro, <span class="dt">size=</span><span class="dv">1</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb27-10" data-line-number="10"><span class="st">  </span><span class="kw">stat_density</span>(<span class="kw">aes</span>(X), <span class="dt">geom =</span> <span class="st">&quot;line&quot;</span>, <span class="dt">position=</span><span class="st">&quot;identity&quot;</span>, <span class="dt">color=</span><span class="st">&quot;green&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb27-11" data-line-number="11"><span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x=</span><span class="op">-</span><span class="dv">10</span>, <span class="dt">y=</span><span class="fl">0.1</span>, <span class="dt">label=</span><span class="st">&quot;X~Normal(-2,3)&quot;</span>, <span class="dt">color=</span><span class="st">&quot;green&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb27-12" data-line-number="12"><span class="st">  </span><span class="kw">stat_density</span>(<span class="kw">aes</span>(Y), <span class="dt">geom =</span> <span class="st">&quot;line&quot;</span>, <span class="dt">position=</span><span class="st">&quot;identity&quot;</span>, <span class="dt">color=</span><span class="st">&quot;red&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb27-13" data-line-number="13"><span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x=</span><span class="fl">8.5</span>, <span class="dt">y=</span><span class="fl">0.3</span>, <span class="dt">label=</span><span class="st">&quot;Y~Normal(0,1) = standard normal&quot;</span>, <span class="dt">color=</span><span class="st">&quot;red&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb27-14" data-line-number="14"><span class="st">  </span><span class="kw">stat_density</span>(<span class="kw">aes</span>(Z), <span class="dt">geom =</span> <span class="st">&quot;line&quot;</span>, <span class="dt">position=</span><span class="st">&quot;identity&quot;</span>, <span class="dt">color=</span><span class="st">&quot;blue&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb27-15" data-line-number="15"><span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x=</span><span class="dv">8</span>, <span class="dt">y=</span><span class="fl">0.15</span>, <span class="dt">label=</span><span class="st">&quot;Y~Normal(2,2)&quot;</span>, <span class="dt">color=</span><span class="st">&quot;blue&quot;</span>)</a></code></pre></div>
<p><img src="I2DA_files/figure-html/unnamed-chunk-23-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Special case of normal distributed random variables is the <em>standard normal</em> distributed variable with <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma=1\)</span>: <span class="math inline">\(Y\sim Normal(0,1)\)</span></p>
<p><strong>Probability density function</strong></p>
<p><span class="math display">\[f(x)=\frac{1}{\sigma\sqrt{2\pi}}\exp\left(-0.5\left(\frac{x-\mu}{\sigma}\right)^2\right)\]</span></p>
<p><strong>Cumulative distribution function</strong></p>
<p><span class="math display">\[F(x)=\int_{-\inf}^{x}f(t)dt\]</span></p>
<p><strong>Expected value</strong> <span class="math inline">\(E(X)=\mu\)</span></p>
<p><strong>Variance</strong> <span class="math inline">\(Var(X)=\sigma^2\)</span></p>
<p><strong>Z-transformation</strong> <span class="math inline">\(Z=\frac{X-\mu}{\sigma}\)</span></p>
<p>**Deviation and *Coverage**</p>
<ul>
<li><span class="math inline">\(P(\mu-\sigma \leq X \leq \mu+\sigma)=0.6827\)</span> (see dark blue area under the curve)</li>
<li><span class="math inline">\(P(\mu-2\sigma \leq X \leq \mu+2\sigma)=0.9545\)</span> (see medium blue area under the curve)</li>
<li><span class="math inline">\(P(\mu-3\sigma \leq X \leq \mu+3\sigma)=0.9973\)</span> (see light blue area under the curve)</li>
</ul>
<p>(Interpretation example: About 68% of values drawn from a normal distribution are within one standard deviation <span class="math inline">\(\sigma\)</span> away from the mean <span class="math inline">\(\mu\)</span>.)</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb28-1" data-line-number="1"><span class="co"># plot standard deviations of a normal distribution</span></a>
<a class="sourceLine" id="cb28-2" data-line-number="2"></a>
<a class="sourceLine" id="cb28-3" data-line-number="3"><span class="kw">ggplot</span>(<span class="ot">NULL</span>, <span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>))) <span class="op">+</span></a>
<a class="sourceLine" id="cb28-4" data-line-number="4"><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">2</span>),</a>
<a class="sourceLine" id="cb28-5" data-line-number="5">                <span class="dt">geom =</span> <span class="st">&quot;line&quot;</span>,</a>
<a class="sourceLine" id="cb28-6" data-line-number="6">                <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb28-7" data-line-number="7"><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">2</span>),</a>
<a class="sourceLine" id="cb28-8" data-line-number="8">                <span class="dt">geom =</span> <span class="st">&quot;area&quot;</span>,</a>
<a class="sourceLine" id="cb28-9" data-line-number="9">                <span class="dt">fill =</span> <span class="st">&quot;blue&quot;</span>,</a>
<a class="sourceLine" id="cb28-10" data-line-number="10">                <span class="dt">alpha =</span> <span class="fl">.2</span>,</a>
<a class="sourceLine" id="cb28-11" data-line-number="11">                <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb28-12" data-line-number="12"><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">2</span>),</a>
<a class="sourceLine" id="cb28-13" data-line-number="13">                <span class="dt">geom =</span> <span class="st">&quot;area&quot;</span>,</a>
<a class="sourceLine" id="cb28-14" data-line-number="14">                <span class="dt">fill =</span> <span class="st">&quot;blue&quot;</span>,</a>
<a class="sourceLine" id="cb28-15" data-line-number="15">                <span class="dt">alpha =</span> <span class="fl">.4</span>,</a>
<a class="sourceLine" id="cb28-16" data-line-number="16">                <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb28-17" data-line-number="17"><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">2</span>),</a>
<a class="sourceLine" id="cb28-18" data-line-number="18">                <span class="dt">geom =</span> <span class="st">&quot;area&quot;</span>,</a>
<a class="sourceLine" id="cb28-19" data-line-number="19">                <span class="dt">fill =</span> <span class="st">&quot;blue&quot;</span>,</a>
<a class="sourceLine" id="cb28-20" data-line-number="20">                <span class="dt">alpha =</span> <span class="fl">.6</span>,</a>
<a class="sourceLine" id="cb28-21" data-line-number="21">                <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb28-22" data-line-number="22"><span class="st">  </span><span class="kw">xlim</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb28-23" data-line-number="23"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;X&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb28-24" data-line-number="24"><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">6</span>,<span class="op">-</span><span class="dv">4</span>,<span class="op">-</span><span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">6</span>), </a>
<a class="sourceLine" id="cb28-25" data-line-number="25">                     <span class="dt">labels=</span><span class="kw">c</span>(<span class="kw">expression</span>(<span class="op">-</span><span class="dv">3</span><span class="op">~</span>sigma),<span class="kw">expression</span>(<span class="op">-</span><span class="dv">2</span><span class="op">~</span>sigma),</a>
<a class="sourceLine" id="cb28-26" data-line-number="26">                              <span class="kw">expression</span>(<span class="op">-</span>sigma),<span class="st">&quot;0&quot;</span>,<span class="kw">expression</span>(sigma),</a>
<a class="sourceLine" id="cb28-27" data-line-number="27">                              <span class="kw">expression</span>(<span class="dv">2</span><span class="op">~</span>sigma),<span class="kw">expression</span>(<span class="dv">3</span><span class="op">~</span>sigma)))</a></code></pre></div>
<p><img src="I2DA_files/figure-html/unnamed-chunk-24-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><strong>Linear transformations</strong></p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(X\sim Normal(\mu, \sigma^2)\)</span> is linear transformed by <span class="math inline">\(Y=a*X+b\)</span>, then the new random variable is again normal distributed with <span class="math inline">\(Y \sim Normal(a\mu+b,a^2\sigma^2)\)</span>. See plot left side.</li>
<li>Are <span class="math inline">\(X\sim Normal(\mu_x, \sigma^2)\)</span> and <span class="math inline">\(Y\sim Normal(\mu_y, \sigma^2)\)</span> normal distributed and independent, then their sum is again normal distributed with <span class="math inline">\(X+Y \sim Normal(\mu_x+\mu_y, \sigma_x^2+\sigma_y^2)\)</span>. See plot right side.</li>
</ol>
</div>
<div id="chi-square-distribution" class="section level3">
<h3><span class="header-section-number">A.3.2</span> Chi-square distribution</h3>
<p><span class="math display">\[Y\sim \chi^2(n)\]</span></p>
<p>The <span class="math inline">\(\chi^2\)</span>-distribution is widely used in hypothesis testing in inferential statistics and less in modelling natural phenomena. The reason why it is used so often in inferential statistics is its relationsship to the standard normal distribution:</p>
<p>The sum of <span class="math inline">\(n\)</span> independent and standard normal distributed random variables <span class="math inline">\(X_1,X_2,...,X_n\)</span> is distributed according to a <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(n\)</span> . The degrees of freedom refers to the number of independent standard normal random variables.</p>
<p><span class="math display">\[Y=X_1^2+X_2^2+...+X_n^2\]</span></p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb29-1" data-line-number="1"><span class="kw">plot</span>(<span class="kw">density</span>(<span class="kw">rchisq</span>(<span class="fl">1e6</span>,<span class="dv">3</span>)),<span class="dt">col=</span><span class="st">&quot;green&quot;</span>, <span class="dt">main=</span><span class="kw">expression</span>(chi<span class="op">^</span><span class="dv">2</span><span class="op">~</span><span class="st">&quot;distribution&quot;</span>), <span class="dt">xlab=</span><span class="st">&quot;RVs&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb29-2" data-line-number="2"><span class="st">  </span><span class="kw">points</span>(<span class="kw">density</span>(<span class="kw">rchisq</span>(<span class="fl">1e6</span>,<span class="dv">10</span>)),<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb29-3" data-line-number="3"><span class="st">  </span><span class="kw">points</span>(<span class="kw">density</span>(<span class="kw">rchisq</span>(<span class="fl">1e6</span>,<span class="dv">20</span>)),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</a></code></pre></div>
<p><img src="I2DA_files/figure-html/unnamed-chunk-25-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>## integer(0)</code></pre>
<p><strong>Expected value</strong> <span class="math inline">\(E(Y)=n\)</span></p>
<p><strong>Variance</strong> <span class="math inline">\(Var(Y)=2n\)</span></p>
<p><strong>Transformations</strong>
Sum of two <span class="math inline">\(\chi^2\)</span>-distributed random variables <span class="math inline">\(X \sim \chi^2(m)\)</span> and <span class="math inline">\(Y \sim \chi^2(n)\)</span> is again a <span class="math inline">\(chi^2\)</span>-distributed random variable <span class="math inline">\(X+Y=\chi^2(m+n)\)</span>.</p>
</div>
<div id="f-distribution" class="section level3">
<h3><span class="header-section-number">A.3.3</span> F distribution</h3>
<p><span class="math display">\[F \sim F(m,n)\]</span></p>
<p>Used in particular in regression and variance analysis and consists of two <span class="math inline">\(chi^2\)</span>-distributed random variables <span class="math inline">\(X\sim \chi^2(m)\)</span> and <span class="math inline">\(Y\sim \chi^2(n)\)</span>:</p>
<p><span class="math display">\[F=\frac{\frac{X}{m}}{\frac{Y}{n}}\]</span></p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb31-1" data-line-number="1"><span class="kw">plot</span>(<span class="kw">density</span>(<span class="kw">rf</span>(<span class="fl">1e6</span>,<span class="dv">10</span>,<span class="dv">10</span>)),<span class="dt">col=</span><span class="st">&quot;green&quot;</span>, <span class="dt">main=</span><span class="st">&quot;F distribution&quot;</span>,<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">2</span>),<span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">5</span>), </a>
<a class="sourceLine" id="cb31-2" data-line-number="2">     <span class="dt">xlab=</span><span class="st">&quot;RVs&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb31-3" data-line-number="3"><span class="st">  </span><span class="kw">points</span>(<span class="kw">density</span>(<span class="kw">rf</span>(<span class="fl">1e6</span>,<span class="dv">20</span>,<span class="dv">20</span>)),<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb31-4" data-line-number="4"><span class="st">  </span><span class="kw">points</span>(<span class="kw">density</span>(<span class="kw">rf</span>(<span class="fl">1e6</span>,<span class="dv">100</span>,<span class="dv">100</span>)),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</a></code></pre></div>
<p><img src="I2DA_files/figure-html/unnamed-chunk-26-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>## integer(0)</code></pre>
<p><strong>Expected value</strong> <span class="math inline">\(E(F)=\frac{n}{n-2}\)</span> (for <span class="math inline">\(n \geq 3\)</span>)</p>
<p><strong>Variance</strong> <span class="math inline">\(Var(F)=\frac{2n^2(n+m-2)}{m(n-4)(n-2)^2}\)</span> (for <span class="math inline">\(n \geq 5\)</span>)</p>
</div>
<div id="student-t-distribution-not-part-of-exponential-family" class="section level3">
<h3><span class="header-section-number">A.3.4</span> Student t-distribution (not part of exponential family)</h3>
<p><span class="math display">\[T \sim t(n)\]</span></p>
<p>The student-t-distribution is not part of the exponential family but is directly related to the normal distribution. It is used in particular when the sample size is small and the variance is unknown, which is often the case in reality. Its shape ressembles the normal bell shape, but the student-t-distribution is a bit lower and wider (bigger tails). For a large sample size (<span class="math inline">\(n \geq 30\)</span>) the student-t-distribution can be approximated by a standard normal distribution.</p>
<p>It consists of a standard normal distributed random variable <span class="math inline">\(X\sim Normal(0,1)\)</span> and a <span class="math inline">\(\chi^2\)</span>-distributed random variable <span class="math inline">\(Y\sim \chi^2(n)\)</span> (X and Y are independent):</p>
<p><span class="math display">\[T=\frac{X}{\sqrt{\frac{Y}{n}}}\]</span></p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb33-1" data-line-number="1"><span class="kw">plot</span>(<span class="kw">density</span>(<span class="kw">rt</span>(<span class="fl">1e6</span>,<span class="dv">3</span>)),<span class="dt">col=</span><span class="st">&quot;green&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Student-t distribution&quot;</span>,<span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>), </a>
<a class="sourceLine" id="cb33-2" data-line-number="2">     <span class="dt">xlab=</span><span class="st">&quot;RVs&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb33-3" data-line-number="3"><span class="st">  </span><span class="kw">points</span>(<span class="kw">density</span>(<span class="kw">rt</span>(<span class="fl">1e6</span>,<span class="fl">3.5</span>)),<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb33-4" data-line-number="4"><span class="st">  </span><span class="kw">points</span>(<span class="kw">density</span>(<span class="kw">rt</span>(<span class="fl">1e6</span>,<span class="dv">15</span>)),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb33-5" data-line-number="5"><span class="st">  </span><span class="kw">points</span>(<span class="kw">density</span>(<span class="kw">rnorm</span>(<span class="fl">1e6</span>)), <span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</a></code></pre></div>
<p><img src="I2DA_files/figure-html/unnamed-chunk-27-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>## integer(0)</code></pre>
<p><strong>Expected value</strong> $E(T)=0</p>
<p><strong>Variance</strong> <span class="math inline">\(Var(T)=\frac{n}{n-2}\)</span> (for <span class="math inline">\(n \geq 3\)</span>)</p>
</div>
</div>
<div id="selected-discrete-distributions-of-random-variables" class="section level2">
<h2><span class="header-section-number">A.4</span> Selected discrete distributions of random variables</h2>
<div id="binomial-distribution" class="section level3">
<h3><span class="header-section-number">A.4.1</span> Binomial distribution</h3>
<p><span class="math display">\[X \sim Binomial(n,p)\]</span></p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb35-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb35-2" data-line-number="2"><span class="kw">hist</span>(<span class="kw">rbinom</span>(<span class="fl">1e6</span>,<span class="dv">15</span>,<span class="fl">0.1</span>),<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">freq =</span> <span class="ot">FALSE</span>,<span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">15</span>),<span class="dt">xlab=</span><span class="st">&quot;RV&quot;</span>, </a>
<a class="sourceLine" id="cb35-3" data-line-number="3">     <span class="dt">main=</span><span class="st">&quot;Binomial(n,0.1)&quot;</span>)</a>
<a class="sourceLine" id="cb35-4" data-line-number="4"><span class="kw">hist</span>(<span class="kw">rbinom</span>(<span class="fl">1e6</span>,<span class="dv">15</span>,<span class="fl">0.5</span>),<span class="dt">col=</span><span class="st">&quot;green&quot;</span>, <span class="dt">freq =</span> <span class="ot">FALSE</span>,<span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">15</span>),<span class="dt">xlab=</span><span class="st">&quot;RV&quot;</span>, </a>
<a class="sourceLine" id="cb35-5" data-line-number="5">     <span class="dt">main=</span><span class="st">&quot;Binomial(n,0.5)&quot;</span>)</a>
<a class="sourceLine" id="cb35-6" data-line-number="6"><span class="kw">hist</span>(<span class="kw">rbinom</span>(<span class="fl">1e6</span>,<span class="dv">15</span>,<span class="fl">0.8</span>),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">freq =</span> <span class="ot">FALSE</span>,<span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">15</span>),<span class="dt">xlab=</span><span class="st">&quot;RV&quot;</span>, </a>
<a class="sourceLine" id="cb35-7" data-line-number="7">     <span class="dt">main=</span><span class="st">&quot;Binomial(n,0.8)&quot;</span>)</a></code></pre></div>
<p><img src="I2DA_files/figure-html/unnamed-chunk-28-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><strong>Probability mass function</strong></p>
<p><span class="math display">\[f(x)=\binom{n}{x}p^x(1-p)^{n-x}\]</span>
<strong>Cumulative function</strong></p>
<p><span class="math display">\[F(x)=\sum_{k=0}^{x}\binom{n}{k}p^k(1-p)^{n-k}\]</span></p>
<p><strong>Expected value</strong> <span class="math inline">\(E(X)=n \cdot p\)</span></p>
<p><strong>Variance</strong> <span class="math inline">\(Var(X)=n \cdot p \cdot (1-p)\)</span></p>
</div>
<div id="poisson-distribution" class="section level3">
<h3><span class="header-section-number">A.4.2</span> Poisson distribution</h3>
<p><span class="math display">\[X \sim Po(\lambda)\]</span></p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb36-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb36-2" data-line-number="2"><span class="kw">hist</span>(<span class="kw">rpois</span>(<span class="fl">1e6</span>,<span class="fl">3.5</span>),<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">freq =</span> <span class="ot">FALSE</span>,<span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">40</span>),<span class="dt">xlab=</span><span class="st">&quot;RV&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Poisson(3.5)&quot;</span>)</a>
<a class="sourceLine" id="cb36-3" data-line-number="3"><span class="kw">hist</span>(<span class="kw">rpois</span>(<span class="fl">1e6</span>,<span class="dv">6</span>),<span class="dt">col=</span><span class="st">&quot;green&quot;</span>, <span class="dt">freq =</span> <span class="ot">FALSE</span>,<span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">40</span>),<span class="dt">xlab=</span><span class="st">&quot;RV&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Poisson(6)&quot;</span>)</a>
<a class="sourceLine" id="cb36-4" data-line-number="4"><span class="kw">hist</span>(<span class="kw">rpois</span>(<span class="fl">1e6</span>,<span class="dv">20</span>),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">freq =</span> <span class="ot">FALSE</span>,<span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">40</span>),<span class="dt">xlab=</span><span class="st">&quot;RV&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Poisson(20)&quot;</span>)</a></code></pre></div>
<p><img src="I2DA_files/figure-html/unnamed-chunk-29-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><strong>Probability mass function</strong></p>
<p><span class="math display">\[f(x)=\frac{\lambda^x}{x!}e^{-\lambda}\]</span></p>
<p><strong>Cumulative function</strong></p>
<p><span class="math display">\[F(x)=\sum_{k=0}^{x}\frac{\lambda^k}{k!}e^{-\lambda}\]</span></p>
<p><strong>Expected value</strong> <span class="math inline">\(E(X)= \lambda\)</span></p>
<p><strong>Variance</strong> <span class="math inline">\(Var(X)=\lambda\)</span></p>
</div>
</div>
<div id="understanding-distributions-as-random-variables" class="section level2">
<h2><span class="header-section-number">A.5</span> Understanding distributions as random variables</h2>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb37-1" data-line-number="1"><span class="co">#initialize parameters</span></a>
<a class="sourceLine" id="cb37-2" data-line-number="2">a =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb37-3" data-line-number="3">b =<span class="st"> </span><span class="fl">1.5</span></a>
<a class="sourceLine" id="cb37-4" data-line-number="4">n =<span class="st"> </span><span class="fl">1e6</span></a>
<a class="sourceLine" id="cb37-5" data-line-number="5"></a>
<a class="sourceLine" id="cb37-6" data-line-number="6"><span class="co">#create random variables</span></a>
<a class="sourceLine" id="cb37-7" data-line-number="7">## normal distributed RVs </a>
<a class="sourceLine" id="cb37-8" data-line-number="8">X_norm &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> n, <span class="dt">mean =</span> <span class="dv">1</span>, <span class="dt">sd =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb37-9" data-line-number="9">Y_norm &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> n, <span class="dt">mean =</span> <span class="fl">1.5</span>, <span class="dt">sd =</span> <span class="fl">2.5</span>)</a>
<a class="sourceLine" id="cb37-10" data-line-number="10">## standard normal distributed RVs</a>
<a class="sourceLine" id="cb37-11" data-line-number="11">stdNormal1 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> n, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb37-12" data-line-number="12">stdNormal2 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> n, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb37-13" data-line-number="13">stdNormal3 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> n, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb37-14" data-line-number="14">## chi-square distributed RV</a>
<a class="sourceLine" id="cb37-15" data-line-number="15">C &lt;-<span class="st"> </span><span class="kw">rchisq</span>(<span class="dt">n=</span>n, <span class="dt">df=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb37-16" data-line-number="16">## student-t distributed RV</a>
<a class="sourceLine" id="cb37-17" data-line-number="17">student &lt;-<span class="st"> </span><span class="kw">rt</span>(<span class="dt">n=</span><span class="dv">20</span>, <span class="dt">df=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb37-18" data-line-number="18">## F distributed RV</a>
<a class="sourceLine" id="cb37-19" data-line-number="19">fisher &lt;-<span class="st"> </span><span class="kw">rf</span>(<span class="dt">n=</span>n, <span class="dt">df1=</span><span class="dv">1</span>, <span class="dt">df2=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb37-20" data-line-number="20"></a>
<a class="sourceLine" id="cb37-21" data-line-number="21">## create linear transformation of X</a>
<a class="sourceLine" id="cb37-22" data-line-number="22">X_lin &lt;-<span class="st"> </span>a<span class="op">*</span>X_norm<span class="op">+</span>b</a>
<a class="sourceLine" id="cb37-23" data-line-number="23">## create RV A as addition of X + Y</a>
<a class="sourceLine" id="cb37-24" data-line-number="24">sum_xy &lt;-<span class="st"> </span>X_norm <span class="op">+</span><span class="st"> </span>Y_norm</a>
<a class="sourceLine" id="cb37-25" data-line-number="25">## create chisquare distributed RV </a>
<a class="sourceLine" id="cb37-26" data-line-number="26">C2 &lt;-<span class="st"> </span>stdNormal2<span class="op">^</span><span class="dv">2</span><span class="op">+</span>stdNormal3<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb37-27" data-line-number="27">C3 &lt;-<span class="st"> </span>stdNormal1<span class="op">^</span><span class="dv">2</span><span class="op">+</span>stdNormal2<span class="op">^</span><span class="dv">2</span><span class="op">+</span>stdNormal3<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb37-28" data-line-number="28">## create student-t distributed RV</a>
<a class="sourceLine" id="cb37-29" data-line-number="29">T1 &lt;-<span class="st"> </span>stdNormal1<span class="op">/</span><span class="kw">sqrt</span>(C2<span class="op">/</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb37-30" data-line-number="30">## create F distributed RV</a>
<a class="sourceLine" id="cb37-31" data-line-number="31">F1 &lt;-<span class="st"> </span>(C2<span class="op">/</span><span class="dv">2</span>)<span class="op">/</span>(C3<span class="op">/</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb37-32" data-line-number="32"></a>
<a class="sourceLine" id="cb37-33" data-line-number="33">## normal ditributed RV B (equal to addition: X+Y)</a>
<a class="sourceLine" id="cb37-34" data-line-number="34">N_add &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> n, <span class="dt">mean =</span> <span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="fl">1.5</span>, <span class="dt">sd =</span> <span class="kw">sqrt</span>(<span class="dv">2</span><span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="fl">2.5</span><span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb37-35" data-line-number="35">## normal distributed RV V (equal to linear transformation: a*X+b)</a>
<a class="sourceLine" id="cb37-36" data-line-number="36">N_lin &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> n, <span class="dt">mean =</span> a<span class="op">*</span><span class="dv">1</span><span class="op">+</span>b, <span class="dt">sd =</span> (a<span class="op">*</span><span class="dv">2</span>))</a></code></pre></div>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb38-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb38-2" data-line-number="2"><span class="kw">plot</span>(<span class="kw">density</span>(X_norm), <span class="dt">main=</span><span class="st">&quot;Normal RV: a*X+b&quot;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">20</span>,<span class="dv">20</span>),<span class="dt">xlab=</span><span class="st">&quot;RVs&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb38-3" data-line-number="3"><span class="st">  </span><span class="kw">points</span>(<span class="kw">density</span>(N_lin), <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb38-4" data-line-number="4"><span class="kw">plot</span>(<span class="kw">density</span>(X_norm), <span class="dt">main=</span><span class="st">&quot;Normal RV: X+Y&quot;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">20</span>,<span class="dv">20</span>),<span class="dt">xlab=</span><span class="st">&quot;RVs&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb38-5" data-line-number="5"><span class="st">  </span><span class="kw">points</span>(<span class="kw">density</span>(Y_norm), <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">lty=</span><span class="st">&quot;dotted&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb38-6" data-line-number="6"><span class="st">  </span><span class="kw">points</span>(<span class="kw">density</span>(N_add), <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</a></code></pre></div>
<p><img src="I2DA_files/figure-html/unnamed-chunk-31-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>## integer(0)</code></pre>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb40-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb40-2" data-line-number="2"><span class="kw">plot</span>(<span class="kw">density</span>(stdNormal1), <span class="dt">main=</span><span class="kw">expression</span>(<span class="st">&quot;X,Y,Z ~N(0,1);&quot;</span><span class="op">~</span>X<span class="op">^</span><span class="dv">2</span><span class="op">+</span>Y<span class="op">^</span><span class="dv">2</span><span class="op">+</span>Z<span class="op">^</span><span class="dv">2</span>),   </a>
<a class="sourceLine" id="cb40-3" data-line-number="3">     <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>),<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.4</span>),<span class="dt">xlab=</span><span class="st">&quot;RVs&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb40-4" data-line-number="4"><span class="st">  </span><span class="kw">points</span>(<span class="kw">density</span>(C3), <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</a></code></pre></div>
<pre><code>## integer(0)</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb42-1" data-line-number="1"><span class="kw">plot</span>(<span class="kw">density</span>(C), <span class="dt">main=</span><span class="st">&quot;Chi-square distribution&quot;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">15</span>),<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.4</span>),</a>
<a class="sourceLine" id="cb42-2" data-line-number="2">     <span class="dt">xlab=</span><span class="st">&quot;RVs&quot;</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="st">&quot;dashed&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb42-3" data-line-number="3"><span class="st">  </span><span class="kw">points</span>(<span class="kw">density</span>(C3), <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</a></code></pre></div>
<p><img src="I2DA_files/figure-html/unnamed-chunk-31-2.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>## integer(0)</code></pre>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb44-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb44-2" data-line-number="2"><span class="kw">plot</span>(<span class="kw">density</span>(stdNormal1), <span class="dt">main=</span><span class="st">&quot;Normal devided by Chi-square RVs&quot;</span>,   </a>
<a class="sourceLine" id="cb44-3" data-line-number="3">     <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>),<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.4</span>),<span class="dt">xlab=</span><span class="st">&quot;RVs&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb44-4" data-line-number="4"><span class="st">  </span><span class="kw">points</span>(<span class="kw">density</span>(C), <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb44-5" data-line-number="5"><span class="st">  </span><span class="kw">points</span>(<span class="kw">density</span>(T1),<span class="dt">type=</span><span class="st">&quot;l&quot;</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</a></code></pre></div>
<pre><code>## integer(0)</code></pre>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb46-1" data-line-number="1"><span class="kw">plot</span>(<span class="kw">density</span>(T1), <span class="dt">main=</span><span class="st">&quot;Student-t distribution&quot;</span>,   </a>
<a class="sourceLine" id="cb46-2" data-line-number="2">     <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>),<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.4</span>),<span class="dt">xlab=</span><span class="st">&quot;RVs&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb46-3" data-line-number="3"><span class="st">  </span><span class="kw">points</span>(<span class="kw">density</span>(student),<span class="dt">type=</span><span class="st">&quot;l&quot;</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<pre><code>## integer(0)</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb48-1" data-line-number="1"><span class="kw">plot</span>(<span class="kw">density</span>(stdNormal1), <span class="dt">main=</span><span class="kw">expression</span>(<span class="st">&quot;X~N(0,1),&quot;</span><span class="op">~</span>Y<span class="op">~</span><span class="st">&quot;~&quot;</span><span class="op">~</span>chi<span class="op">^</span><span class="dv">2</span><span class="op">~</span>(<span class="dv">2</span>)<span class="op">~</span><span class="st">&quot;;&quot;</span><span class="op">~</span>X<span class="op">/</span><span class="kw">sqrt</span>(Y<span class="op">/</span>n)),   </a>
<a class="sourceLine" id="cb48-2" data-line-number="2">     <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>),<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.4</span>),<span class="dt">xlab=</span><span class="st">&quot;RVs&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb48-3" data-line-number="3"><span class="st">  </span><span class="kw">points</span>(<span class="kw">density</span>(C2), <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb48-4" data-line-number="4"><span class="st">  </span><span class="kw">points</span>(<span class="kw">density</span>(T1), <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</a></code></pre></div>
<pre><code>## integer(0)</code></pre>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb50-1" data-line-number="1"><span class="kw">plot</span>(<span class="kw">density</span>(stdNormal1), <span class="dt">main=</span><span class="st">&quot;Student-t distribution&quot;</span>,   </a>
<a class="sourceLine" id="cb50-2" data-line-number="2">     <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>),<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.4</span>),<span class="dt">xlab=</span><span class="st">&quot;RVs&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb50-3" data-line-number="3"><span class="st">  </span><span class="kw">points</span>(<span class="kw">density</span>(student), <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<p><img src="I2DA_files/figure-html/unnamed-chunk-32-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>## integer(0)</code></pre>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-khanAcademy2019">
<p>Academy, Khan. 2019. “Lagrange multipliers, introduction.” 2019. <a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-single-constraint" class="uri">https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-single-constraint</a>.</p>
</div>
<div id="ref-deMartino2018">
<p>De Martino, Andrea, and Daniele De Martino. 2018. “An Introduction to the Maximum Entropy Approach and its Application to Inference Problems in Biology.” <em>Heliyon</em> 4 (4). Elsevier.</p>
</div>
<div id="ref-finlayson2017">
<p>Finlayson, Sam. 2017. “Deriving probability distributions using the Principal of Maximum Entropy.” 2017. <a href="https://sgfin.github.io/2017/03/16/Deriving-probability-distributions-using-the-Principle-of-Maximum-Entropy/#introduction ">https://sgfin.github.io/2017/03/16/Deriving-probability-distributions-using-the-Principle-of-Maximum-Entropy/#introduction</a>.</p>
</div>
<div id="ref-jaynes2003">
<p>Jaynes, Edwin T. 2003. <em>Probability Theory: The Logic of Science</em>. Cambridge university press.</p>
</div>
<div id="ref-keng2017">
<p>Keng, Brian. 2017. “Maximum Entropy Distributions.” 2017. <a href="http://bjlkeng.github.io/posts/maximum-entropy-distributions/ ">http://bjlkeng.github.io/posts/maximum-entropy-distributions/</a>.</p>
</div>
<div id="ref-reza1994">
<p>Reza, Fazlollah M. 1994. <em>An Introduction to Information Theory</em>. Courier Corporation.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="hierarchical-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["I2DA.epub", "I2DA.pdf"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
