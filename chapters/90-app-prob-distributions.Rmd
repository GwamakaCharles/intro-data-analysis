
# (APPENDIX) Appendix {-} 

# Common probability distributions

## Selected continous distributions of random variables

### Normal distribution
One of the most important distribution families is the *gaussian* or *normal family* because it fits many natural phenomena. Furthermore the sampling distributions of many estimators depend on the normal distribution. On the one hand because they are derived from normally distributed random variables or on the other hand because they can be asymptotically approximated by a normal distribution for large samples (*Central limit theorem*). 

Distributions of the normal family are symmetric with range $(-\infty,+\infty)$, depending on two parameters $\mu$ and $\sigma$ that are referred to, respectively, as the *mean* and the *standard deviation* of the normal random variable. These parameters are examples of *location* and *scale* parameters. The normal distribution is located at $\mu$ and its width is scaled by choice of $\sigma$. The distribution is symmetric with most observations lying aroung the central peak $\mu$ and more extreme values are further away depending on $\sigma$. 

$$X\sim Normal(\mu,\sigma^2)$$

Fig.~\ref{fig:ch-app-01-normal-distribution-density} shows the probability density function of three normal distributed random variables with different parameters. Fig.~\ref{fig:ch-app-01-normal-distribution-cumulative} shows the corresponding cumulative function of the three normal distributions.

```{r ch-app-01-normal-distribution-density, fig.cap = "Examples of probability density function of normal distributions."}
rv_normal <- tibble(
  x = seq(-15, 15, .01),
  y1 = dnorm(x),
  y2 = dnorm(x, 2, 2),
  y3 = dnorm(x, -2, 3)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(0,1)",
                          parameter == "y2" ~ "(2,2)",
                          parameter == "y3" ~ "(-2,3)")
  )

ggplot(rv_normal, aes(x, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X~Normal", y = "Density")
```

```{r ch-app-01-normal-distribution-cumulative, fig.cap = "Examples of the cumulative distribution function of normal distributions corresponding to the previous probability density functions."}
rv_normal %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_line(size = 2) +
    labs(color = "X~Normal", y = "y")
```

A special case of normal distributed random variables is the *standard normal* distributed variable with $\mu=0$ and $\sigma=1$: $Y\sim Normal(0,1)$. Each normal distribution can be converted into a standard normal distribution by *z-standardization* (see equation below). The advantage of standardization is that values from different scales can be compared. This is because they become *scale independent* by z-transformation. 

**Probability density function**

$$f(x)=\frac{1}{\sigma\sqrt{2\pi}}\exp\left(-0.5\left(\frac{x-\mu}{\sigma}\right)^2\right)$$

**Cumulative distribution function**

$$F(x)=\int_{-\inf}^{x}f(t)dt$$

**Expected value** $E(X)=\mu$

**Variance** $Var(X)=\sigma^2$

**Z-transformation** $Z=\frac{X-\mu}{\sigma}$

**Deviation and *Coverage**
The normal distribution is often associated with the \emph{68-95-99.7 rule}. It is associated with the probability of a random data point landing within \emph{one}, \emph{two} or \emph{three} standard deviations of the mean (Fig.~\ref{fig:ch-app-01-normal-distribution-coverage} depicts this three intervals). For example, about 68% of values drawn from a normal distribution are within one standard deviation $\sigma$ away from the mean $\mu$.

* $P(\mu-\sigma \leq X \leq \mu+\sigma)=0.6827$ 
* $P(\mu-2\sigma \leq X \leq \mu+2\sigma)=0.9545$ 
* $P(\mu-3\sigma \leq X \leq \mu+3\sigma)=0.9973$ 

```{r ch-app-01-normal-distribution-coverage, fig.cap = "Coverage of normal distribution"}
# plot normal distribution with intervals
ggplot(NULL, aes(x = c(-10, 10))) +
  # plot area under the curve
  stat_function(fun = dnorm, args = list(mean = 0, sd = 2),
                geom = "area",
                fill = project_colors[1],
                xlim = c(-6, 6)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 2),
                geom = "area",
                fill = project_colors[2],
                xlim = c(-4, 4)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 2),
                geom = "area",
                fill = project_colors[3],
                xlim = c(-2, 2)) +
  # plot the curve
  stat_function(fun = dnorm, args = list(mean = 0, sd = 2),
                geom = "line",
                xlim = c(-10, 10),
                size = 2) +
  # scale x-axis
  xlim(-10, 10) +
  # label x-axis
  xlab("X") +
  # label ticks of x-axis
  scale_x_continuous(breaks = c(-6,-4,-2,0,2,4,6), 
                     labels = c(expression(-3~sigma),expression(-2~sigma),
                              expression(-sigma),"0",expression(sigma),
                              expression(2~sigma),expression(3~sigma)))
```

**Linear transformations**

1. If $X\sim Normal(\mu, \sigma^2)$ is linear transformed by $Y=a*X+b$, then the new random variable is again normal distributed with $Y \sim Normal(a\mu+b,a^2\sigma^2)$. See Fig.~\ref{} left side.
2. Are $X\sim Normal(\mu_x, \sigma^2)$ and $Y\sim Normal(\mu_y, \sigma^2)$ normal distributed and independent, then their sum is again normal distributed with $X+Y \sim Normal(\mu_x+\mu_y, \sigma_x^2+\sigma_y^2)$. See plot right side.

### Chi-squared distribution
The $\chi^2$-distribution is widely used in hypothesis testing in inferential statistics, because many test statistics are approximately distributed as $\chi^2$-distribution. 

The $\chi^2$-distribution is directly related to the standard normal distribution: The sum of $n$ independent and standard normal distributed random variables $X_1,X_2,...,X_n$ is distributed according to a $\chi^2$ distribution with $n$ \emph{degrees of freedom}:

$$Y=X_1^2+X_2^2+...+X_n^2.$$

The $\chi^2$ distribution is a skew probability distribution with range $[0,+\infty)$, depending on only one parameter: $n$, the *degrees of freedom*. (If $n=1$, then $(0,+\infty)$.) 

$$X\sim \chi^2(n)$$

(Note, there exists also a generalization of the $\chi^2$ distribution: the *non-central* $\chi^2$ distribution, where a second parameter $\lambda$ is introduced.)

Fig.~\ref{fig:ch-app-01-chi-squared-distribution-density} shows the probability density function of three chi-squared distributed random variables with different parameters. Note, that with increasing degrees of freedom the chi-squared distribution approximates the normal distribution. For $n \geq 30$ the chi-squared distribution can be approximated by a normal distribution. Fig.~\ref{fig:ch-app-01-chi-squared-distribution-cumulative} shows the corresponding cumulative function of the three chi-squared density distributions.

```{r ch-app-01-chi-squared-distribution-density, fig.cap = "Examples of probability density function of chi-squared distributions."}
rv_chisq <- tibble(
  x = seq(0, 30, .01),
  y1 = dchisq(x, 2),
  y2 = dchisq(x, 4),
  y3 = dchisq(x, 9)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(2)",
                          parameter == "y2" ~ "(4)",
                          parameter == "y3" ~ "(9)")
  )

# dist plot
ggplot(rv_chisq, aes(x, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X~Chi-Square", y = "Density")
```

```{r ch-app-01-chi-squared-distribution-cumulative, fig.cap = "Examples of the cumulative distribution function of chi-squared distributions corresponding to the previous probability density functions."}
rv_chisq %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_line(size = 2) +
    labs(color = "X~Chi-Square", y = "y")
```

**Probability density function**
$$f(x)=\begin{cases}\frac{x^{\frac{n}{2}-1}e^{-\frac{x}{2}}}{2^{\frac{n}{2}}\Gamma (\frac{n}{2})} &\textrm{, for }x>0,\\ 0 &\textrm{, otherwise.}\end{cases}$$
Where $\Gamma (\frac{n}{2})$ denotes the Gamma function.

**Cumulative distribution function**
$$F(x)=\frac{\gamma (\frac{n}{2},\frac{x}{2})}{\Gamma \frac{n}{2},}$$
with $\gamma(s,t)$ being the lower incomplete gamma function:
$$\gamma(s,t)=\int_0^t t^{s-1}e^{-t} dt.$$

**Expected value** $E(X)=n$

**Variance** $Var(X)=2n$

**Transformations**
Sum of two $\chi^2$-distributed random variables $X \sim \chi^2(m)$ and $Y \sim \chi^2(n)$ is again a $chi^2$-distributed random variable $X+Y=\chi^2(m+n)$.

### F distribution
The F distribution, named after R.A. Fisher, is used in particular in regression and variance analysis. It is defined by the ratio of two $chi^2$-distributed random variables $X\sim \chi^2(m)$ and $Y\sim \chi^2(n)$, each divided by its degree of freedom:

$$F=\frac{\frac{X}{m}}{\frac{Y}{n}}.$$
The F distribution is a continous skew probability distribution with range $[0,+\infty]$, depending on two parameters $m$ and $n$, corresponding to the degrees of freedom of the two $chi^2$-distributed random variables:

$$X \sim F(m,n).$$

Fig.~\ref{fig:ch-app-01-F-distribution-density} shows the probability density function of three F distributed random variables with different parameters. For a small number of degrees of freedom the density distribution is skewed to the left side. When the number increases, the density distribution gets more and more symmetric. Fig.~\ref{fig:ch-app-01-F-distribution-cumulative} shows the corresponding cumulative function of the three F density distributions.
 
```{r ch-app-01-F-distribution-density, fig.cap = "Examples of probability density function of F distributions."}
rv_F <- tibble(
  x = seq(-.5, 7, .01),
  y1 = df(x, 2, 4),
  y2 = df(x, 4, 6),
  y3 = df(x, 12, 12)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(2,4)",
                          parameter == "y2" ~ "(4,6)",
                          parameter == "y3" ~ "(12,12)")
  )

# dist plot
ggplot(rv_F, aes(x, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X~F", y = "Density")
```

```{r ch-app-01-F-distribution-cumulative, fig.cap = "Examples of the cumulative distribution function of F distributions corresponding to the previous probability density functions."}
rv_F %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_line(size = 2) +
    labs(color = "X~F", y = "y")
```

**Probability density function**
$$F(x)=m^{\frac{m}{2}}n^{\frac{n}{2}} \cdot \frac{\Gamma (\frac{m+n}{2})}{\Gamma (\frac{m}{2})\Gamma (\frac{n}{2})} \cdot \frac{x^{\frac{m}{2}-1}}{(mx+n)^{\frac{m+n}{2}}} \textrm{, for } x>0.$$ 
Where $\Gamma(x)$ denotes the gamma function.

**Cumulative distribution function**
$$F(x)=I\left(\frac{m \cdot x}{m \cdot x+n},\frac{m}{2},\frac{n}{2}\right),$$
with $I(z,a,b)$ being the regularized incomplete Beta function:
$$I(z,a,b)=\frac{1}{B(a,b)} \cdot \int_0^z t^{a-1}(1-t)^{b-1} dt.$$

**Expected value** $E(X) = \frac{n}{n-2}$ (for $n \geq 3$)

**Variance** $Var(X) = \frac{2n^2(n+m-2)}{m(n-4)(n-2)^2}$ (for $n \geq 5$)

### Student t-distribution 
The t or student-t distribution was discovered by William S. Gosset in 1908 [@vallverdu2015], who published his work under the pseudonym "student". He worked at the Guinness factory and had to deal with the problem of small sample sizes. This challenge resulted finally in the t distribution. Accordingly, this distribution is used in particular when the sample size is small and the variance unknown, which is often the case in reality. Its shape ressembles the normal bell shape and has a peak at zero, but the t distribution is a bit lower and wider (bigger tails) than the normal distribution. 

The t distribution consists of a standard normal distributed random variable $X\sim Normal(0,1)$ and a $\chi^2$-distributed random variable $Y\sim \chi^2(n)$ (X and Y are independent):

$$T = \frac{X}{\sqrt{\frac{Y}{n}}}.$$
The t distribution has range $(-\infty,+\infty)$, depending on one parameter $n$, the degrees of freedom. The degrees of freedom can be calculated by the sample size $n$ minus one:
$$X \sim t(n).$$

Fig.~\ref{fig:ch-app-01-t-distribution-density} shows the probability density function of three t distributed random variables with different parameters. Notice that for small degrees of freedom $n$, the t-distribution has fatter tails. This is because the t distribution was specially designed to provide more conservative test results when analyzing small samples. When the degrees of freedom increases, the t distribution approaches a normal distribution. For $n \geq 30$ this approximatio is quite good. Fig.~\ref{fig:ch-app-01-t-distribution-cumulative} shows the corresponding cumulative function of the three t density distributions.
 
```{r ch-app-01-t-distribution-density, fig.cap = "Examples of probability density function of t distributions."}
rv_student <- tibble(
  x = seq(-6, 6, .01),
  y1 = dt(x, 1),
  y2 = dt(x, 2),
  y3 = dt(x, 10)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(1)",
                          parameter == "y2" ~ "(2)",
                          parameter == "y3" ~ "(10)")
  )

# dist plot
ggplot(rv_student, aes(x, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X~t", y = "Density")
```

```{r ch-app-01-t-distribution-cumulative, fig.cap = "Examples of the cumulative distribution function of t distributions corresponding to the previous probability density functions."}
rv_student %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_line(size = 2) +
    labs(color = "X~t", y = "y")
```

**Probability density function**
$$ f(x)=\frac{\Gamma(\frac{n+1}{2})}{\sqrt{n\pi} \cdot \Gamma(\frac{n}{2})}\left(1+\frac{x^2}{n}\right)^{-\frac{n+1}{2}},$$
with $\Gamma(x)$ denoting the gamma function.

**Cumulative distribution function**
$$F(x)=I\left(\frac{x+\sqrt{x^2+n}}{2\sqrt{x^2+n}},\frac{n}{2},\frac{n}{2}\right),$$
where $I(z,a,b)$ denotes the regularized incomplete beta function:
$$I(z,a,b)=\frac{1}{B(a,b)} \cdot \int_0^z t^{a-1}(1-t)^{b-1} dt.$$

**Expected value** $E(X) = 0$

**Variance** $Var(X) = \frac{n}{n-2}$ (for $n \geq 3$)

### Beta distribution
The beta distribution creates a continuous distribution of numbers between 0 and 1, therefore this distribution is useful if the uncertain quantity is bounded by 0 and 1 (or 100%), is continuous, and has a single mode. In Bayesian Data Analysis the beta distribution has a special standing as prior distribution for a bernoulli or binomial (see discrete distributions) likelihood. The reason is, that if a beta prior and a bernoulli (or binomial) liklihood are combined the resulting posterior distribution has the same form as the beta distribution. Such priors are referred to as *conjugate priors*.

A beta distribution has two parameters $a$ and $b$:
$$X \sim Beta(a,b).$$
The two parameters can be interpreted as the number of observations made, such that: $n=a+b$. If $a$ and $b$ get bigger, the beta distribution gets narrower. If only $a$ gets bigger the distribution moves rightward and if only $b$ gets bigger the distribution moves leftward. Thus, the parameters define the shape of the distribution, therefore they are also called *shape parameters*. A Beta(1,1) is equivalent to a uniform. Fig.~\ref{fig:ch-app-01-beta-distribution-density} shows the probability density function of four beta distributed random variables with different parameters. Fig.~\ref{fig:ch-app-01-beta-distribution-cumulative} shows the corresponding cumulative function of the four beta density distributions.

```{r ch-app-01-beta-distribution-density, fig.cap = "Examples of probability density function of beta distributions."}
rv_beta <- tibble(
  x = seq(-.1, 1.1, .01),
  y1 = dbeta(x, 1, 1),
  y2 = dbeta(x, 4, 4),
  y3 = dbeta(x, 4, 2),
  y4 = dbeta(x, 2, 4)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(1,1)",
                          parameter == "y2" ~ "(4,4)",
                          parameter == "y3" ~ "(4,2)",
                          parameter == "y4" ~ "(2,4)")
  )

# dist plot
ggplot(rv_beta, aes(x, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X~beta", y = "Density")
```

```{r ch-app-01-beta-distribution-cumulative, fig.cap = "Examples of the cumulative distribution function of beta distributions corresponding to the previous probability density functions."}
rv_beta %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_line(size = 2) +
    labs(color = "X~beta", y = "y")
```

**Probability density function**

$$f(x)=\frac{\theta^{(a-1)} (1-\theta)^{(b-1)}}{B(a,b)},$$
where $B(a,b)$ is the beta *function*:

$$B(a,b)=\int^1_0 \theta^{(a-1)} (1-\theta)^{(b-1)}d\theta.$$

**Cumulative distribution function**

$$F(x)=\frac{B(x;a,b)}{B(a,b)},$$
where $B(x;a,b)$ is the *incomplete beta function*:

$$B(x;a,b)=\int^x_0 t^{(a-1)} (1-t)^{(b-1)} dt,$$

and $B(a,b)$ the (complete) *beta function*

$$B(a,b)=\int^1_0 \theta^{(a-1)} (1-\theta)^{(b-1)}d\theta.$$

**Expected value** 
Mean: $E(X)=\frac{a}{a+b}$
Mode: $\omega=\frac{(a-1)}{a+b-2}$

**Variance** 
Variance: $Var(X)=\frac{ab}{(a+b)^2(a+b+1)}$
Concentration: $\kappa=a+b$ (related to variance such that, the bigger $a$ and $b$ are, the narrower the distribution)

**Reparameterization of the beta distribution**
Sometimes it is helpful (and more intuitive) to write the beta distribution in terms of its mode $\omega$ and concentration $\kappa$ instead of $a$ and $b$:

$$Beta(a,b)=Beta(\omega(\kappa-2)+1, (1-\omega)(\kappa-2)+1), \textrm{ for } \kappa > 2.$$

### Uniform distribution
The (continous) uniform distribution takes values within a specified range $a$ and $b$ that have constant probability.  Sometimes the distribution is also called rectangular distribution, due to its shape of a rectangle. The uniform distribution is in particular common for random number generation. In Bayesian Data Analysis it is often used as prior distribution to express *ignorance*. This can be thought in the following way: When different events are possible but no (reliable) information exists about their probabilty of occurence, the most conservative (and also intuitive) choice would be to assign probability such that all events are equally likely to occur. The uniform distribution model this intuition, it generates a completely random number in some interval [a,b).

The distribution is specified by two parameters: the end points $a$ (minimum) and $b$ (maximum): 
$$X \sim Unif(a,b).$$
When $a=0$ and $b=1$ the distribution is referred to as *standard* uniform distribution. Fig.~\ref{fig:ch-app-01-uniform-distribution-density} shows the probability density function of two uniform distributed random variables with different parameters. Fig.~\ref{fig:ch-app-01-uniform-distribution-cumulative} shows the corresponding cumulative function of the two uniform density distributions.

```{r ch-app-01-uniform-distribution-density, fig.cap = "Examples of probability density function of uniform distributions."}
rv_unif <- tibble(
  x = seq(-.1, 4.1, .01),
  y1 = dunif(x),
  y2 = dunif(x, 2, 4)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = ifelse(parameter == "y1",
                       "(0,1)", "(2,4)")
  )

# dist plot
ggplot(rv_unif, aes(x, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X~uniform", y = "Density")
```

```{r ch-app-01-uniform-distribution-cumulative, fig.cap = "Examples of the cumulative distribution function of uniform distributions corresponding to the previous probability density functions."}
rv_unif %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_line(size = 2) +
    labs(color = "X~uniform", y = "y")
```

**Probability density function**

$$f(x)=\begin{cases} \frac{1}{b-a} &\textrm{ for } x \in [a,b],\\0 &\textrm{ otherwise.}\end{cases}$$

**Cumulative distribution function**

$$F(x)=\begin{cases}0 & \textrm{ for } x<a,\\\frac{x-a}{b-a} &\textrm{ for } a\leq x < b,\\ 1 &\textrm{ for }x \geq b. \end{cases}$$

**Expected value** $E(X)=\frac{a+b}{2}$

**Variance**  $Var(X)=\frac{(b-a)^2}{12}$

## Selected discrete distributions of random variables

### Binomial distribution
The binomial distribution is a useful model for binary decisions where the outcome is a choice between two alternatives (e.g. Yes/No, Left/Right, Present/Absent, Head/Tail, ...). The two outcomes are often referred to as $0$ (failure) and $1$ (success). Consequently, let the probability of occurence of the outcome "success" be $p$, then the probability of occurence of "failure" is $1-p$. 
Considering a coin-flip experiment, if we flip a coin repeatedly, e.g. $30$ times, the successive trials are independent of each other and the probability $p$ is constant, then the resulting binomial distribution is a discrete random variable with outcomes $\{0,1,2,...,30\}$.  
The Binomial distribution has two parameters "size" and "prob", often denoted as $n$ and $p$, respectively. The "size" refers to the number of trials and "prob" to the probability of success:
$$X \sim Binomial(n,p).$$

Fig.~\ref{fig:ch-app-01-binomial-distribution-mass} shows the probability mass function of three binomial distributed random variables with different parameters. As stated above, $p$ refers to the probability of success. The higher this probability the more often we will observe the outcome coded with "1". Therefore the distribution tends toward the right side and vice-versa. The distribution gets more symmetrical if the parameter $p$ approximates 0.5. Fig.~\ref{fig:ch-app-01-binomial-distribution-cumulative} shows the corresponding cumulative function of the three binomial distributions.

```{r ch-app-01-binomial-distribution-mass, fig.cap = "Examples of probability mass function of Binomial distributions."}
# how many trials
trials = 30

rv_binom <- tibble(
  x = seq(0, trials),
  y1 = dbinom(x, trials, .2),
  y2 = dbinom(x, trials, .5),
  y3 = dbinom(x, trials, .8)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(n,0.2)",
                          parameter == "y2" ~ "(n,0.5)",
                          parameter == "y3" ~ "(n,0.8)")
  )

# dist plot; needs some love... :/
ggplot(rv_binom, aes(x, y, fill = parameter)) +
  geom_col(position = "dodge", color = "white") +
  labs(fill = "X~Binomial", y = "Probability")
```


```{r ch-app-01-binomial-distribution-cumulative, fig.cap = "Examples of the cumulative distribution function of Binomial distributions corresponding to the previous probability mass functions."}
rv_binom %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_step(size = 2) +
    labs(color = "X~Binomial", y = "y")
```

**Probability mass function**

$$f(x)=\binom{n}{x}p^x(1-p)^{n-x},$$
where $\binom{n}{x}$ is the binomial coefficient.

**Cumulative function**

$$F(x)=\sum_{k=0}^{x}\binom{n}{k}p^k(1-p)^{n-k}$$

**Expected value** $E(X)=n \cdot p$

**Variance** $Var(X)=n \cdot p \cdot (1-p)$

### Bernoulli distribution
The Bernoulli distribution is a special case of the Binomial distribution with "size" = 1, therefore the outcome of a bernoulli random variable is either 0 or 1. Apart from that the same information holds as with the binomial distribution.
As the "size" parameter is now negligible, the bernoulli distribution has only one parameter $p$:
$$X \sim Bern(p).$$
Fig.~\ref{fig:ch-app-01-bernoulli-distribution-mass} shows the probability mass function of three bernoulli distributed random variables with different parameters. 

```{r ch-app-01-bernoulli-distribution-mass, fig.cap = "Examples of probability mass function of Bernoulli distributions."}

rv_bern <- tibble(
  x = seq(0, 1),
  y1 = dbern(x, .2),
  y2 = dbern(x, .5),
  y3 = dbern(x, .8)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(0.2)",
                          parameter == "y2" ~ "(0.5)",
                          parameter == "y3" ~ "(0.8)")
  )

# dist plot; needs some love... :/
ggplot(rv_bern, aes(x, y, fill = parameter)) +
  geom_col(position = "dodge", color = "white") +
  labs(fill = "X~Bernoulli", y = "Probability")
```

```{r ch-app-01-bernoulli-distribution-cumulative, fig.cap = "Examples of the cumulative distribution function of Bernoulli distributions corresponding to the previous probability mass functions."}
rv_bern %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_point(size = 2) +
    labs(color = "X~Binomial", y = "y") +
  xlim(-0.5,1.5)
```

**Probability mass function**

$$f(x)=\begin{cases} p &\textrm{ if } x=1,\\ 1-p &\textrm{ if } x=0.\end{cases}$$

**Cumulative function**

$$F(x)=\begin{cases} 0 &\textrm{ if } x < 0, \\ 1-p &\textrm{ if } 0 \leq x <1,\\1 &\textrm{ if } x \geq 1.\end{cases}$$

**Expected value** $E(X)=p$

**Variance** $Var(X)=p \cdot (1-p)$

### Beta Binomial distribution

### Poisson distribution

$$X \sim Po(\lambda)$$

```{r ch-app-01-poisson-distribution-mass, fig.cap = "Examples of probability mass function of Poisson distributions."}
# rv_poisson_intro <- tibble(
#   X = rpois(n, lambda = 5),
#   Y = rpois(n, lambda = 15),
#   Z = rpois(n, lambda = 25)
# ) %>% 
#   pivot_longer(cols = c("X", "Y", "Z"), 
#                names_to  = "parameter_set", 
#                values_to = "x")
# 
# ggplot(rv_poisson_intro, aes(x = x, color = parameter_set)) +
#   # draw density lines
#   stat_density(geom = "line", position="identity", size = 2) +
#   # add text annotations to lines
#   annotate("text", x = 10, y = 0.15, label = "X~Poisson(5)", color = project_colors[1], size = 6) +
#   annotate("text", x = 10, y = 0.09, label = "X~Poisson(15)", color = project_colors[2], size = 6) +
#   annotate("text", x = 10, y = 0.05, label = "X~Poisson(25)", color = project_colors[3], size = 6) +
#   # suppress legend
#   theme(legend.position = "none") 


### this isn't quite right...
rv_pois <- tibble(
  x = seq(0, 50, .1),
  y1 = dpois(x, 5),
  y2 = dpois(x, 15),
  y3 = dpois(x, 25)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(5)",
                          parameter == "y2" ~ "(15)",
                          parameter == "y3" ~ "(25)")
  )

# dist plot
ggplot(rv_pois, aes(x, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X~Poisson", y = "Density") 
```


```{r ch-app-01-poisson-distribution-cumulative, fig.cap = "Examples of the cumulative distribution function of Poisson distributions corresponding to the previous probability mass functions."}
# cumdist plot
rv_pois %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_line(size = 2) +
    labs(color = "X~Poisson", y = "y")
```

**Probability mass function**

$$f(x)=\frac{\lambda^x}{x!}e^{-\lambda}$$

**Cumulative function**

$$F(x)=\sum_{k=0}^{x}\frac{\lambda^k}{k!}e^{-\lambda}$$

**Expected value** $E(X)= \lambda$

**Variance** $Var(X)=\lambda$

## Understanding distributions as random variables

```{r}
#initialize parameters
a = 3
b = 1.5
n = 1e6

#create random variables
## normal distributed RVs 
X_norm <- rnorm(n = n, mean = 1, sd = 2)
Y_norm <- rnorm(n = n, mean = 1.5, sd = 2.5)
## standard normal distributed RVs
stdNormal1 <- rnorm(n = n, mean = 0, sd = 1)
stdNormal2 <- rnorm(n = n, mean = 0, sd = 1)
stdNormal3 <- rnorm(n = n, mean = 0, sd = 1)
## chi-square distributed RV
C <- rchisq(n=n, df=3)
## student-t distributed RV
student <- rt(n=20, df=3)
## F distributed RV
fisher <- rf(n=n, df1=1, df2=3)

## create linear transformation of X
X_lin <- a*X_norm+b
## create RV A as addition of X + Y
sum_xy <- X_norm + Y_norm
## create chisquare distributed RV 
C2 <- stdNormal2^2+stdNormal3^2
C3 <- stdNormal1^2+stdNormal2^2+stdNormal3^2
## create student-t distributed RV
T1 <- stdNormal1/sqrt(C2/1)
## create F distributed RV
F1 <- (C2/2)/(C3/1)

## normal ditributed RV B (equal to addition: X+Y)
N_add <- rnorm(n = n, mean = 1 + 1.5, sd = sqrt(2^2 + 2.5^2))
## normal distributed RV V (equal to linear transformation: a*X+b)
N_lin <- rnorm(n = n, mean = a*1+b, sd = (a*2))
```

```{r}
par(mfrow=c(1,2))
plot(density(X_norm), main="Normal RV: a*X+b", xlim=c(-20,20),xlab="RVs")+
  points(density(N_lin), type = "l", col="blue")+
plot(density(X_norm), main="Normal RV: X+Y", xlim=c(-20,20),xlab="RVs")+
  points(density(Y_norm), type = "l", lty="dotted")+
  points(density(N_add), type = "l", col="blue")
par(mfrow=c(1,2))
plot(density(stdNormal1), main=expression("X,Y,Z ~N(0,1);"~X^2+Y^2+Z^2),   
     xlim=c(-10,10),ylim=c(0,0.4),xlab="RVs")+
  points(density(C3), type = "l", col="blue")
plot(density(C), main="Chi-square distribution", xlim=c(0,15),ylim=c(0,0.4),
     xlab="RVs", col="red", lwd=2, lty="dashed")+
  points(density(C3), type = "l", col="blue")
```

```{r}
par(mfrow=c(2,2))
plot(density(stdNormal1), main="Normal devided by Chi-square RVs",   
     xlim=c(-10,10),ylim=c(0,0.4),xlab="RVs")+
  points(density(C), type = "l")+
  points(density(T1),type="l", col="blue")
plot(density(T1), main="Student-t distribution",   
     xlim=c(-10,10),ylim=c(0,0.4),xlab="RVs")+
  points(density(student),type="l", col="red")
plot(density(stdNormal1), main=expression("X~N(0,1),"~Y~"~"~chi^2~(2)~";"~X/sqrt(Y/n)),   
     xlim=c(-10,10),ylim=c(0,0.4),xlab="RVs")+
  points(density(C2), type = "l")+
  points(density(T1), type = "l", col="blue")
plot(density(stdNormal1), main="Student-t distribution",   
     xlim=c(-10,10),ylim=c(0,0.4),xlab="RVs")+
  points(density(student), type = "l", col="red")

```
