# Two approaches to statistical inference

## Overview

In the first section we introduce the \emph{model chapter} based on a short revision of the debate between \emph{Frequentists} and \emph{Bayesians}. After a short overview we formalize the conceptual ideas and the components of the presented model. We finish by discussing further example models.

## Two notions of probability (revisited)

What are probabilities?
Two viewpoints can be distinguished: Probabilities exist "outside in the world" or they are "subjective beliefs" [@kruschke2015]. Although both notions imply different approaches how to deal with probabilities, the mathematical properties are quite similar [@kruschke2015].

### Frequentism --- Probabilities as properties of the world

When thinking about probabilities as existing "outside in the world" one has to consider the random process that produces the observed data. A Frequentist imagine this random process being repeated a large number of times so that the probability of an outcome is the number of observed outcomes divided by the total number of observations $n$ [@dobson2008]. Frequentism is an approach that searches for relative frequencies in a large number of trials [@vallverdu2015].
The parameter $\theta$, the probability of interest, is the value of the relative frequency when $n$ becomes infinitely large. Consequently, the parameter $\theta$ can be estimated from the observed data, $\hat{\theta}$, by maximizing the likelihood function (see section "Likelihood, Prior & Posterior") [@dobson2008].

### Bayesianism --- Probabilities as subjective beliefs

Another notion of probabilities is to think of them as "beliefs" inside one's mind.

\paragraph{Bayes' Theorem}
The core of Bayesian methods is Bayes' theorem which describes how prior belief is combined with observed data:

$$P(H|Data)=\frac{P(Data|H)*P(H)}{P(Data)}$$

\begin{center}
	or in plain language,
\end{center}

$$Posterior=\frac{Likelihood*Prior}{Marginal\textrm{ } Likelihood},$$

The job of the "marginal Likelihood" in the denominator is to standardize the posterior and thus to ensure it sums up to one (integrates to one). Therefore, the key lesson of Bayes' theorem is [@mcelreath2015]:
$$Posterior \propto Likelihood * Prior$$

\paragraph{To summarize up:}
A parameter in *Bayesian methods* is conceptualized as a random variable with its own distribution (the posterior) that summarizes the current state of knowledge. The expected value of the posterior is the best guess about the true value of the parameter and its variability reflects the amount of uncertainty [@kline2013].

In *Frequentist statistics*, a parameter is seen as a constant that should be estimated with sample statistics [@kline2013].