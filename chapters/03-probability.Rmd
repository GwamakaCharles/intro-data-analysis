# Basics of Probability Theory


**to be covered:** axiomatic definition, interpretation, joint distributions,
marginalization, conditional probability & Bayes rule. Random variables: discrete and
continuous, expected values & variance, examples.

**learning goal:** get comfortable with basic notions of probability theory

## Probability

### Outcomes, events, observations

We are interested in the space $\Omega$ of all **elementary outcome** $\omega_1,
\omega_2, \dots$ of a process or event whose execution is (partially) random or
unknown. Elementary outcomes are mutually exclusive. The set $\Omega$ exhausts all
possibilities.^[For simplicity of exposure, we gloss over subtleties arising when
dealing with infinite sets $\Omega$. We make up for this when we define probability
*density* functions for continuous random variables, which is all the uncountable
infinity that we will usually be concerned with in applied statistics.]
  

<div style="background-color:lightgray">
**Example.** The set of elementary outcomes of a single coin flip is $\Omega_{\text{coin flip}} =
  \set{\text{heads}, \text{tails}}$. The elementary outcomes of tossing a six-sided die is
  $\Omega_{\text{standard die}} = \{$ &#9856; , &#9857; , &#9858; , &#9859; ,
    &#9860; , &#9861; $\}$.^[Think of $\Omega$ as a partition of the space of all possible ways in
  which the world could be, where we lump together into one partition cell all ways in which
  the world could be that are equivalent regarding those aspects of reality that we are
  interested in. We do not care whether the coin lands in the mud or in the sand. It only
  matters whether it came up heads or tails. Each elementary event can be realized in myriad
  ways. $\Omega$ is our, the modellers', first crude simplification of nature, abstracting away aspects we
  currently do not care about.]
</div>
  
An **event** $A$ is a subset of $\Omega$. Think of an event as a (possibly partial)
observation. We might observe, for instance, not the full outcome of tossing a die, but only
that there is a dot in the middle. This would correspond to the event
$A = \{$  &#9856; , &#9858; ,  &#9860; $\}$,
i.e., observing an odd numbered outcome. The *trivial observation* $A = \Omega$ and the
*impossible observation* $A = \emptyset$ are counted as events, too. The latter is included for
technical reasons.


For any two events $A, B \subseteq \Omega$, standard set operations correspond to logical
connections in the usual way. For example, the conjunction $A \cap B$ is the observation of
both $A$ and $B$; the disjunction $A \cup B$ is the observation that it is either $A$ or $B$;
the negation of $A$, $\overline{A} = \set{\omega \in \Omega \mid \omega \not \in A}$, is the
observation that it is not $A$.

### Probability distributions

A **probability distribution** $P$ over $\Omega$ is a function
$P \ \colon \ \mathfrak{P}(\Omega) \rightarrow \mathbb{R}$ that assigns to all events
$A \subseteq \Omega$ a real number (from the unit interval, see A1 below), such that the following (so-called Kolmogorov axioms) are satisfied:

A1. $0 \le P(A) \le 1$

A2. $P(\Omega) = 1$

A3. $P(A_1 \cup A_2 \cup A_3 \cup \dots) = P(A_1) + P(A_2) + P(A_3) + \dots $ whenever $A_1, A_2, A_3, \dots$ are mutually exclusive^[A3 is the axiom of *countable additivity*. Finite additivity may be enough for finite or countable sets $\Omega$, but infinite additivity is necessary for full generality in the uncountable case.]

Occasionally we encounter notation $P \in \Delta(\Omega)$ to express that $P$ is a probability
distribution over $\Omega$. (E.g., in physics, theoretical economics or game
  theory. Less so in psychology or statistics.) If $\omega \in \Omega$ is an elementary event,
we often write $P(\omega)$ as a shorthand for $P(\set{\omega})$. In fact, if $\Omega$ is
finite, it suffices to assign probabilities to elementary outcomes.

A number of rules follow immediately from of this definition (prove this!):

C1. $P(\emptyset) = 0$

C2. $P(\overline{A}) = 1 - P(A)$

C3. $P(A \cup B) = P(A) + P(B) - P(A \cap B)$ for any $A, B \subseteq \Omega$

### Interpretations of probability

It is reasonably safe, at least preliminarily, to think of probability, as defined above, as a
handy mathematical primitive which is useful for certain applications. There are at least three
ways of thinking about where this primitive probability might come from, roughly paraphrasable
like so:

1. **Frequentist:** Probabilities are generalizations of intuitions/facts about frequencies of events in
  repeated executions of a random event.
2. **Subjectivist:** Probabilities are subjective beliefs by a rational agent who is
  uncertain about the outcome of a random event.
3. **Realist:** Probabilities are a property of an intrinsically random world.

### Urns and frequencies

Think of an urn as a container which contains a number of $N > 1$ balls. Balls can be of
different color. For example, let us suppose that our urn has $k > 0$ black balls and $N-k$
white balls. (There is at least one black and one white ball.) For a single random draw from
our urn we have: $\Omega_{\text{our urn}} = \set{\text{white}, \text{black}}$. If we imagine an
infinite sequence of single draws from our urn, putting whichever ball we drew back in after
every draw, the limiting proportion with which we draw a black ball is
$\frac{k}{N}$. (If in doubt, execute this experiment. By hand or by computer.) This
statement about frequency is what motivates saying that the probability of drawing a black ball
on a single trial is (or should be^[If probabilities are subjective beliefs, a rational
  agent is, in a sense, normatively required to assign exactly this probability.])
$P(\text{black}) = \frac{k}{N}$.

## Structured events & marginal distributions

### Probability table for a flip-&-draw scenario

Suppose we have two urns. Both have $N=10$ balls. Urn 1 has $k_1=2$ black and $N-k_1 = 8$ white
balls. Urn 2 has $k_2=4$ black and $N-k_2=6$ white balls. We sometimes draw from urn 1,
sometimes from urn 2. To decide, we flip a fair coin. If it comes up heads, we draw from urn 1;
if it comes up tails, we draw from urn 2.

An elementary outcome of this two-step process of flip-&-draw is a pair
$\tuple{\text{outcome-flip}, \text{outcome-draw}}$. The set of all possible such outcomes is
$\Omega_{\text{flip-&-draw}} = \set{\tuple{\text{heads}, \text{black}}, \tuple{\text{heads},
    \text{white}}, \tuple{\text{tails}, \text{black}}, \tuple{\text{tails},
    \text{white}}}$. The probability of event $\tuple{\text{heads}, \text{black}}$ is given by
multiplying the probability of seeing "heads" on the first flip, which happens with
probability $0.5$, and then drawing a black ball, which happens with probability $0.2$, so that
$P(\tuple{\text{heads}, \text{black}}) = 0.5 \mult 0.2 = 0.1$. The probability distribution
over $\Omega_{\text{flip-draw}}$ is consequently as in
Table \@ref(tab:flipdrawprobabilities). (If in doubt, start flipping &
  drawing and count your outcomes.)

```{r flipdrawprobabilities, echo = F}
knitr::kable(
  tibble(
    " " = c("heads", "tails"),
    black = c("$0.5 \\mult 0.2 = 0.1$", "$0.5 \\mult 0.8 = 0.4$"),
    white = c("$0.5 \\mult 0.4 = 0.2$", "$0.5 \\mult 0.6 = 0.3$"),
  ),
  booktabs = T,
  caption = 'my first table',
  escape = F
)
```


### Structured events and joint-probability distributions

Table \@ref(tab:flipdrawprobabilities) is an example of a **joint probability distribution** over a structured event space, which here has two dimensions. Since
our space of outcomes is the Cartesian product of two simpler outcome spaces, namely
$\Omega_{flip-\&-draw} = \Omega_{flip} \times \Omega_{draw}$,^[With
  $\Omega_{\text{flip}} = \set{\text{heads}, \text{tails}}$ and
  $\Omega_{\text{draw}} = \set{\text{black}, \text{white}}$.] we can use notation
$P(\text{heads}, \text{black})$ as shorthand for $P(\tuple{\text{heads}, \text{black}})$. More
generally, if $\Omega = \Omega_1 \times \dots \Omega_n$, we can think of $P \in \Delta(\Omega)$
as a joint probability distribution over $n$ subspaces.

### Marginalization

If $P$ is a joint-probability distribution over event space $\Omega = \Omega_1 \times \dots \Omega_n$, the **marginal distribution** over subspace  $\Omega_i$, $1 \le i \le n$ is the probability distribution that assigns to all $A_i \subseteq \Omega_i$ the probability:^[This notation, using $\sum$, assumes that subspaces are countable. In other cases, a parallel definition with integrals can be used.]

$$  P(A_i) = \sum_{A_1 \subseteq \Omega_{1}, \dots , A_{i-1} \subseteq \Omega_{i-1}, A_{i+1} \subseteq \Omega_{i+1}, \dots, A_n \subseteq \Omega_n} P(A_1, \dots, A_{i-1}, A_{i}, A_{i+1}, \dots A_n) $$

For example, the marginal distribution over coin flips derivable from the joint probability distribution in Table \@ref(tab:flipdrawprobabilities) gives $P(\text{heads}) = P(\text{tails}) = 0.5$, since the sum of each row is exactly $0.5$. The marginal distribution over flips derivable from Table~\ref{tab:flip-and-draw:probabilities} has $P(\text{black}) = 0.3$ and $P(\text{black}) = 0.7$.^[The term ``marginal distribution'' derives from such   probability tables, where traditionally the sum of each row/column was written in the   margins.]

## Conditional probability

Fix probability distribution $P \in \Delta(\Omega)$ and events $A,B \subseteq \Omega$. The conditional probability of $A$ given $B$, written as $P(A \mid B)$, gives the probability of $A$ on the assumption that $B$ is true.^[We also verbalize this as "the conditional probability of $A$ conditioned on $B$."] It is defined like so:

$$P(A \mid B) = \frac{P(A \cap B)}{P(B)}$$

Conditional probabilities are only defined when $P(B) > 0$.^[Updating with events which have probability zero entails far more severe adjustments of the underlying belief system than just ruling out information hitherto considered possible. Formal systems that capture such *belief revision* are studied in formal epistemology @Halpern2003:Reasoning-about .]

<div style="background-color:lightgray">
**Example.** If a dice is unbiased, each of its six faces has equal probability to come up after a toss. The probability of event $B = \{$ &#9856; , &#9858; , &#9860; $\}$ that the tossed number is odd has probability $P(B) = \frac{1}{2}$. The probability of event $A = \{$ &#9858; , &#9859; ,  &#9860; , &#9861; $\}$ that the tossed number is bigger than two is $P(A) = \frac{2}{3}$. The probability that the tossed number is bigger than two \emph{and} odd is $P(A \cap B) = P(\{$ &#9858; ,  &#9860; $\}) = \frac{1}{3}$. The conditional probability of tossing a number that is bigger than two, when we know that the toss is even, is $P(A \mid B) = \frac{1 / 3}{1 / 2} = \frac{2}{3}$.
</div>

Algorithmically, conditional probability first rules out all events in which $B$ is not true
and then simply renormalizes the probabilities assigned to the remaining events in such a way
that the relative probabilities of surviving events remains unchanged. Given this, another way
of interpreting conditional probability is that $P(A \mid B)$ is what a rational agent
\emph{should} believe about $A$ after observing that $B$ is in fact true and nothing more. The
agent rules out, possibly hypothetically, that $B$ is false, but otherwise does not change
opinion about the relative probabilities of anything that is compatible with $B$.





## Continous distributions of random variables

### Normal distribution

$$X\sim Normal(\mu,\sigma^2)$$

```{r}
n <- 1e6

rv.normal.intro <- data.frame(
  X = rnorm(n, mean=-2, sd=3),
  Y = rnorm(n, mean=0, sd=1),
  Z = rnorm(n, mean=2, sd=2)
)

ggplot(data=rv.normal.intro, size=1)+
  stat_density(aes(X), geom = "line", position="identity", color="green")+
  annotate("text", x=-10, y=0.1, label="X~Normal(-2,3)", color="green")+
  stat_density(aes(Y), geom = "line", position="identity", color="red")+
  annotate("text", x=8.5, y=0.3, label="Y~Normal(0,1) = standard normal", color="red")+
  stat_density(aes(Z), geom = "line", position="identity", color="blue")+
  annotate("text", x=8, y=0.15, label="Y~Normal(2,2)", color="blue")+
  theme_classic()
```

Special case of normal distributed random variables is the *standard normal* distributed variable with $\mu=0$ and $\sigma=1$: $Y\sim Normal(0,1)$ 

#### Probability density function

$$f(x)=\frac{1}{\sigma\sqrt{2\pi}}\exp\left(-0.5\left(\frac{x-\mu}{\sigma}\right)^2\right)$$

#### Cumulative distribution function

$$F(x)=\int_{-\inf}^{x}f(t)dt$$

#### Expected value

$$E(X)=\mu$$

#### Variance 

$$Var(X)=\sigma^2$$

#### Z-transformation

$$Z=\frac{X-\mu}{\sigma}$$

#### Deviation and Coverage

* $P(\mu-\sigma \leq X \leq \mu+\sigma)=0.6827$ (see dark blue area under the curve)
* $P(\mu-2\sigma \leq X \leq \mu+2\sigma)=0.6827$ (see medium blue area under the curve)
* $P(\mu-3\sigma \leq X \leq \mu+3\sigma)=0.6827$ (see light blue area under the curve)

```{r}
# plot standard deviations of a normal distribution

ggplot(NULL, aes(x = c(-10, 10))) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 2),
                geom = "line",
                xlim = c(-10, 10)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 2),
                geom = "area",
                fill = "blue",
                alpha = .2,
                xlim = c(-6, 6)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 2),
                geom = "area",
                fill = "blue",
                alpha = .4,
                xlim = c(-4, 4)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 2),
                geom = "area",
                fill = "blue",
                alpha = .6,
                xlim = c(-2, 2)) +
  xlim(-10, 10)
```


#### Linear transformations

1. If $X\sim Normal(\mu, \sigma^2)$ is linear transformed by $Y=a*X+b$, then the new random variable is again normal distributed with $Y \sim Normal(a\mu+b,a^2\sigma^2)$. See plot left side.
2. Are $X\sim Normal(\mu_x, \sigma^2)$ and $Y\sim Normal(\mu_y, \sigma^2)$ normal distributed and independent, then their sum is again normal distributed with $X+Y \sim Normal(\mu_x+\mu_y, \sigma_x^2+\sigma_y^2)$. See plot right side.

```{r}
#initialize parameters
muX = 1
sigmaX = 2
muY = 1.5
sigmaY = 2.5
a = 1.5
b = 1
n = 1e6

#create random variables
## normal distributed RV X
X <- rnorm(n = n, mean = muX, sd = sigmaX)
Y <- rnorm(n = n, mean = muY, sd = sigmaY)
## create linear transformation of X
Z <- a*X+b
## normal distributed RV V (equal to linear transformation: a*X+b)
V <- rnorm(n = n, mean = a*muX+b, sd = (a*sigmaX))
## create RV A as addition of X + Y
A <- X + Y
## normal ditributed RV B (equal to addition: X+Y)
B <- rnorm(n = n, mean = muX + muY, sd = sqrt(sigmaX^2 + sigmaY^2))

#create dataset
rv.normal <- data.frame(Y=Y, Z=Z, V=V, A=A, B=B)

#plot RVs
linear.tranform <- ggplot(data=rv.normal)+
  stat_density(aes(Y), geom = "line", position = "identity", size=1)+
  annotate("text", x=5, y=0.175, label=c(paste("Y~Normal(",muY,",",sigmaY,")")))+
  stat_density(aes(Z), col="green", geom = "line", position = "identity", size=1)+
  annotate("text", x=10, y=0.125, label=c(paste(a,"*Y+",b)), color="green")+
  stat_density(aes(V), col="red", linetype="dashed", geom = "line", position = "identity", size=1)+
  ggtitle("linear transformation")+
  xlim(-20,20)+
  ylim(0,0.21)+
  theme_classic()

addition.transform <- ggplot(data=rv.normal)+
  stat_density(aes(X), col="blue", size=1, geom = "line", 
                             position = "identity")+
  annotate("text", x=-10, y=0.175, label=c(paste("X~Normal(",muX,",",sigmaX,")")), 
           color="blue")+
  stat_density(aes(Y), size=1, geom = "line", 
                             position = "identity")+
  annotate("text", x=-12, y=0.125, label=c(paste("Y~N(",muY,",",sigmaY,")")))+
  stat_density(aes(A), col="green", size=1, geom = "line", 
                             position = "identity")+
  annotate("text", x=-12, y=0.075, label= "X + Y", color="green")+
  stat_density(aes(B), col="red", linetype="dashed", size=1, geom = "line", 
                             position = "identity")+
  ggtitle("transformation by addition")+
  xlim(-20,20)+
  ylim(0,0.21)+
  theme_classic()

grid.arrange(linear.tranform, addition.transform, ncol=2)
```

### Chi-square distribution

$$Y\sim \chi^2(n)$$

Sum of $n$ independent and standard normal distributed random variables $X_1,X_2,...,X_n$.

$$Y=X_1^2+X_2^2+...+X_n^2$$

#### Expected value

$$E(Y)=n$$

#### Variance

$$Var(Y)=2n$$

#### Transformations

Sum of two $\chi^2$-distributed random variables $X \sim \chi^2(m)$ and $Y \sim \chi^2(n)$ is again a $chi^2$-distributed random variable $X+Y=\chi^2(m+n)$.

```{r, warning=FALSE}
#initialize parameters
n = 1e6
df = 3 #number of RVs

#create RV 
## standard normal distributed RV X
X <- rnorm(n = n, mean = 0, sd = 1)
Y <- rnorm(n = n, mean = 0, sd = 1)
Z <- rnorm(n = n, mean = 0, sd = 1)

## create chi-square distributed RV
A <- X^2+Y^2+Z^2

## chi-square distributed RV
B <- rchisq(n=n, df=df)

#create data frame
rv.chisquare <- data.frame(X=X, Y=Y, Z=Z, A=A, B=B)

#plot RVs
rv.trans.chi <- ggplot(data=rv.chisquare)+
  stat_density(aes(X), size=1, geom = "line", 
                       position = "identity")+       
  annotate("text", x=10, y=0.25, label="std. normal")+
  stat_density(aes(A), col="green", size=1, geom = "line", 
                       position = "identity")+
  annotate("text", x=10, y=0.15, label="chi-square", color="green")+
  xlim(-10,30)+
  ylim(0,0.42)+
  ggtitle("Transform. of N(0,1)-RV")+
  theme_classic()

rv.chisquare <- ggplot(data=rv.chisquare)+
  stat_density(aes(A), col="green", size=1, geom = "line", 
                       position = "identity")+
  stat_density(aes(B), col="red",linetype="dashed", size=1, geom = "line", 
                       position = "identity")+
  xlim(0,30)+
  ylim(0,0.42)+
  ggtitle(expression(chi^2~"distribution"))+
  theme_classic()

grid.arrange(rv.trans.chi, rv.chisquare, ncol=2)
```

### Student t-distribution

$$T \sim t(n)$$

Consists of a standard normal distributed random variable $X\sim Normal(0,1)$ and a $chi^2$-distributed random variable $Y\sim \chi^2(n)$ (X and Y are independent):

$$T=\frac{X}{\sqrt{\frac{Y}{n}}}$$

#### Expected value

$$E(T)=0$$

#### Variance

$Var(T)=\frac{n}{n-2}$ (for $n \geq 3$)

```{r, warning=FALSE}
#initialize parameters
n = 1e6
df = 2

# create RV 
## normal distributed RV
X <- rnorm(n = n, mean = 0, sd = 1)
Y <- rnorm(n = n, mean = 0, sd = 1)
Z <- rnorm(n = n, mean = 0, sd = 1)

## create chisquare distributed RV 
C <- X^2+Y^2

## create student-t distributed RV
B <- Z/sqrt(C/df)

## Student-t distributed RV
A <- rt(n=n, df=df)

# create data frame
data.studentt <- data.frame(X=X,C=C, B=B, A=A)

# plot RVs
rv.trans.t <- ggplot(data=data.studentt)+
  stat_density(aes(X), size=1, geom = "line", 
                     position = "identity")+
  annotate("text", x=5, y=0.35, label="std. normal")+
  stat_density(aes(C), col="blue", size=1, geom = "line", 
                     position = "identity")+
  annotate("text", x=5, y=0.4, label="chi-square", color="blue")+
  stat_density(aes(B), col="green", size=1, geom = "line", 
                     position = "identity")+
  annotate("text", x=5, y=0.3, label="stundent-t", color="green")+
  xlim(-10,10)+
  ylim(0,0.5)+
  ggtitle(expression("transform"~chi^2~"and N(0,1)-RV"))+
  theme_classic()

rv.studentt <- ggplot(data=data.studentt)+
  stat_density(aes(B), col="green", size=1, geom = "line", 
                      position = "identity")+
  stat_density(aes(A), col="red", linetype="dashed", size=1, geom = "line", 
                      position = "identity")+
  xlim(-10,10)+
  ylim(0,0.5)+
  ggtitle("student-t distribution")+
  theme_classic()

grid.arrange(rv.trans.t, rv.studentt, ncol=2)
```

### F distribution

$$F \sim F(m,n)$$

Consists of two $chi^2$-distributed random variables $X\sim \chi^2(m)$ and $Y\sim \chi^2(n)$:

$$F=\frac{\frac{X}{m}}{\frac{Y}{n}}$$

#### Expected value

$E(F)=\frac{n}{n-2}$ (for $n \geq 3$)

#### Variance

$Var(F)=\frac{2n^2(n+m-2)}{m(n-4)(n-2)^2}$ (for $n \geq 5$)


```{r, warning=FALSE}
#initialize parameters
n = 1e6
df = 3 #number of RVs

# create RV 
## standard normal distributed RVs
Q <- rnorm(n = n, mean = 0, sd = 1)
V <- rnorm(n = n, mean = 0, sd = 1)
W <- rnorm(n = n, mean = 0, sd = 1)
X <- rnorm(n = n, mean = 0, sd = 1)
Y <- rnorm(n = n, mean = 0, sd = 1)
Z <- rnorm(n = n, mean = 0, sd = 1)

## create new chisquare distributed RVs
D <- (Q^2+V^2+W^2)
C <- (X^2+Y^2+Z^2)

## create addition of chisquare RVs
B <- (D/df)/(C/df)

## F distributed RV
A <- rf(n=n, df1=df, df2=df)

#create data frame
data.F <- data.frame(X=X,Y=Y,Z=Z,W=W,D=D,C=C,B=B,A=A)

#plot RVs
rv.trans.f <- ggplot(data=data.F)+
  stat_density(aes(X), size=1, geom = "line", 
                     position = "identity")+
  annotate("text", x=5, y=0.3, label="std. normal")+
  stat_density(aes(D), col="blue", size=1, geom = "line", 
                     position = "identity")+
  annotate("text", x=6.5, y=0.15, label="chi-square",color="blue")+
  stat_density((aes(B)), col="green", size=1, geom = "line", 
                     position = "identity")+
  annotate("text", x=5, y=0.5, label="F distribution", color="green")+
  xlim(-5,15)+
  ggtitle(expression("transform"~chi^2~"RVs"))+
  theme_classic()
  
rv.f <- ggplot(data=data.F)+
  stat_density((aes(B)), col="green", size=1, geom = "line", position = "identity")+
  stat_density(aes(A), col="red", linetype="dashed", size=1, geom = "line", position = "identity")+
  xlim(0,15)+
  ggtitle("F distribution")+
  theme_classic()
  
grid.arrange(rv.trans.f, rv.f, ncol=2)
```

