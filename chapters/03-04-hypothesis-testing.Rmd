# Hypothesis Testing {#ch-03-05-hypothesis-testing}

Hypothesis testing is the workhorse of much of frequentist statistics.^[A more precise but less elegant heading for this section would be: ``Null-hypothesis significance testing with controlled $\alpha$-errors.'']
Researchers hold a research hypothesis $H_1$, such as discrimination takes longer than go/no-go decisions in the [mental chronometry experiment](app-93-data-sets-mental-chronometry).
The hypothesis that is tested is the logical negation of $H_1$, usually referred to as the null-hypothesis $H_0$, e.g., that there is zero difference in reaction times between discrimination and the go/no-go task.
If empirical observations are sufficiently *un*likely from the point of view of the null-hypothesis $H_0$, this is treated as evidence in favor of the research hypothesis $H_1$. 
A measure, perhaps approximate, of how unlikely the data is in the light of $H_0$ is the $p$-value.
A conventional threshold on $p$-values governs a categorical decision whether to reject or not reject the null-hypothesis. This threshold is essentially an upper-bound on a particular kind of error, namely the error to falsely reject the null-hypothesis when it is in fact true (a so-called type-1 error or $\alpha$-error). 
In the following we will fill these general ideas with life.^[Null-hypothesis significance testing is superficially related to Popperian falsificationism. It is, however, quite the opposite when looked at more carefully. Popper famously denied that empirical observation could constitute positive evidence in favor of a research hypothesis. Research hypotheses can only be refuted, viz., when their logically consequences are logically incompatible with the observed data. In a Popperian science, what is refuted are research hypotheses; frequentist statistics instead seeks to refute null-hypotheses and counts successful refutation of a null-hypothesis as evidence in favor of a research hypothesis.]


- error control Fisher/Neyman-Pearson
- $p$-values (based on binomial example)
- tests to cover 
  - binomial test 
  - Chi-squared test
  - t-test
  - ANOVA
  - linear regression

## $p$-values

As before we fix a parameterized likelihood function $P(D \mid \theta)$, such as the binomial distribution in the [Binomial Model](Chap-03-03-models-examples). As described in Section \@ref(Chap-03-03-models-hypotheses), we may think of a research hypothesis as a statement about the parameter (vector) $\theta$ in a given model. In the most common case, the **null-hypothesis** fixes a single parameter of interest to a point-value.^[It may be objected that it makes no sense to conceive of a point-value null-hypothesis in the first place. The probability that the coin is exactly fair has no non-trivial measure. Indeed, it is also possible to consider interval-valued null-hypotheses in frequentist testing, but this is the exception and not the norm. Most formal results also apply to point-valued null hypotheses, not necessarily interval-based null hypotheses.] 

```{block2, type='infobox'}
**Example.**  A research hypothesis for the 24/7 example and its corresponding null-hypothesis are:

- $H_1$: the coin is biased $\theta \neq 0.5$ 
- $H_0$: the coin is fair $\theta = 0.5$ 
```

The null-hypothesis fixes a likelihood function for any kind of data we might perceive on (hypothetical) repetitions of an experiment. We here write $P(\mathcal{D} = D \mid H_0)$ or, for short, $P(D \mid H_0)$, e.g., $P(\mathcal{D} = D \mid \Theta = 0.5)$ for a the binomial 24/7 example. Let's write $\mathcal{D}^{|H_0}$ for the random variable that captures the probability distribution over possible data observations, on the assumption that the null-hypothesis is true. For example, when fixing $N=24$ the space of logically possible outcomes is $K = \set{0, 1, \dots, 24}$ observations of heads. 
Then $T^{|H_0} = t(D^{|H_0})$ is a derived random variable, the so-called **sampling distribution**, with the function $t \colon d \mapsto t(d) \in \mathbb{R}$ a so-called **test statistic**. The $p$-value associated with actual data observation $D_{\text{obs}}$ is the probability of observing any alternative outcome with a value of the test statistic that is at least as extreme as $t(D_{\text{obs}})$:^[Keep in mind that the null-hypothesis is implicit in the sampling distribution.]

$$
  p(D_{\text{Obs}}) = P(T^{|H_0} \ge t(D_{\text{obs}})) % = P(\mathcal{D}^{|H_0} \in \{D \mid t(D) \ge t(D_{\text{obs}})\}) 
$$

A special case obtains when the model used in the background makes no further assumptions that are not covered by the (point-valued) null-hypothesis (e.g., there are no other free parameters that the null-hypothesis does not cover) and the test statistic is the inverse likelihood function: $t(D) = P(\mathcal{D} = D \mid H_0)^{-1}$. We then speak of an **exact $p$-value** (and an ensuing **exact test**). It follows that the exact $p$-value of observation $D_{\text{obs}}$ gives the probability of observing an event which is, from the point of view of $H_0$, no more likely than $D_{\text{obs}}$ (or: an event at least as unlikely (so: at least as extreme)).^[Using a test statistic that does not yield an exact test can be useful for various reasons. We might want to select a particular aspect of the data that we want to measure the extremes of. Or, we might wish to avoid the complexity of calculating an exact $p$-test and so resort to a clever test statistic that simplifies computation, yet approximates the exact calculation reasonably well (under certain conditions).]

<div class="infobox">
**Example.**  A coin was flipped $N = 24$ times and landed heads $k = 7$ times. To calculate the associated exact $p$-value under the null-hypothesis $\theta = 0.5$, we consult the likelihood function in Figure \@ref(fig:ch-03-04-testing-binomial-p-value), note the likelihood of the observed data (on the $x$-axis at $k = 7$) in comparison to all other conceivable outcomes of the flip-$N=24$-times experiment, and sum the probabilities of all events which are at least as unlikely as the observed $k=7$. This yields the value $p \approx 0.06391$.

```{r ch-03-04-testing-binomial-p-value, echo = F, fig.cap = "Binomial likelihood function and exact $p$-value for the observation of $k=7$ successes in $N = 24$ coin flips, under the assumption of a null-hypothesis $\\theta = 0.5$."}
plotData = data.frame(x = 0:24, y = dbinom(0:24, 24, 0.5))
plotData2 = data.frame(x = c(0:7, 17:24), y = dbinom(c(0:7, 17:24), 24, 0.5))
ggplot(plotData, aes(x = x , y = y )) + geom_bar(stat = "identity", fill = "gray", width = 0.35) +
  geom_bar(data = plotData2, aes(x = x, y = y), stat = "identity", fill = "firebrick", width = 0.35) +
  geom_hline(yintercept=dbinom(7,24,0.5)) + 
  # geom_text(data.frame(x = 3, y = 0.05, label = paste0("p = " , round(1-sum(dbinom(8:16, 24, 0.5)),3), collapse = "")), aes(x = x, y = y, label = label))
  geom_label(x = 3, y = 0.03, label = paste0("p(k=7) = " , round(1-sum(dbinom(8:16, 24, 0.5)),5), collapse = "")) +
  labs(
    x = latex2exp::TeX("$k$"),
    y = latex2exp::TeX("$Binomial(k \\, | \\, n = 24, \\theta = 0.5)$")
  )
  

```

Here is custom-made code to obtain this result:

```{r}
# exact p-value for k=7 with N=24 and null-hypothesis theta = 0.5
tibble( lh = dbinom(0:24, 24, 0.5) ) %>% 
  filter( lh <=  dbinom(7, 24, 0.5) ) %>% 
  pull(lh) %>% sum %>% round(5)
```

There is, of course, also a ready-made R function for a **binomial test**:

```{r}
binom.test(
  x = 7,     # observed successes
  n = 24,    # total nr. observations
  p = 0.5    # null hypothesis
)
```

</div>
 
 

## Some results

### Law of Large Numbers

The Law of Large Numbers justifies why taking (large) samples from a random variable sufficiently approximates a mean (the most prominent Bayesian estimator).

```{theorem "Law-of-Large-Numbers"}
**Law of Large Numbers.** Let $X_1, \dots, X_n$ be a sequence of $n$ differentiable random variables with equal mean, such that $\mathbb{E}_{X_i} = \mu_X$ for all $1 \le i \le n$.^[Though the result is more general, it is convenient to think of a natural application as the case where all $X_i$ are samples from the exact same distribution.] As the number of samples $n$ goes to infinity the mean of any tuple of samples, one from each $X_i$, convergences almost surely to $\mu_X$:
  
$$ P(\lim_{n \rightarrow \infty} \frac{1}{n} \sum_{i = 1}^n X_i = \mu_X) = 1 $$
```


Computer simulation makes the point and usefulness of this fact easier to appreciate:

```{r}
# sample from a standard normal distribution (mean = 0, sd = 1)
samples <- rnorm(100000)
# collect the mean after each 10 samples & plot
tibble(
  n = seq(100, length(samples), by = 10)
  ) %>% 
  group_by(n) %>% 
  mutate(
  mu = mean(samples[1:n])
)  %>% 
  ggplot(aes(x = n, y = mu)) +
  geom_line()
```


### Central Limit Theorem

For practical puporses, think of the Central Limit Theorem as an extension of the Law of Large Numbers. While the latter tells us that, as $n \rightarrow \infty$, the mean of repeated samples from a random variable $X$ converges to the mean of $X$, the Central Limit Theorem tells us something about the distribution of our estimate of $X$'s mean. The Central Limit Theorem tells us that the sampling distribution of the mean approximates a normal distribution for large enough sample size.

```{theorem, "Central-Limit-Theorem"}
**Central Limit Theorem**. Let $X_1, \dots, X_n$ be a sequence of $n$ differentiable random variables with equal mean  $\mathbb{E}_{X_i} = \mu_X$ and equal finite variance $\text{Var}(X_i) = \sigma_X^2$ for all $1 \le i \le n$.^[As with the Law of Large Numbers, the most common application is the case where all $X_i$ are samples from the exact same distribution.] The random variable $S_n$ which captures the distribution of the sample mean for any $n$ is:

$$ S_n = \frac{1}{n} \sum_{i=1}^n X_i $$

  
As the number of samples $n$ goes to infinity the random variable $\sqrt{n} (S_n - \mu_X)$ converges in distribution to a normal distribution with mean 0 and standard deviation $\sigma_X$.
  
```


### Uniform distribution of $p$-values

```{theorem, "Probability-Integral-Transform"}
**Probability Integral Transform**. If $X$ be a continuous random variable with cumulative distribution function $F_X$, the random variable $Y = F_X(X)$ is a uniformly distributed over interval $[0;1]$, i.e., $y \sim \text{Uniform}(0,1)$. 
  
```

```{proof}
Notice that the cumulative density function of a standard uniform distribution $y \sim \text{Uniform}(0,1)$ is linear line with intercept 0 and slope 1. It therefore suffices to show that $F_Y(y) = y$.
$$
\begin{aligned}
F_Y(y) & = P(Y \le y)  && [\text{def. of cumulative distribution}] \\
 & = P(F_X(X) \le y)  && [\text{by construction / assumption}] \\
 & = P(X \le F^{-1}_X(y))  && [\text{applying inverse cumulative function}] \\
 & = F_X(F^{-1}_X(y))  && [\text{def. of cumulative distribution}] \\
 & = y  && [\text{inverses cancel out}] \\
\end{aligned}
$$
```




&nbsp;
