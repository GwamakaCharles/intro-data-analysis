# Hypothesis Testing {#ch-03-05-hypothesis-testing}

<hr>

<div style = "float:right; width:35%;">
<img src="visuals/badge-testing.png" alt="badge testing">  
</div>  

Hypothesis testing is the workhorse of much of frequentist statistics.
Researchers usually have a **research hypothesis**, such as: 

- "truth-value judgements are at chance level for sentences with false presupositions" (a claim about data from the [King of France experiment](app-93-data-sets-king-of-france)); or: 
- "discrimination takes longer than go/no-go decisions" (a claim about data from the [mental chronometry experiment](app-93-data-sets-mental-chronometry)).

However, the hypothesis that is tested is not necessarily the research hypothesis. 
What is tested is rather a so-called **null hypothesis** $H_0$.^[The term "null hypothesis" does not want to imply that the hypothesis is that some value of interest is equal to zero (although in practice that is frequently the case). The term rather implicates that this hypothesis is put out there in order to be possibly refuted, i.e., nullified, by the data [@Gigerenzer2004:Mindless-Statis].]
For technical reasons, the null hypothesis is usually a *point-valued hypothesis* in that it assumes that some model parameter takes a single fixed value (see the earlier chapter "[Expressing hypotheses with models](Chap-03-03-models-hypotheses)").
Classical hypothesis testing then looks at whether the null hypothesis is plausible, given some observed data.

Of course, the null hypothesis is chosen in such a way as to give us information on the research question in the background.
For example, depending on the framework we work in (see below), the null-hypothesis can be the research hypothesis, as in the case of the King of France example above.
Alternatively, the research hypothesis is a statement about the existence of a difference between groups or an effect of an experimental manipulation, and in this case the null-hypothesis is opposed to the research hypothesis. E.g., the null-hypothesis could state that there is *no* difference in reaction times between discrimination and the go/no-go task.

Unfortunately, there is no single uncontroversially accepted and universally practiced recipe for frequentist hypothesis testing.^[There is no universally accepted Bayesian treatment either. This is why it is important to apply rational deliberation in each case, and not to succumb to the temptation of following easy recipes too quickly.]
But there are three main approaches: Fisher's approach, the Neyman-Pearson apprach and the (modern) hybrid approch, often referred to simply as NHST (= null-hypothesis significance testing).
These approaches differ in the way they relate the null hypothesis to the research question.
They also differ in several technical details. 
We will cover differences and commonalities of these three major approaches in Section \@ref(ch-03-05-hypothesis-testing-3-approaches) at the end of this chapter, after we have covered basic technical notions and seen some appliations in terms of some common tests.

More concretely, the chapter is structured as follows. Section \@ref(ch-03-05-hypothesis-p-values)) will elaborate on the notion of a $p$-value. 
We will then pay a visit to a very important mathematical result, the Central Limit Theorem, in Section \@ref(ch-03-05-hypothesis-testing-CLT), which allows us to derive approximations of $p$-values for complex cases.
Section \@ref(ch-03-05-hypothesis-testing-tests) covers a select sample of important tests.
Section \@ref(ch-03-05-hypothesis-testing-3-approaches) discusses the three major approaches to significance testing.
Finally, Section \@ref(ch-03-05-hypothesis-testing-3-model-checking) takes a step back and shows how we can think of hypothesis testing also as something more general, namely a method of **model checking**. 


and after we have seen some applications of these basic ingredients (in Section \@ref(ch-03-05-hypothesis-testing-tests)).


```{block, type='infobox'}
The learning goals for this chapter are:

- become familiar with frequentist hypothesis testing
  - see the differences between different approaches
- understand key statistical notions such as:
  - sampling distribution
  - $p$-value
  - $\alpha$- and $\beta$-error
- understand and be able to exploit the relation between $p$-values and confidence intervals
- understand and become able to apply and interpret basic tests:
  - binomial test, t-tests, ANOVA, linear regression, $\chi^2$-test
```

## *p*-values {#ch-03-05-hypothesis-p-values}

All prominent frequentist approaches to statistical hypothesis testing (see Section \@ref(ch-03-05-hypothesis-testing-3-approaches)) agree that if empirical observations are sufficiently *un*likely from the point of view of the null-hypothesis $H_0$, this should be treated (in some way or other) as evidence *against* the null-hypothesis.^[To preview later material (see Section \@ref(ch-03-05-hypothesis-testing-3-approaches)), the Neyman-Pearson approach goes further and also looks at evidence in favor of the null-hypothesis. It also tries to quantify something like evidence in favor of the research hypothesis. But Fisher's approach and some flavors of the hybrid approach only consider how much the data speaks against the null hypothesis.] 
A measure, perhaps approximate, of how unlikely (some aspect of) the data is in the light of $H_0$ is the $p$-value.
To preview the main definition and intuition (to be worked out in detail hereafter), let's first consider a verbal and then a mathematical formulation.

```{block, type='infobox'}
**Definition $p$-value.** The $p$-value associated with observed data $D_\text{obs}$ gives the probability, derived from the assumption that $H_0$ is true, of observing an outcome for the chosen test statistic that is at least as extreme evidence against $H_0$ as the observed outcome.

Formally, the $p$-value of observed data $D_\text{obs}$ is:
$$
p(D_{\text{obs}}) = P(T^{|H_0} \succeq^{H_{0,a}} t(D_{\text{obs}})) % = P(\mathcal{D}^{|H_0} \in \{D \mid t(D) \ge t(D_{\text{obs}})\}) 
$$
where $t \colon \mathcal{D} \rightarrow \mathbb{R}$ is a **test statistic** which picks out a relevant summary statistic of each potential data observation, $T^{|H_0}$ is the **sampling distribution**, namely the random variable derived from test statistic $t$ and the assumption that $H_0$ is true, and $\succeq^{H_{0,a}}$ is a linear order on the image of $t$ such that $t(D_1) \succeq^{H_{0,a}} t(D_2)$ expresses that test value $t(D_1)$ is at least as extreme evidence *against* $H_0$ as test value $t(D_2)$.^[This formulation in terms of a context-dependent (i.e., $H_0$-dependent ordering) is not usual. However, the interpretation *is* de facto context-dependent in this way, and so it makes sense to highlight this aspect of the use of $p$-values also formally. Notice, however, that we can get rid of the context-dependence by using different test-statistics. But this is also not how it is done in practice. Essentially, this definition aims for maximal generality so as to cover all cases of use. Since the class of use cases is fuzzy, the definition needs this flexibility. Alternative mathematical definitions that appear to be simpler just do not capture all the use cases.]
```


A few aspects of this defintion are particularly important (and subsequent text is dedicated to making these aspects more comprehensible):

1. this is a frequentist approach in the sense that probabilities are entirely based on (hypothetical) repetitions of the assumed data-generating process, which assumes that $H_0$ is true;
2. the test statistic *t* plays a fundamental role and should be chosen such that:
    - it must necessarily select exactly those aspects of the data that matter to our research question,
    - it should optimally make it possible to derive a closed form (approximation) of $T$, and^[This latter aspect has been particularly important historically. Given more readily available computing power, alternative approaches based on Monte Carlo simulation of $p$-values can also be used.]
    - it would be desirable (but not necessary) to formulate $t$ in such a way that the comparison relation $\succeq^{H_{0,a}}$ coincides with a simple comparison of numbers: $t(D_1) \succeq^{H_{0,a}} t(D_2)$ iff $t(D_1) \ge t(D_2)$;
3. there is an assumed data-generating model buried inside notation $T^{|H_0}$; and
4. the notion of "more extreme evidence against $H_0$", captured in comparison relation $\succeq^{H_{0,a}}$ depends on our epistemic purposes, i.e., what research question we are ultimately interested in.^[It is admittedly a bit of a notational overkill to write this comparison relation as a function of $H_0$ and $H_a$ (the alternative hypothesis). Other definitions of the $p$-value do not. But the comparison *is* context dependent, and you deserve to see this clearly. To see it clearly, a certain heaviness of notation is the price to pay.]

The following sections will elaborate on all of these points. It is important to mention that especially the third aspect (that there is an implicit data-generating model "inside of" classical hypothesis tests) is not something that receives a lot of emphasis in traditional statistics textbooks. Bad textbooks do not even mention the assumptions implicit in a given test. Better textbooks mention these assumptions, good ones stress them. Using a model-centric approach, as we do here, tries to go even a bit further. We will not only stress key assumptions behind a test but present all of the assumptions behind classical tests in a graphical model, similar to what we did for Bayesian models. This arguably makes all implicit assumptions maximally transparent in a concise and lucid representation. It will also help see parallels between Bayesian and frequentist approaches, thereby helping to see both as more of the same rather than as something completely different. In order to cash in this model-based approach, the following sections will therefore introduce new graphical tools to communicate the data-generating model implicit in the classical tests we cover.
  
### Binomial Model - frequentist version

We start with the Binomial Model because it is the simplest and perhaps most intuitive case. We work out what a $p$-value is for data for this model and introduce the new graphical language to communicate "frequentist models" in the following. We also introduce the notions of *test statistic* and *sampling distribution* based on a case that should be very intuitive, if not familar.

The [Binomial Model](Chap-03-03-models-examples-binomial) was covered before from a Bayesian point of view, where we represented it using graphical notation like in Figure \@ref(fig:ch-03-04-Binomial-Model-repeated) (repeated from above). Remember that this is a model to draw inferences about a coin's bias $\theta$ based on observations of outcomes of flips of that coin. The Bayesian modeling approach treated the number of observed heads $k$ and the number of flips in total $N$ as given, and the coin's bias parameter $\theta$ as latent.

```{r ch-03-04-Binomial-Model-repeated, echo = F, fig.cap="The Binomial Model (repeated from before) for a Bayesian approach to parameter inference/testing.", out.width = '40%'}
knitr::include_graphics("visuals/binomial-model.png")
```

Actually, this way of writing the Binomial Model is rather a shortcut. It glosses over each individual data observation (whether the $i$-the coin flip was heads or tails) and jumps directly to the most relevant summary statistic of how many of the $N$ flips were heads. This might, of course, be just the relevant level of analysis. If our assumption is true that the outcome of each coin flip is independent of any other flip, and given our goal to learn something about $\theta$, all that really matters is $k$. But to prepare ourselves for subsequent frequentist approaches, and in order to appreciate (later!) how powerful a tool a test statistic can be, we can also rewrite the Bayesian model from Figure \@ref(fig:ch-03-04-Binomial-Model-repeated) as the equivalent extended model in Figure \@ref(fig:ch-03-04-Binomial-Model-extended). In the latter representation, the individual outcomes of each flip are represented as $x_i \in \{0,1\}$. Each individual outcome is sampled from a [Bernoulli distribution](app-91-distributions-bernoulli). Based on the whole vector of $x_i$-s, together with knowledge of $N$, we derive the **test statistic** $k$, which maps each observation (a vector $x$ or zeros and ones) to a single number $k$ (the number of heads in the vector). Notice that the node for $k$ has a solid double edge, indicating that it follows deterministically from its parent nodes. This is why we can think of $k$ as a sample from a random variable constructed from "raw data" observations $x$.

```{r ch-03-04-Binomial-Model-extended, echo = F, fig.cap="The Binomial Model for a Bayesian approach, extended to show 'raw observations' and the 'summary statistic' implicitly used.", out.width = '60%'}
knitr::include_graphics("visuals/binomial-model-extended.png")
```

Compare this latter representation in Figure \@ref(fig:ch-03-04-Binomial-Model-extended) with the frequentist Binomial Model in Figure \@ref(fig:ch-03-04-Binomial-Model-frequentist). The frequentist model treats the number of observations $N$ as observed, just like the Bayesian model. But it also fixes a specific value for the coin's bias $\theta$. This is where the (point-valued) null hypothesis comes in. For purposes of analysis, we fix the value of the relevant unobservable latent parameter to a specific value (because we do not want to assign probabilities to latent parameters, but we still like to talk about probabilities somehow). In our graphical model in Figure \@ref(fig:ch-03-04-Binomial-Model-frequentist) the node for the coin's bias is shaded (=treated as known) but also has a doted second edge to indicate that this is where our null-hypothesis assumption kicks in. We then treat the data vector $x$ and with it the associated test statistic $k$ as unobserved. The data we actually observed will, of course, come in at some point. But the frequentist model leaves the observed data out at first in order to bring in the kinds of probabilities frequentist approaches feel comfortable with: probabilities derived from (hypothetical) repetitions of chance events. So, the frequentist model can now make statements about the likelihood of (raw) data $x$ and values of the derived summary statistic $k$ based on the assumption that the null hypothesis is true. Indeed, for the case at hand, we already know that the **sampling distribution**, i.e., the distribution of values for $k$ given $\theta_0$ is the [Binomial distribution](app-91-distributions-binomial).


```{r ch-03-04-Binomial-Model-frequentist, echo = F, fig.cap="The Binomial Model for a frequentist binomial test.", out.width = '80%'}
knitr::include_graphics("visuals/binomial-model-frequentist.png")
```

Let's take a step back. The frequentist model for the binomial case considers ("raw") data of the form $\langle x_1, \dots, x_N \rangle$ where each $x_i \in \{0,1\}$ indicates whether the $i$-th flip was a success (= heads, =1) or a failure (=tails, =0). We identify the set of all binary vectors of length $N$ as the set of hypothetical data which we could, in principle, observe in fictitious repetition of this data-generating process. $\mathcal{D}^{|H_0}$ is then the random variable that assigns each potential observation $D = \langle x_1, \dots, x_N \rangle$ the probability with which it would occur if $H_0$ (=a specific value of $\theta$) is true. In our case, that is:

$$P(\mathcal{D}^{|H_0} = \langle x_1, \dots, x_N \rangle) = \prod_{i=1}^N \text{Bernoulli}(x_i, \theta_0)$$

The model does not work with this raw data and its implied distribution (represented by random variable $\mathcal{D}^{|H_0}$), it uses a (very natural!) **test statistic** $t \colon \langle x_1, \dots, x_N \rangle \mapsto \sum_{i=1}^N x_i$ instead. The **sampling distribution** for this model is therefore the distribution of values for the derived measure $k$ - a distribution which follows from the distribution of the raw data ($\mathcal{D}^{|H_0}$) and this particular test statistic $t$. In its most general form, we write the sampling distribution as $T^{|H_0} = t(\mathcal{D^{H_0}})$. ^[Most often the random variable capturing the sampling distribution is just written as $T$, but it does make sense to stress also notationally that $T$ depends crucially on $H_0$.] It just so happens (what a relief!) that we know how to express $T^{|H_0}$ in a mathematically very concise fashion. It's just the Binomial distribution, so that $k \sim \text{Binomial}(\theta_0, N)$. (Notice how the sampling distribution is really a function of $\theta_0$ (the null-hypothesis) and also of $N$.)


### *p*-values for the Binomial Model

After seeing a frequentist model and learning about test statistic and sampling distribution, let's explore what a $p$-value is based on the frequentist Binomial Model. Our running example will be the 24/7 case, where $N = 24$ and $k = 7$. Notice that we are glossing over the "raw" data immediately and work with the value of the test statistic of the observed data directly: $t(D_{\text{obs}}) = 7$.

Remember that, by the definition given above, $p(D_{\text{obs}})$ is the probability of observing a value of the test statistic that is at least as extreme evidence against $H_0$ as $t(D_{\text{obs}})$, under the assumption that $H_0$ is true:

$$
  p(D_{\text{obs}}) = P(T^{|H_0} \succeq^{H_{0,a}} t(D_{\text{obs}})) % = P(\mathcal{D}^{|H_0} \in \{D \mid t(D) \ge t(D_{\text{obs}})\}) 
$$

To fill this with life, we need to set a null hypothesis, i.e., a value $\theta_0$ of coin bias $\theta$, that we would like to collect evidence *against*. A fixed $H_0$ will directly fix $T^{|H_0}$ but we will have to put extra thought into how to conceptualize $\succeq^{H_{0,a}}$ for any given $H_0$. To make exactly this clearer is the job of this section. Specifically, we will look at what is standardly called a **two-sided $p$-value** and a **one-sided $p$-value**.

As stated in the introduction to this chapter, since this testing routine is geared to give us evidence against $H_0$, we should choose $H_0$ in such a way as to give us information about the research question that really matter to us. So, let's suppose that our research question is either one of the following:

- Is the coin fair ($\theta = 0.5$)?
- Is the coin biased towards heads ($\theta > 0.5$)?

As we will see below, in both cases the null hypothesis will be the same: we are going to assume that $\theta_0 = 0.5$. But given our research question, the **alternative hypothesis** $H_a$ to the null-hypothesis will be different. In the case of testing for fairness  ($\theta = 0.5$), the pair of null hypothesis and alternative hypothesis are:

$$
\begin{aligned}
H_0 \colon \theta = 0.5 && H_a \colon \theta \neq 0.5
\end{aligned}
$$
Notice that here the alternative hypothesis $H_a$ is two-sided in the sense that it departs from $H_0$ left and right, so to speak.

But in the case of testing whether there is a bias towards heads ($\theta > 0.5$), the null hypothesis is the same but the alternative hypothesis is one-sided:^[An alternative way of looking at this case is to say that, at first, we test the interval-range null hypothesis $\theta > 0.5$, so that research and null hypothesis coincide. To gather evidence against this interval-range null hypothesis, we will compare it to the alternative hypothesis $\theta < 0.5$. Since we need a point-value to easily generated the sampling distribution, the question becomes which point for the null-hypothesis to take. We then choose the value that if we gather evidence *against* this value, this is most disastrous to the null hypothesis in question.]

$$
\begin{aligned}
H_0 \colon \theta = 0.5 && H_a \colon \theta < 0.5
\end{aligned}
$$

**Research question $\theta = 0.5$.** To begin with, assume that we want to address the question of whether the coin is fair, i.e., whether $\theta = 0.5$. In this case, we identify the research question with the null hypothesis and set $\theta_0 = 0.5$. Figure \@ref(fig:ch-03-04-testing-binomial-sampling-distribution) shows the sampling distribution of the test statistic $k$. The probability of the observed value of the sampling statistic is shown in red. 

```{r ch-03-04-testing-binomial-sampling-distribution, echo = F, fig.cap = "Sampling distribution (here: Binomial distribution) and probability associated with observed data $k=7$ highlighted in red, for $N = 24$ coin flips, under the assumption of a null-hypothesis $\\theta = 0.5$."}
plotData = data.frame(x = 0:24, y = dbinom(0:24, 24, 0.5))
plotData2 = data.frame(x = 7, y = dbinom(7, 24, 0.5))
ggplot(plotData, aes(x = x , y = y )) + geom_bar(stat = "identity", fill = "gray", width = 0.35) +
  geom_bar(data = plotData2, aes(x = x, y = y), stat = "identity", fill = "firebrick", width = 0.35) +
  labs(
    x = latex2exp::TeX("test statistic $k$"),
    y = latex2exp::TeX("$Binomial(k \\, | \\, n = 24, \\theta = 0.5)$")
  )
```

The question we need to settle to obtain a $p$-value is which alternative values of $k$ would count as more extreme evidence against the chosen null hypothesis, i.e., how to interpret $\succeq^{H_{0,a}}$ for this case. The obvious approach is to use the probability of any value of the test statistic $k$ directly and say that observing $D_1$ counts as at least as extreme evidence against $H_0$ as observing $D_2$, $t(D_1) \succeq^{H_{0,a}} t(D_2)$, iff the probability of observing the test statistic associated with $D_1$ is at least as unlikely as observing $D_2$: $P(T^{|H_0} = t(D_1)) \le P(T^{|H_0} = t(D_1))$. To calculate the $p$-value in this way, we therefore need to sum up the probabilities of all values $k$ under the Binomial distribution (with parameters $N=24$ and $\theta = \theta_0 = 0.5$) that are no larger than the value of the observed $k = 7$. In mathematical language:^[Here, the bracket notation $[ \mathit{Boolean} ]$ is the [Iverson bracket](https://en.wikipedia.org/wiki/Iverson_bracket), evaluation to 1 if the Boolean expression is true and to 0 otherwise.]

$$
p(k) = \sum_{k' = 0}^{N} [\text{Binomial}(k', N, \theta_0) <= \text{Binomial}(k, N, \theta_0)] \ \text{Binomial}(k', N, \theta_0)
$$

In code, we calculate the this $p$-value as follows:

```{r}
# exact p-value for k=7 with N=24 and null-hypothesis theta = 0.5
k_obs <- 7
N <-  24
theta_0 <-  0.5
tibble( lh = dbinom(0:N, N, theta_0) ) %>% 
  filter( lh <=  dbinom(k_obs, N, theta_0) ) %>% 
  pull(lh) %>% sum %>% round(5)
```

Figure \@ref(fig:ch-03-04-testing-binomial-p-value) shows the values that need to be summed over in red.

```{r ch-03-04-testing-binomial-p-value, echo = F, fig.cap = "Sampling distribution (Binomial likelihood function) and $p$-value for the observation of $k=7$ successes in $N = 24$ coin flips, under the assumption of a null-hypothesis $\\theta = 0.5$."}
plotData = data.frame(x = 0:24, y = dbinom(0:24, 24, 0.5))
plotData2 = data.frame(x = c(0:7, 17:24), y = dbinom(c(0:7, 17:24), 24, 0.5))
ggplot(plotData, aes(x = x , y = y )) + geom_bar(stat = "identity", fill = "gray", width = 0.35) +
  geom_bar(data = plotData2, aes(x = x, y = y), stat = "identity", fill = "firebrick", width = 0.35) +
  geom_hline(yintercept=dbinom(7,24,0.5)) + 
  # geom_text(data.frame(x = 3, y = 0.05, label = paste0("p = " , round(1-sum(dbinom(8:16, 24, 0.5)),3), collapse = "")), aes(x = x, y = y, label = label))
  geom_label(x = 3, y = 0.03, label = paste0("p(k=7) = " , round(1-sum(dbinom(8:16, 24, 0.5)),5), collapse = "")) +
  labs(
    x = latex2exp::TeX("test statistic $k$"),
    y = latex2exp::TeX("$Binomial(k \\, | \\, n = 24, \\theta = 0.5)$")
  )
```


Of course, R also has a built-in function for a Binomial test. We can use it to verify that we get the same result for the $p$-value:

```{r}
binom.test(
  x = 7,     # observed successes
  n = 24,    # total nr. observations
  p = 0.5    # null hypothesis
)
```


**Research question $\theta < 0.5$.** Let's now look at the case where our research hypothesis is that $\theta > 0.5$, i.e., that there is a bias towards heads. Again we need to settle what the null hypothesis should be and how $\succeq^{H_{0,a}}$ should be interpreted. In this case, we could consider an interval-valued null hypothesis like $\theta_0 > 0.5$ (making the research hypothesis the null). But even so, we would still resort to testing a point-valed null hypothesis $\theta_0 = 0.5$ eventually. This is because point-valued null hypotheses are easy to work with.^[The problem for interval-valued null hypothesis is that we would need to specify some more information about how to rank parameters, if only to say that they are all ranked equally (plausible, *a priori*), which is something that we try to avoid in frequentist approaches.] It has therefore become customary to pick the value from the relevant interval which is most favorable of the interval-valued null-hypothesis. In case we find strong evidence *against* even the most favorable value from the interval, than this does consititute the strongest possible case *against* the whole interval-based null hypothesis.

But even though we use the the same null-value of $\theta_0 = 0.5$, the calculation of the $p$-value will be different from the case we looked at previously. The reason lies in a change to what we should consider more extreme evidence against this interval-valued null hypothesis, i.e., the interpretation of $\succeq^{H_{0,a}}$. In the case at hand, observing values of $k$ larger than 12, even if they are unlikely for the point-valued hypothesis $\theta_0 = 0.5$ do not consititute evidence against the interval-valued hypothesis we are interested in. So, therefore, we disregard the contribution of the right hand side in Figure \@ref(fig:ch-03-04-testing-binomial-p-value) to arrive at a picture like in Figure \@ref(fig:ch-03-04-testing-binomial-p-value-one-sided). The associated $p$-value with this, so-called **one-sided test**, is consequently:

```{r}
k_obs <- 7
N <- 24
theta_0 <- 0.5
# exact p-value for k=7 with N=24 and null-hypothesis theta > 0.5
dbinom(0:k_obs, N, theta_0) %>% sum %>% round(5)
```

We can double-check against the built-in function `binom.test` when we ask for a one-sided test:

```{r}
binom.test(
  x = 7,     # observed successes
  n = 24,    # total nr. observations
  p = 0.5,    # null hypothesis
  alternative = "less" # the alternative to compare against is theta < 0.5
)
```


```{r ch-03-04-testing-binomial-p-value-one-sided, echo = F, fig.cap = "Sampling distribution (Binomial likelihood function) and $p$-value for the observation of $k=7$ successes in $N = 24$ coin flips, under the assumption of a null-hypothesis $\\theta > 0.5$."}
plotData = data.frame(x = 0:24, y = dbinom(0:24, 24, 0.5))
plotData2 = data.frame(x = c(0:7), y = dbinom(c(0:7), 24, 0.5))
ggplot(plotData, aes(x = x , y = y )) + geom_bar(stat = "identity", fill = "gray", width = 0.35) +
  geom_bar(data = plotData2, aes(x = x, y = y), stat = "identity", fill = "firebrick", width = 0.35) +
  geom_hline(yintercept=dbinom(7,24,0.5)) + 
  # geom_text(data.frame(x = 3, y = 0.05, label = paste0("p = " , round(1-sum(dbinom(8:16, 24, 0.5)),3), collapse = "")), aes(x = x, y = y, label = label))
  geom_label(x = 3, y = 0.03, label = paste0("p(k=7) = " , round(sum(dbinom(0:7, 24, 0.5)),5), collapse = "")) +
  labs(
    x = latex2exp::TeX("test statistic $k$"),
    y = latex2exp::TeX("$Binomial(k \\, | \\, n = 24, \\theta = 0.5)$")
  )
```

### Statistical significance

Fisher's early writing suggests that he considered $p$-values as quantitative measures of strength of evidence against the null hypothesis. What would need to be done or concluded from such a quantitative measure would need to depend on further careful case-by-base deliberation. In contrast, present practice often uses $p$-values to check whether a test result is noteworthy in a categorical, not quantitative way. Fixing an $\alpha$-level of significance (with common values $\alpha \in \{0.05, 0.01, 0.001\}$), we say that a test result is sigificant if the $p$-value of the observed data is lower than the specified $\alpha$.

Significance of a test result, as a categorical measure, can then be further interpreted as a trigger for decision making. Commonly, a significant test results is interpreted as the signal to reject the null hypothesis, i.e., to speak and act as if it was false. 

### *p*-values and $\alpha$-errors

Some blends of frequentist statistics focus on establishing a tight regime of error control (see Section \@ref(ch-03-05-hypothesis-testing-3-approaches) below): we want to keep a cap on the long-run amount of errors that we make in statistical decision making. The $p$-value is then related to the $\alpha$-error, or Type-I error, when decisions to reject the null hypothesis are made based on whether the $p$-value crosses a particular threshold.

An $\alpha$-error occurs when we falsely reject the null hypothesis, i.e., we reject the null hypothesis (as implausible) when it is actually true.
Suppose we decide to reject the null hypothesis that the coin is fair in a two-sided test exactly when the $p$-value of our test is smaller than $\alpha$, e.g., $\alpha = 0.05$. In this case, $\alpha$ is an upper bound on the $\alpha$-error.

### Relation of *p*-values to confidence intervals

There is a close relation between $p$-values and confidence intervals.^[An important caveat applies here. There can be different (approximate) ways of defining $p$-values and confidence intervals. The relation described here does not hold, when the (approximate) way of computing the $p$-value does not match the (approximate) way of computing the confidence interval.] For a two sided test of null hypothesis $\theta = \theta_0$, with alternatve $H_a \colon \theta \neq \theta_0$, it holds for all possible data observations $D$ that

$$ p(D) < \alpha \ \ \text{iff} \ \ \theta_0 \not \in \text{CI}(D) $$
where $\text{CI}(D)$ is the $(1-\alpha) \cdot 100\%$ confidence interval constructed for data $D$.

This connection is intuitive when we think about long-term error. Decisions to reject the null hypothesis are false in exactly $(\alpha \cdot 100)\%$ of the cases when the null hypothesis is true. The definition of a confidence interval was exactly the same: the true value should like outside a $(1-\alpha) \cdot 100\%$ confidence interval in exactly $(\alpha \cdot 100)\%$ of the cases. (Of course, this is only a vague and intuitively appealing argument based on the overall rate, not any particular case.)

### Distribution of $p$-values

A result that might seem surprising at first is that if the null hypothesis is true, the distribution of $p$-values is uniform. This, however, is intuitive on second thought. Mathematically it is a direct consequence of the **Probability Integral Transform Theorem**.

```{theorem, label = "Probability-Integral-Transform", name = "Probability Integral Transform"}
If $X$ be a continuous random variable with cumulative distribution function $F_X$, the random variable $Y = F_X(X)$ is a uniformly distributed over interval $[0;1]$, i.e., $y \sim \text{Uniform}(0,1)$. 
  
```

```{proof}
Notice that the cumulative density function of a standard uniform distribution $y \sim \text{Uniform}(0,1)$ is linear line with intercept 0 and slope 1. It therefore suffices to show that $F_Y(y) = y$.
$$
\begin{aligned}
F_Y(y) & = P(Y \le y)  && [\text{def. of cumulative distribution}] \\
 & = P(F_X(X) \le y)  && [\text{by construction / assumption}] \\
 & = P(X \le F^{-1}_X(y))  && [\text{applying inverse cumulative function}] \\
 & = F_X(F^{-1}_X(y))  && [\text{def. of cumulative distribution}] \\
 & = y  && [\text{inverses cancel out}] \\
\end{aligned}
$$
```

&nbsp;

Seeing the uniform distribution of $p$-values (under a true null hypothesis) helps appreciate how the $\alpha$-level of significance is related to long-term error control. If the null hypothesis is true, the probability of a significant test result is exactly the significance level. 

### How (not) to interpret *p*-values

Though central to much of frequentist statistics, $p$-values are frequently misinterpreted, even by seasoned scientists [@HallerKrauss2002:Misinterpretati]. To repeat, the $p$-value measures the probability of observing, if the null-hypothesis is correct, a value of the test statistic that is (in a specific, contextually specified sense) more extreme than the value of the test statistic that we assign to the observed data. We can therefore treat $p$-values as a measure of evidence *against* the null-hypothesis. And if we want to be even more precise, we interpret this as evidence against the whole assumed data-generating process, a central part of which is the null-hypothesis.

The $p$-value is *not* a statement about the probability of the null hypothesis given the data. So, it is *not* something like $P(H_0 \mid D)$. The latter is a very appealing notion, but it is one that the frequentist denies herself access to. It can also only be computed based on some consideration of prior plausibility of $H_0$ in relation to some alternative hypothesis. Indeed, to calculate $P(H_0 \mid D)$ is the topic of the chapter on [Model Comparison](#Chap-03-06-model-comparison).

## Central Limit Theorem {#ch-03-05-hypothesis-testing-CLT}

The previous section expanded of the notion of a $p$-value and it showed how to calculate $p$-values for different kinds of research questions for data from repeated Bernoulli trials (= coin flips). We saw that a natural test statistic is the Binomial distribution. The Binomial distribution described the sampling distribution precisely, i.e., the sampling distribution for the frequentist Binomial Model as we set it up *is* the Binomial distribution. Unfortunately, there are models and types of data for which the sampling distribution is not known precisely. In these cases, frequentist statistics works with approximations to the true sampling distribution. These approximations get better the more data was observed, i.e., these are limit-approximations that hold in the limit when the amount of data observed goes towards infinity. For small samples, the error might be substantial. Rules of thumb have become conventional guides for judging when (not) to use a given approximation. Which (approximation for a) sampling distribution to use needs to be decided on a case-by-case basis.

To establish that a particular distribution is a good approximation of the true sampling distribution, the most important formal result is the *Central Limit Theorem* (CLT). In rough terms, the CLT says that, under certain conditions, we can use a normal distribution as an approximation of the sampling distribution. 

To appreciate the CLT, let's start with another seminal results, the **Law of Large Numbers**, which we have already relied on when we discussed a sample-based approach to representing probability distributions. For example, the Law of Large Numbers justifies why taking (large) samples from a random variable sufficiently approximates a mean (the most prominent Bayesian point-estimator of, e.g., a posterior approximated by samples from MCMC algorithms).

```{theorem label = "Law-of-Large-Numbers", name = "Law of Large Numbers"}
Let $X_1, \dots, X_n$ be a sequence of $n$ differentiable random variables with equal mean, such that $\mathbb{E}_{X_i} = \mu_X$ for all $1 \le i \le n$.^[Though the result is more general, it is convenient to think of a natural application as the case where all $X_i$ are samples from the exact same distribution.] As the number of samples $n$ goes to infinity the mean of any tuple of samples, one from each $X_i$, convergences almost surely to $\mu_X$:
  
$$ P \left(\lim_{n \rightarrow \infty} \frac{1}{n} \sum_{i = 1}^n X_i = \mu_X \right) = 1 $$
```


Computer simulation makes the point and usefulness of this fact easier to appreciate:

```{r}
# sample from a standard normal distribution (mean = 0, sd = 1)
samples <- rnorm(100000)
# collect the mean after each 10 samples & plot
tibble(
  n = seq(100, length(samples), by = 10)
  ) %>% 
  group_by(n) %>% 
  mutate(
  mu = mean(samples[1:n])
)  %>% 
  ggplot(aes(x = n, y = mu)) +
  geom_line()
```


For practical purposes, think of the Central Limit Theorem as an extension of the Law of Large Numbers. While the latter tells us that, as $n \rightarrow \infty$, the mean of repeated samples from a random variable $X$ converges to the mean of $X$, the Central Limit Theorem tells us something about the distribution of our estimate of $X$'s mean. The Central Limit Theorem tells us that the sampling distribution of the mean approximates a normal distribution for large enough sample size.

```{theorem, label = "Central-Limit-Theorem", name = "Central Limit Theorem"}
Let $X_1, \dots, X_n$ be a sequence of $n$ differentiable random variables with equal mean  $\mathbb{E}_{X_i} = \mu_X$ and equal finite variance $\text{Var}(X_i) = \sigma_X^2$ for all $1 \le i \le n$.^[As with the Law of Large Numbers, the most common application is the case where all $X_i$ are samples from the exact same distribution.] The random variable $S_n$ which captures the distribution of the sample mean for any $n$ is:

$$ S_n = \frac{1}{n} \sum_{i=1}^n X_i $$

  
As the number of samples $n$ goes to infinity the random variable $\sqrt{n} (S_n - \mu_X)$ converges in distribution to a normal distribution with mean 0 and standard deviation $\sigma_X$.
  
```

A proof of the CLT is not trivial, and we will omit it here. We will only point to the CLT when justifying approximations of sampling distributions, just as exemplified in the next section, which deals with Pearson's $\chi^2$-test.

## Selected tests {#ch-03-05-hypothesis-testing-tests}

### Pearson's $\chi^2$-tests {#ch-03-05-hypothesis-testing-Pearsons-Chi}

There are many tests that use the [$\chi^2$-distribution](app-91-distributions-chi2) as an (approximate) sampling distribution. But given relevance and historical prominence, the name "$\chi^2$-test" is usually interpreted to refer to one of several flavor's of what we could specifically call "Pearson's $\chi^2$-test".

We will look at two flavors here. Pearson's $\chi^2$-test for **goodness of fit** tests whether an observed vector of counts is well explained by a given vector of predicted proportion. Pearson's $\chi^2$-test for **independence** tests whether a (two-dimensional) table of counts could plausibly have been generated by a process of independently selecting the column and the row category. We will explain how both of these work based on an application to the [BLJM data](app-93-data-sets-BLJM), which we load as usual:

```{r, echo = F}
data_BLJM_processed <- read_csv('data_sets/bio-logic-jazz-metal-data-processed.csv')
```

```{r, eval = F}
data_BLJM_processed <- read_csv(url('https://processed.githubusercontent.com/michael-franke/intro-data-analysis/master/data_sets/bio-logic-jazz-metal-data-processed.csv'))
```

The focus is on the counts of music-subject choices:

```{r}
BLJM_associated_counts <- data_BLJM_processed %>% 
  select(submission_id, condition, response) %>% 
  pivot_wider(names_from = condition, values_from = response) %>% 
  # drop the Beach-vs-Mountain condition
  select(-BM) %>% 
  dplyr::count(JM,LB) 
BLJM_associated_counts
```

Remember that the lecturer's bold conjecture was that a preference for Logic over Biology goes together with a preference for Metal over Jazz. Visualization suggests that there might be such a trend but that the (statistical) jury is still out as to whether this conjecture has empirical support.

#### Pearson's $\chi^2$-test for goodness of fit

"Goodness of fit" is a term used in model checking (a.k.a. model criticism, model validation, ...). In such a context, tests for goodness-of-fit investigate whether a model's predictions compatible with the observed data. Pearson's $\chi^2$-test for goodness of fit does exactly this for categorical data. 

Categorical data is data where each data observation falls into one of several unordered categories. If we have $k$ such categories, a **prediction vector** $\vec{p} = \langle p_1, \dots, p_k \rangle$  is a probability vector of length $k$ such that $p_i$ gives the probability with which a single data observation falls into the $i$-th category. The likelihood of a single data observation is given by the [Categorical distribution](), and the likelihood of $N$ data observations is given by the [Multinomial distribution](). These are generalizations of the Bernoulli and Binomial distributions, which cover the case of two unordered categories, to the case of more than two unordered categories.

The BLJM data supplies us with categorical data. Here is the vector of counts of how many participants selected a given music+subject pair:

```{r}
# add category names
BLJM_associated_counts <- BLJM_associated_counts %>% 
  mutate(
    category = str_c(
      BLJM_associated_counts %>% pull(LB),
      "-",
      BLJM_associated_counts %>% pull(JM)
    )
  )
counts_BLJM_choice_pairs_vector <- BLJM_associated_counts %>% pull(n)
names(counts_BLJM_choice_pairs_vector) <- BLJM_associated_counts %>% pull(category)
counts_BLJM_choice_pairs_vector
```

Figure \@ref(fig:ch-03-04-BLJM-count-pairs-plot) shows a crude plot of these counts, together with a baseline prediction of equal proportion in each category.

```{r ch-03-04-BLJM-count-pairs-plot, echo = F, fig.cap="Observed counts of choice pairs of music+subject preference in the BLJM data."}
BLJM_associated_counts %>% 
  ggplot(aes(x = category, y = n)) +
  geom_col(aes(fill = category)) +
  guides(fill = "none") +
  geom_hline(aes(yintercept = sum(counts_BLJM_choice_pairs_vector) / 4), color = "firebrick") +
  geom_label(aes(x = 4, y = 27, label = "baseline expectation"), size = 3, color = "firebrick")
```

Pearson's $\chi^2$-test for goodness of fit allows us to test whether this data could plausibly have been generated by (a model whose predictions are given by) a prediction vector $\vec{p} = \langle p_1, \dots, p_4 \rangle$, where $p_1$ would be the predicted probability of a choice pair "Biology-Jazz" occurring for a single participant etc. Frequently, this test is used to check whether a baseline equal distribution could have generated the data. We do that here, too. We form the null-hypothesis that $\vec{p} = \vec{p}_0$ with $p_{0i} = \frac{1}{4}$ for all categories $i$. 

Figure \@ref(fig:ch-03-04-chi2-model-goodness) shows a graphical representation of the model implicitly assumed in the background for a Pearson's $\chi^2$-test for goodness of fit. The model assumes that the observed vector of counts (like our `counts_BLJM_choice_pairs_vector` from above) is follows a Multinomial distribution.^[Notice that for economy or presentation we now (again) gloss over the "raw" data of individual choices, and present the summarized count data instead. In the previous case of the Binomial Test it made good pedagogical sense to tease apart the "raw" observations from the summarized counts because this helped to show what the test statistic is for a case where the choice of test statistic was very, very obvious; so much so, that we would normally not even bother to make it explicit. Now that we understood what a test statistic is in principle, we can gloss over some steps of data summarizing.] Each vector of (hypothetical) data is associated with a test statistic, called $\chi^2$, which sums over the standardized squared deviation of the observed counts from the predicted baseline in each cell. It can be shown that, if the number of observations $N$ is large enough, the sampling distribution of the $\chi^2$ test statistic is approximated well enough by the [$\chi^2$ distribution](app-91-distributions-chi2) with $k-1$ degrees of freedom (where $k$ is the number of categories).^[A proof of this fact is non-trivial, but an intuition why this might be so is available, if we think of each cell independently first. In each cell, with more and more samples, the distribution of counts will approximate a normal distribution by the CLT. The $\chi^2$-distribution rests (by construction) on a sum of squared samples from a standard normal distribution.]

```{r ch-03-04-chi2-model-goodness, echo = F, fig.cap="Graphical representation of Pearson's $\\chi^2$ test for goodness of fit (testing a vector of predicted proportion).", out.width = '90%'}
knitr::include_graphics("visuals/chi2-model-goodness.png" )
```

We can compute the $\chi^2$-value associated with the observed data $t(D_{obs})$ as follows:

```{r}
# observed counts
n <- counts_BLJM_choice_pairs_vector
# proprortion predicted 
p <- rep(1/4,4)
# expected number in each cell
e <- sum(n)*p
# chi-squared for observed data
chi2_observed <- sum((n-e)^2 *1/e)
chi2_observed
```

We can then compare this value to the sampling distribution, which is a $\chi^2$-distribution with $k-1 = 3$ degrees of freedom. We compute the $p$-value associated with our data as the tail of the sampling distribution, as shown also in Figure \@ref(fig:ch-03-04-chi2-plot):^[Notice that this is a one-sided test due to the nature of the test statistic, which measures squared deviation from the baseline and not deviation in any particular direction (because it is hard to say what a "direction" would be in this case anyway).]

```{r}
p_value_BLJM <- 1 - pchisq(chi2_observed, df = 3)
```


```{r ch-03-04-chi2-plot, echo = F, fig.cap="Sampling distribution for a Pearson's $\\chi^2$ test of goodness of fit ($\\chi^2$-distribution with $k-1 = 3$ degrees of freedom), testing a flat baseline null hypothesis based on the BLJM data. .", out.width = '90%'}
tibble(
  x = seq(0,16, length.out = 1000) ,
  y = dchisq(x, df = 3)
) %>% 
  ggplot(aes(x = x , y = y)) +
  geom_line() +
  labs(
    x = latex2exp::TeX("$\\chi^2$"),
    y = "density"
  ) +
  # line and label for observed value
  geom_segment(
    aes(
      x = chi2_observed,
      xend = chi2_observed,
      y = 0,
      yend = 0.15
    ), color = "firebrick"
  ) +
  geom_label(aes(x = chi2_observed, y = 0.16, label = "observed value\nof test statistic")) +
  # line and label for tail area
  geom_segment(
    aes(
      x = 10,
      xend = 12,
      y = 0.005,
      yend = 0.05
    ), color = "darkgray"
  ) +
  geom_label(aes(x = 12, y = 0.06, label = str_c("tail area =\n", round(p_value_BLJM,5)))) +
  # shading tail area
  geom_area(aes(y = ifelse(
    x >= chi2_observed, y, 0)),
    fill = "firebrick", alpha = 0.5) 
  
```

Of course, these calculations can be performed also by using a built-in R function, namely `chisq.test`:

```{r}
counts_BLJM_choice_pairs_vector <- BLJM_associated_counts %>% pull(n)
chisq.test(counts_BLJM_choice_pairs_vector)
```

The common interpretation of our calculations would be to say that the test yielded a significant result, at least at the significance level of $\alpha = 0.5$. In a research paper we might report this results roughly as follows:

> Observed counts deviated significantly from what is expected if each category (here: pair of music+subject choice) was equally likely ($\chi^2$-test, with $\chi^2 \approx 9.53$, $df = 3$ and $p \approx 0.023$).

Notice that this test is an "omnibus test of difference". We can conclude from a significant test result that the whole vector of observations is unlikely to have been generated from chance, but we cannot conclude from this result (without anything doing else) why, where or how the observations deviated from the assumed prediction vector. Looking at the plot of the data in Figure \@ref(fig:ch-03-04-BLJM-count-pairs-plot) above, it seems intuitive to think that Metal is disproportionally disfavored and that the combination of Biology and Jazz looks particularly outliery, when compared to baseline expectations.

#### Pearson's $\chi^2$-test for independence

The previous test of goodness of fit does not allow us to address the lecturer's conjecture that a preference of Metal over Jazz goes with a preference of Logic over Biology. A slightly different kind of $\chi^2$-test is better suited for this. In Pearson's $\chi^2$-test of independence, we look at a two-dimensional table of correlated data observations, like this one:

```{r}
BLJM_table <- BLJM_associated_counts %>% 
  select(-category) %>% 
  pivot_wider(names_from = LB, values_from = n)
BLJM_table
```

For easier computation and compatibility with the function `chisq.test` we handle the same data but stored as a matrix:

```{r}
counts_BLJM_choice_pairs_matrix <- matrix(
  counts_BLJM_choice_pairs_vector, 
  nrow = 2, 
  byrow = T
)
rownames(counts_BLJM_choice_pairs_matrix) <- c("Jazz", "Metal")
colnames(counts_BLJM_choice_pairs_matrix) <- c("Biology", "Logic")
counts_BLJM_choice_pairs_matrix
```

Pearson's $\chi^2$-test of independence addresses the question of whether two-dimensional tabular count data like the above could plausibly have been generated by a prediction vector $\vec{p}$ which results from the assumption that the realizations of row- and column-choices are [stochastically independent](Chap-03-01-probability-independence). If row- and column-choices are independent, the probability of seeing an outcome result in cell $ij$ is the probability of realizing row $i$ times the probability of realizing column $j$. So, under an independence assumption, we expect a matrix, and a resulting vector of choice proportions like this:

```{r}
# number of observations in total
N <- sum(counts_BLJM_choice_pairs_matrix)
# marginal proportions observed in the data 
# the following is the vector r in the model graph
row_prob <- counts_BLJM_choice_pairs_matrix %>% rowSums() / N
# the following is the vector c in the model graph
col_prob <- counts_BLJM_choice_pairs_matrix %>% colSums() / N
# table of expected observation under independence assumption
# NB: %o% is the outer product of vectors
BLJM_expectation_matrix <- (row_prob %o%  col_prob) * N 
BLJM_expectation_matrix
# the following is the vector p in the model graph
BLJM_expectation_vector <- as.vector(BLJM_expectation_matrix)
BLJM_expectation_vector
```

Figure \@ref(fig:ch-03-04-chi2-model-independence) shows a graphical representation of the $\chi^2$-test of independence. The main difference to the previous test of goodness of fit is that we do no longer just fix any-old predicition vector $\vec{p}$, but consder $\vec{p}$ the deterministic results of independence *and* the best estimates (based on the data at hand) of the row- and column probabilities.

```{r ch-03-04-chi2-model-independence, echo = F, fig.cap="Graphical representation of Pearson's $\\chi^2$ test for independence.", out.width = '90%'}
knitr::include_graphics("visuals/chi2-model-independence.png")
```

We can compute the observed $\chi^2$-test statistic as follows:

```{r}
chi2_observed <- sum((counts_BLJM_choice_pairs_matrix - BLJM_expectation_matrix)^2 / BLJM_expectation_matrix)
p_value_BLJM <- 1-pchisq(q = chi2_observed, df = 1)
round(p_value_BLJM,5)
```

Figure \@ref(fig:ch-03-04-chi2-plot-independence) shows the sampling distribution, the value of the test statistic for the observed data and the $p$-value.

```{r ch-03-04-chi2-plot-independence, echo = F, fig.cap="Sampling distribution for a Pearson's $\\chi^2$ test of independence ($\\chi^2$-distribution with $1$ degree of freedom), testing a flat baseline null hypothesis based on the BLJM data. .", out.width = '90%'}
tibble(
  x = seq(0,4, length.out = 1000) ,
  y = dchisq(x, df = 1)
) %>% 
  ggplot(aes(x = x , y = y)) +
  geom_line() +
  labs(
    x = latex2exp::TeX("$\\chi^2$"),
    y = "density"
  ) +
  # line and label for observed value
  geom_segment(
    aes(
      x = chi2_observed,
      xend = chi2_observed,
      y = 0,
      yend = 3
    ), color = "firebrick"
  ) +
  geom_label(aes(x = chi2_observed, y = 3, label = "observed value\nof test statistic")) +
  # line and label for tail area
  geom_segment(
    aes(
      x = 0.8,
      xend = 1.5,
      y = 0.15,
      yend = 1.2
    ), color = "darkgray"
  ) +
  geom_label(aes(x = 1.5, y = 1.2, label = str_c("tail area =\n", round(p_value_BLJM,5)))) +
  # shading tail area
  geom_area(aes(y = ifelse(
    x >= chi2_observed, y, 0)),
    fill = "firebrick", alpha = 0.5) 
  
```

We can also use the built-in function `chisq.test` in R to obtain this result more efficiently:

```{r}
# we calculate 
chisq.test(
  # supply data as a matrix, not as a vector, for test of independence
  counts_BLJM_choice_pairs_matrix, 
  # do not use a the default correction (because we didn't introduce it)
  correct = FALSE
)
```

With $p$-value of about `r round(p_value_BLJM,5)` we should conclude that there is no indication of strong evidence *against* the assumption of independence. Consequently, there is no evidence *in favor* of the lecturer's conjecture of dependence of musical and academic preferences. In a research paper we might report this result as follows:

> A $\chi^2$-test of independence did not yield a significant test result ($\chi^2$-test, with $\chi^2 \approx 0.44$, $df = 1$ and $p \approx 0.5$). Therefore, we cannot claim to have found any evidence for the research hypothesis of dependence.

### *z*-test {#ch-03-05-hypothesis-testing-z-test}

The Central Limit Theorem tells us that, given enough data, we can treat means of repeated samples from any arbitrary probability distribution as approximately normally distributed. If we notice in addition that if $X$ and $Y$ are random variables following a normal distribution, then so is $Z = X - Y$ (see also the [chapter on the normal distribution](app-91-distributions-normal)), it becomes clear how research questions about means and about differences between means (e.g., in the Mental Chronometry experiment) can be addressed, at least approximately, by using tests that hinge on a sampling distribution which is a normal distribution (usually a standard normal distribution).

The $z$-test is perhaps the simplest of a family of tests that rely on normality of the sampling distribution. Unfortunately, what makes it so simple is also what makes it inapplicable in a wide range of cases. The $z$-test assumes that a quantity is normally distributed with unknown mean but it also assumes that the *variance is known*. Since we do not know the variance in most cases of practical relevance, the $z$-test needs to be replace by a more adequate test, usually a test from the $t$-test family, to be discussed below.

We start with the $z$-test nonetheless because of the added benefit to our understanding. Figure \@ref(fig:ch-03-04-z-test-model) shows the model which lies implicitly underneath a $z$-test for checking whether data $\vec{x}$, which are assumed to be normally distributed with known $\sigma$, could have been generated by a hypothesized mean $\mu = \mu_0$. The sampling distribution of the derived test statistic $z$ is a standard normal distribution. 

```{r ch-03-04-z-test-model, echo = F, fig.cap="Graphical representation of a $z$-test.", out.width = '90%'}
knitr::include_graphics("visuals/z-test-model.png")
```

We know that IQ test results are normally distributed around a mean of 100 with a standard deviation of 10. This holds when the sample is representative of the whole population. But suppose we have reason to believe that the sample is from CogSci students. The standard deviation in a sample from CogSci students might still plausibly be fixed to 10, but we'd like to test whether the assumption that *this* sample was generated by a mean $\mu = 100$, out null hypothesis.

For illustration, suppose we observed this data set of IQ test results:

```{r}
IQ_data <- c(87, 91, 93, 97, 100, 101, 103, 104, 104, 105, 105, 106, 108, 110, 111, 112, 114, 115, 119, 121)
```

The mean of this data set is `r mean(IQ_data)`. Suspcicious!

Following the model in Figure \@ref(fig:ch-03-04-z-test-model) we calculate a the value of the test statistic for the observed data.

```{r}
# number of observations
N <- length(IQ_data)
# null hypothesis to test
mu_0 <- 100
# standard deviation (known/assumed as true)
sd <- 10
z_observed <-  (mean(IQ_data) - mu_0) / (sd / sqrt(N))
z_observed %>% round(4)
```

We would then focus on a one-sided $p$-value (because our "research" hypothesis is that CogSci students have, on average, a higher IQ). Since we observed a mean of `r mean(IQ_data)` in the data, which is higher than the critical value of 100, we test the null hypothesis against an alternative hypothesis which assumes that the data was generated by mean bigger than 100.

We can then compute the $p$-value, as before, by checking the area under the sampling distribution, here a standard normal, in the appropriate way. Figure \@ref(fig:ch-03-04-z-test) shows this result graphically.

```{r}
p_value_IQ_data <- 1 - pnorm(z_observed)
p_value_IQ_data %>% round(6)
```


```{r ch-03-04-z-test, echo = F, fig.cap="Sampling distribution for a $z$-test, testing the null hypothesis based that the IQ-data was generated by $\\mu = 100$ (with assumed/known $\\sigma$).", out.width = '90%'}
tibble(
  x = seq(-4,4, length.out = 1000) ,
  y = dnorm(x)
) %>% 
  ggplot(aes(x = x , y = y)) +
  geom_line() +
  labs(
    x = latex2exp::TeX("$z$"),
    y = "density"
  ) +
  # line and label for observed value
  geom_segment(
    aes(
      x = z_observed,
      xend = z_observed,
      y = 0,
      yend = 0.15
    ), color = "firebrick"
  ) +
  geom_label(aes(x = z_observed, y = 0.16, label = "observed value\nof test statistic")) +
  # line and label for tail area
  geom_segment(
    aes(
      x = 2.7,
      xend = 3,
      y = 0.005,
      yend = 0.05
    ), color = "darkgray"
  ) +
  geom_label(aes(x = 3, y = 0.06, label = str_c("tail area =\n", round(p_value_IQ_data,6)))) +
  # shading tail area
  geom_area(aes(y = ifelse(
    x >= z_observed, y, 0)),
    fill = "firebrick", alpha = 0.5) 
  
```

We can also use a ready-made function for the $z$-test. However, as the $z$-test is so uncommon, it is not built into core R. We need to rely on the `BSDA` package for find the function `z.test`.

```{r}
BSDA::z.test(x = IQ_data, mu = 100, sigma.x = 10, alternative = "greater")
```

The conclusion to be drawn from this test could be formualted in a research report as follows:

> We tested the null-hypothesis of a mean equal to 100, assuming a known standard deviation of 10, in a one-sided $z$-test against the alternative hypothesis that the data was generated by a mean greater than 100 (our research hypothesis). The significant test result ($N = `r length(IQ_data)`$, $z \approx `r z_observed %>% round(4)`$, $p \approx `r p_value_IQ_data %>% round(5)`$) suggests that the data provides strong evidence against the assumption that the mean is not bigger than 100.

### *t*-tests {#ch-03-05-hypothesis-testing-t-test}

In most practical applications where a $z$-test might be useful the standard deviation is not known, and should also not lightly be fixed by clever guess-work. This is where the family of $t$-tests come in. We look at two examples of these, the one-sample $t$-test, which compares on set of samples to a fixed mean, and a two-sample $t$-test, which compares the means of two sets of samples.

#### One-sample $t$-test

The simplest example of this family, a $t$-test for one metric vector $\vec{x}$ of normally distributed observations, tests the null hypothesis, just like the $z$-test, that $\vec{x}$ was generated by some $\mu = \mu_0$. Unlike the $z$-test, this one-sample $t$-test does not, however, assume that the standard deviation is known. It rather uses the observed data to obtain an estimate for the parameter. More concretely, a one-sample $t$-test for $\vec{x}$ estimates the standard deviation in the usual way (see Chapter \@ref(Chap-02-03-summary-statistics)):

$$\hat{\sigma}_x = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (x_i - \mu_{\vec{x}})^2}$$

Figure \@ref(fig:ch-03-04-t-test-model-one-population) shows a graphical representation of a one-sample $t$-test model. The light shading of the node for the standard deviation is meant to indicate that this parameter is estimated from the observed data. Importantly, the distribution of the test statistic $t$ is no longer well approximated by a normal distribution when the sample size is low. It is better captured by a [Student's $t$ distribution](app-91-distributions-students-t).

```{r ch-03-04-t-test-model-one-population, echo = F, fig.cap="Graphical representation of the model underlying a frequentist one-sample $t$-test. Notice that the lightly shaded node for the standard deviation represents that the value for this parameter is estimated from the data.", out.width = '90%'}
knitr::include_graphics("visuals/t-test-model-one-population.png")
```

Let's revisit our IQ-data set from above to calculate a $t$-test, so assuming now that the standard deviation is actually unknown. We can calculate a the value of the test statistic for the observed data and use this to compute a $p$-value, much like in the case of the $z$-test before.

```{r}
N <- length(IQ_data)
# fix the null hypothesis
mean_0 <- 100
# unlike in a z-test we use the sample to estimate SD
sigma_hat <- sd(IQ_data) 
t_observed <- (mean(IQ_data) - mean_0) / sigma_hat * sqrt(N)
t_observed %>% round(4)
```

We calculate the relevant one-sided $p$-value using the cumulative distribution function `pt` of the $t$-distribution.

```{r}
p_value_t_test_IQ <- 1 - pt(t_observed, df = N-1)
p_value_t_test_IQ %>% round(6)
```


```{r ch-03-04-t-test-one-sample, echo = F, fig.cap="Sampling distribution for a $t$-test, testing the null hypothesis based that the IQ-data was generated by $\\mu = 100$ (with unknown $\\sigma$).", out.width = '90%'}
tibble(
  x = seq(-4,4, length.out = 1000) ,
  y = dt(x, df = N-1)
) %>% 
  ggplot(aes(x = x , y = y)) +
  geom_line() +
  labs(
    x = latex2exp::TeX("$t$"),
    y = "density"
  ) +
  # line and label for observed value
  geom_segment(
    aes(
      x = t_observed,
      xend = t_observed,
      y = 0,
      yend = 0.15
    ), color = "firebrick"
  ) +
  geom_label(aes(x = t_observed, y = 0.16, label = "observed value\nof test statistic")) +
  # line and label for tail area
  geom_segment(
    aes(
      x = 2.7,
      xend = 3.3,
      y = 0.005,
      yend = 0.05
    ), color = "darkgray"
  ) +
  geom_label(aes(x = 3.3, y = 0.06, label = str_c("tail area =\n", round(p_value_t_test_IQ,6)))) +
  # shading tail area
  geom_area(aes(y = ifelse(
    x >= t_observed, y, 0)),
    fill = "firebrick", alpha = 0.5) 
  
```

Compare these calculations against the built-in function `t.test`:

```{r}
t.test(x = IQ_data, mu = 100, alternative = "greater")
```

These results could be stated in a research report much like those of the previous $z$-test.

#### Two-sample $t$-test (for unpaired data with equal variance and unequal sample sizes)

```{r ch-03-04-t-test-model-two-populations, echo = F, fig.cap="Graphical representation of the model underlying a frequentist two-population $t$-test (for unpaired data with equal variance and unequal sample sizes). Notice that the light shadeding of the node for the standard deviation and of the node for the mean represents that the values for these parameters are estimated from the data.", out.width = '90%'}
knitr::include_graphics("visuals/t-test-model-two-populations.png")
```

### ANOVA {#ch-03-05-hypothesis-testing-ANOVA}

## Three approaches {#ch-03-05-hypothesis-testing-3-approaches}

### Fisher

Frequentist hypothesis testing is superficially similar to Popperian falsificationism. It is, however, quite the opposite when looked at more carefully. Popper famously denied that empirical observation could constitute positive evidence in favor of a research hypothesis. Research hypotheses can only be refuted, viz., when their logically consequences are logically incompatible with the observed data. In a Popperian science, what is refuted are research hypotheses; frequentist statistics instead seeks to refute null-hypotheses and counts successful refutation of a null-hypothesis as evidence in favor of a research hypothesis.

### Neyman-Pearson

A conventional threshold on $p$-values may govern a categorical decision whether to reject or not reject the null-hypothesis (in the Neyman-Pearson approach). This threshold is essentially an upper-bound on a particular kind of error, namely the error to falsely reject the null-hypothesis when it is in fact true (a so-called type-1 error or $\alpha$-error). 

### Hypbrid modern NHST

## Relation to model checking Section {#ch-03-05-hypothesis-testing-3-model-checking}

## Snippets / Formal results



### ANOVA Sum of Squares decomposition

We have $k$ groups of metric observations. For group $1 \le j \le k$, there are $n_j$ observations. Let $x_{ij}$ be observation $1 \le i \le n_j$ for group $1 \le j \le k$. Let $\bar{x}_j = \frac{1}{n} \sum_{i = 1}^{n_j} x_{ij}$ be the mean of group $j$ and let $\bar{\bar{x}} = \frac{1}{k} \sum_{j=1}^k \frac{1}{n_j} \sum_{i=1}^{n_j} x_{ij}$ be the grand mean of all data points. We would like to show that the total sum of squares can be decomposed into two summands: the within-group sum of squares and the between-group sum of squares:

$$ 
\underbrace{
  \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{\bar{x}})^2
}_{\text{Total SS}}  = 
\underbrace{
  \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2
}_{\text{Within-Group SS}} +
\underbrace{
  \sum_{j=1}^k n_j (\bar{x}_j - \bar{\bar{x}})^2
}_{\text{Between-Group SS}}
$$ 

To show this, we first establish a lemma, which will also be useful later:

```{lemma, label = "SS-cancellation", name = "Sum of squares cancellation"}
Let $\vec{x}$ be a vector of $n$ real-valued numbers, and let $\bar{x} = \frac{1}{n} \sum_{i=i}^n x_i$ be its mean. The sum of squares around the mean is zero:
$$
  \sum_{i=1}^n (x_i - \bar{x}) = 0
$$
  
```

```{proof}
$$
\begin{aligned}
  \sum_{i=i}^n (x_i - \bar{x}) 
  & = 
  \sum_{i=i}^n (x_i - \frac{1}{n} \sum_{j=1}^n x_j) 
  && 
  [\text{by def. of mean}] 
  \\
  & = 
  \sum_{i=i}^n x_i - \frac{n}{n} \sum_{j=1}^n x_j) 
  && 
  [\text{second summand independent of } i] 
  \\ 
  & = 0
  \\ 
\end{aligned}
$$
```


&nbsp;


```{proposition, label = "ANOVA-SS-decomposition", name = "Sum of squares decomposition (ANOVA)"}
If $x_{ij}$ is observation $1 \le i \le n_j$ for group $1 \le j \le k$, $\bar{x}_j = \frac{1}{n} \sum_{i = 1}^{n_j} x_{ij}$ the mean of group $j$ and $\bar{\bar{x}} = \frac{1}{k} \sum_{j=1}^k \frac{1}{n_j} \sum_{i=1}^{n_j} x_{ij}$ be the grand mean of all data points, then:
$$ 
\underbrace{
  \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{\bar{x}})^2
}_{\text{Total SS}}  = 
\underbrace{
  \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2
}_{\text{Within-Group SS}} +
\underbrace{
  \sum_{j=1}^k n_j (\bar{x}_j - \bar{\bar{x}})^2
}_{\text{Between-Group SS}}
$$ 
  
```


```{proof}
$$
\begin{aligned}
  & 
  \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{\bar{x}})^2 
  \\
  = &  
  \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j + \bar{x}_j - \bar{\bar{x}})^2
  && 
  [- \bar{x}_j + \bar{x}_j = 0] 
  \\
  = &  
  \sum_{j=1}^k \sum_{i=1}^{n_j} \left [ (x_{ij} - \bar{x}_j)^2 + 2(x_{ij} - \bar{x}_j)(\bar{x}_j - \bar{\bar{x}}) + (\bar{x}_j - \bar{\bar{x}})^2 \right ]
  && 
  [\text{binomial theorem}] 
  \\
  = &  
  \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2 + 
  \sum_{j=1}^k \sum_{i=1}^{n_j} (\bar{x}_j - \bar{\bar{x}})^2 + 
  \sum_{j=1}^k \sum_{i=1}^{n_j} 2(x_{ij} - \bar{x}_j)(\bar{x}_j - \bar{\bar{x}})
  && 
  [\text{rearranging} ] 
  \\
  = &  
  \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2 + 
  \sum_{j=1}^k n_j (\bar{x}_j - \bar{\bar{x}})^2 + 
  2 \sum_{j=1}^k (\bar{x}_j - \bar{\bar{x}}) \underbrace{\sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)}_{\text{=0 by lemma}}
  && 
  [\text{independences} ] 
  \\
    = &  
  \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2 + 
  \sum_{j=1}^k n_j (\bar{x}_j - \bar{\bar{x}})^2
  && 
  [\text{by lemma} ] 
\end{aligned}
$$
```

&nbsp;

