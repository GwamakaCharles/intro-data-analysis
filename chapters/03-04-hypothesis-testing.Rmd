# Hypothesis Testing {#ch-03-05-hypothesis-testing}

<hr>

<div style = "float:right; width:35%;">
<img src="visuals/badge-testing.png" alt="badge testing">  
</div>  

Hypothesis testing is the workhorse of much of frequentist statistics.
Researchers usually have a **research hypothesis**, such as: 

- "truth-value judgements are at chance level for sentences with false presupositions" (a claim about data from the [King of France experiment](app-93-data-sets-king-of-france)); or: 
- "discrimination takes longer than go/no-go decisions" (a claim about data from the [mental chronometry experiment](app-93-data-sets-mental-chronometry)).

However, the hypothesis that is tested is not necessarily the research hypothesis. 
What is tested is rather a so-called **null hypothesis** $H_0$.^[The term "null hypothesis" does not want to imply that the hypothesis is that some value of interest is equal to zero (although in practice that is frequently the case). The term rather implicates that this hypothesis is put out there in order to be possibly refuted, i.e., nullified, by the data [@Gigerenzer2004:Mindless-Statis].]
For technical purposes, the null hypothesis is usually a *point-valued hypothesis* in that it assumes that some model parameter takes a single fixed value (see the earlier chapter "[Expressing hypotheses with models](Chap-03-03-models-hypotheses)").
Classical hypothesis testing then looks at whether the null hypothesis is plausible, given some observed data.

Of course, the null hypothesis is chosen in such a way as to give us information on the research question in the background.
For example, depending on the framework we work in (see below), the null-hypothesis can be the research hypothesis, as in the case of the King of France example above.
Alternatively, the research hypothesis is a statement about the existence of a difference between groups or an effect of an experimental manipulation, and in this case the null-hypothesis is opposed to the research hypothesis. E.g., the null-hypothesis could state that there is *no* difference in reaction times between discrimination and the go/no-go task.

Unfortunately, there is no single uncontroversially accepted and universally practiced recipe for frequentist hypothesis testing.^[There is no universally accepted Bayesian treatment either. This is why it is important to apply rational deliberation in each case, and not to succumb to the temptation of following easy recipes too quickly.]
But there are three main approaches: Fisher's approach, the Neyman-Pearson apprach and the (modern) hybrid approch, often referred to simply as NHST (= null-hypothesis significance testing).
These approaches differ in the way they relate the null hypothesis to the research question.
They also differ in several technical details. 
We will cover differences and commonalities of these three major approaches in Section \@ref(ch-03-05-hypothesis-testing-3-approaches) at the end of this chapter, after we have covered the basic technical notions (in Section \@ref(ch-03-05-hypothesis-p-values)) and after we have seen some applications of these basic ingredients (in Section \@ref(ch-03-05-hypothesis-testing-tests)).


```{block, type='infobox'}
The learning goals for this chapter are:

- become familiar with frequentist hypothesis testing
  - see the differences between different approaches
- understand key statistical notions such as:
  - sampling distribution
  - $p$-value
  - $\alpha$- and $\beta$-error
- understand and become able to apply and interpret basic tests:
  - binomial test, t-tests, ANOVA, linear regression, $\chi^2$-test
```

## *p*-values {#ch-03-05-hypothesis-p-values}

All prominent frequentist approaches to statistical hypothesis testing (see Section \@ref(ch-03-05-hypothesis-testing-3-approaches)) agree that if empirical observations are sufficiently *un*likely from the point of view of the null-hypothesis $H_0$, this should be treated (in some way or other) as evidence *against* the null-hypothesis.^[To preview later material (see Section \@ref(ch-03-05-hypothesis-testing-3-approaches)), the Neyman-Pearson approach goes further and also looks at evidence in favor of the null-hypothesis. It also tries to quantify something like evidence in favor of the research hypothesis. But Fisher's approach and some flavors of the hybrid approach only consider how much the data speaks against the null hypothesis.] 
A measure, perhaps approximate, of how unlikely (some aspect of) the data is in the light of $H_0$ is the $p$-value.
To preview the main definition and intuition (to be worked out in detail hereafter), let's first consider a verbal and then a mathematical formulation.

```{block, type='infobox'}
**Definition $p$-value.** The $p$-value associated with observed data $D_\text{obs}$ gives the probability, derived from the assumption that $H_0$ is true, of observing an outcome for the chosen test statistic that is at least as extreme evidence against $H_0$ as the observed outcome.

Formally, the $p$-value of observed data $D_\text{obs}$ is:
$$
p(D_{\text{obs}}) = P(T^{|H_0} \succeq^{H_0} t(D_{\text{obs}})) % = P(\mathcal{D}^{|H_0} \in \{D \mid t(D) \ge t(D_{\text{obs}})\}) 
$$
where $t \colon \mathcal{D} \rightarrow \mathbb{R}$ is a **test statistic** which picks out a relevant summary statistic of each potential data observation, $T^{|H_0}$ is the **sampling distribution**, namely the random variable derived from test statistic $t$ and the assumption that $H_0$ is true, and $\succeq^{H_0}$ is a linear order on the image of $t$ such that $t(D_1) \succeq^{H_0} t(D_2)$ expresses that test value $t(D_1)$ is at least as extreme evidence *against* $H_0$ as test value $t(D_2)$.^[This formulation in terms of a context-dependent (i.e., $H_0$-dependent ordering) is not usual. However, the interpretation *is* de facto context-dependent in this way, and so it makes sense to highlight this aspect of the use of $p$-values also formally. Notice, however, that we can get rid of the context-dependence by using different test-statistics. But this is also not how it is done in practice. Essentially, this definition aims for maximal generality so as to cover all cases of use. Since the class of use cases is fuzzy, the definition needs this flexibility. Alternative mathematical definitions that appear to be simpler just do not capture all the use cases.]
```


A few aspects of this defintion are particularly important (and subsequent text is dedicated to making these aspects more comprehensible):

1. this is a frequentist approach in the sense that probabilities are entirely based on (hypothetical) repetitions of the assumed data-generating process, which assumes that $H_0$ is true;
2. the test statistic *t* plays a fundamental role and should be chosen such that:
    - it must necessarily select exactly those aspects of the data that matter to our research question,
    - it should optimally make it possible to derive a closed form (approximation) of $T$, and^[This latter aspect has been particularly important historically. Given more readily available computing power, alternative approaches based on Monte Carlo simulation of $p$-values can also be used.]
    - it would be desirable (but not necessary) to formulate $t$ in such a way that the comparison relation $\succeq^{H_0}$ coincides with a simple comparison of numbers: $t(D_1) \succeq^{H_0} t(D_2)$ iff $t(D_1) \ge t(D_2)$.
3. there is an assumed data-generating model buried inside notation $T^{|H_0}$.

The following sections will elaborate on all of these points. It is important to mention that especially the third aspect (that there is an implicit data-generating model "inside of" classical hypothesis tests) is not something that receives a lot of emphasis in traditional statistics textbooks. Bad textbooks do not even mention the assumptions implicit in a given test. Better textbooks mention these assumptions, good ones stress them. Using a model-centric approach, as we do here, tries to go even a bit further. We will not only stress key assumptions behind a test but present all of the assumptions behind classical tests in a graphical model, similar to what we did for Bayesian models. This arguably makes all implicit assumptions maximally transparent in a concise and lucid representation. It will also help see parallels between Bayesian and frequentist approaches, thereby helping to see both as more of the same rather than as something completely different. In order to cash in this model-based approach, the following sections will therefore introduce new graphical tools to communicate the data-generating model implicit in the classical tests we cover.
  
### Binomial Model - frequentist version

We start with the Binomial Model because it is the simplest and perhaps most intuitive case. We work out what a $p$-value is for data for this model and introduce the new graphical language to communicate "frequentist models" in the following. We also introduce the notions of *test statistic* and *sampling distribution* based on a case that should be very intuitive, if not familar.

The [Binomial Model](Chap-03-03-models-examples-binomial) was covered before from a Bayesian point of view, where we represented it using graphical notation like in Figure \@ref(fig:ch-03-04-Binomial-Model-repeated) (repeated from above). Remember that this is a model to draw inferences about a coin's bias $\theta$ based on observations of outcomes of flips of that coin. The Bayesian modeling approach treated the number of observed heads $k$ and the number of flips in total $N$ as given, and the coin's bias parameter $\theta$ as latent.

```{r ch-03-04-Binomial-Model-repeated, echo = F, fig.cap="The Binomial Model (repeated from before) for a Bayesian approach to parameter inference/testing.", out.width = '40%'}
knitr::include_graphics("visuals/binomial-model.png")
```

Actually, this way of writing the Binomial Model is rather a shortcut. It glosses over each individual data observation (whether the $i$-the coin flip was heads or tails) and jumps directly to the most relevant summary statistic of how many of the $N$ flips were heads. This might, of course, be just the relevant level of analysis. If our assumption is true that the outcome of each coin flip is independent of any other flip, and given our goal to learn something about $\theta$, all that really matters is $k$. But to prepare ourselves for subsequent frequentist approaches, and in order to appreciate (later!) how powerful a tool a test statistic can be, we can also rewrite the Bayesian model from Figure \@ref(fig:ch-03-04-Binomial-Model-repeated) as the equivalent extended model in Figure \@ref(fig:ch-03-04-Binomial-Model-extended). In the latter representation, the individual outcomes of each flip are represented as $x_i \in \{0,1\}$. Each individual outcome is sampled from a [Bernoulli distribution](app-91-distributions-bernoulli). Based on the whole vector of $x_i$-s, together with knowledge of $N$, we derive the **test statistic** $k$, which maps each observation (a vector $x$ or zeros and ones) to a single number $k$ (the number of heads in the vector). Notice that the node for $k$ has a solid double edge, indicating that it follows deterministically from its parent nodes. This is why we can think of $k$ as a sample from a random variable constructed from "raw data" observations $x$.

```{r ch-03-04-Binomial-Model-extended, echo = F, fig.cap="The Binomial Model for a Bayesian approach, extended to show 'raw observations' and the 'summary statistic' implicitly used.", out.width = '60%'}
knitr::include_graphics("visuals/binomial-model-extended.png")
```

Compare this latter representation in Figure \@ref(fig:ch-03-04-Binomial-Model-extended) with the frequentist Binomial Model in Figure \@ref(fig:ch-03-04-Binomial-Model-frequentist). The frequentist model treats the number of observations $N$ as observed, just like the Bayesian model. But it also fixes a specific value for the coin's bias $\theta$. This is where the (point-valued) null hypothesis comes in. For purposes of analysis, we fix the value of the relevant unobservable latent parameter to a specific value (because we do not want to assign probabilities to latent parameters, but we still like to talk about probabilities somehow). In our graphical model in Figure \@ref(fig:ch-03-04-Binomial-Model-frequentist) the node for the coin's bias is shaded (=treated as known) but also has a doted second edge to indicate that this is where our null-hypothesis assumption kicks in. We then treat the data vector $x$ and with it the associated test statistic $k$ as unobserved. The data we actually observed will, of course, come in at some point. But the frequentist model leaves the observed data out at first in order to bring in the kinds of probabilities frequentist approaches feel comfortable with: probabilities derived from (hypothetical) repetitions of chance events. So, the frequentist model can now make statements about the likelihood of (raw) data $x$ and values of the derived summary statistic $k$ based on the assumption that the null hypothesis is true. Indeed, for the case at hand, we already know that the **sampling distribution**, i.e., the distribution of values for $k$ given $\theta_0$ is the [Binomial distribution](app-91-distributions-binomial).


```{r ch-03-04-Binomial-Model-frequentist, echo = F, fig.cap="The Binomial Model for a frequentist binomial test.", out.width = '80%'}
knitr::include_graphics("visuals/binomial-model-frequentist.png")
```

Let's take a step back. The frequentist model for the binomial case considers ("raw") data of the form $\langle x_1, \dots, x_N \rangle$ where each $x_i \in \{0,1\}$ indicates whether the $i$-th flip was a success (= heads, =1) or a failure (=tails, =0). We identify the set of all binary vectors of length $N$ as the set of hypothetical data which we could, in principle, observe in fictitious repetition of this data-generating process. $\mathcal{D}^{|H_0}$ is then the random variable that assigns each potential observation $D = \langle x_1, \dots, x_N \rangle$ the probability with which it would occur if $H_0$ (=a specific value of $\theta$) is true. In our case, that is:

$$P(\mathcal{D}^{|H_0} = \langle x_1, \dots, x_N \rangle) = \prod_{i=1}^N \text{Bernoulli}(x_i, \theta_0)$$

The model does not work with this raw data and its implied distribution (represented by random variable $\mathcal{D}^{|H_0}$), it uses a (very natural!) **test statistic** $t \colon \langle x_1, \dots, x_N \rangle \mapsto \sum_{i=1}^N x_i$ instead. The **sampling distribution** for this model is therefore the distribution of values for the derived measure $k$ - a distribution which follows from the distribution of the raw data ($\mathcal{D}^{|H_0}$) and this particular test statistic $t$. In its most general form, we write the sampling distribution as $T^{|H_0} = t(\mathcal{D^{H_0}})$. ^[Most often the random variable capturing the sampling distribution is just written as $T$, but it does make sense to stress also notationally that $T$ depends crucially on $H_0$.] It just so happens (what a relief!) that we know how to express $T^{|H_0}$ in a mathematically very concise fashion. It's just the Binomial distribution, so that $k \sim \text{Binomial}(\theta_0, N)$. (Notice how the sampling distribution is really a function of $\theta_0$ (the null-hypothesis) and also of $N$.)


### *p*-values for the Binomial Model

After seeing a frequentist model and learning about test statistic and sampling distribution, let's explore what a $p$-value is based on the frequentist Binomial Model. Our running example will be the 24/7 case, where $N = 24$ and $k = 7$. Notice that we are glossing over the "raw" data immediately and work with the value of the test statistic of the observed data directly: $t(D_{\text{obs}}) = 7$.

Remember that, by the definition given above, $p(D_{\text{obs}})$ is the probability of observing a value of the test statistic that is at least as extreme evidence against $H_0$ as $t(D_{\text{obs}})$, under the assumption that $H_0$ is true:

$$
  p(D_{\text{obs}}) = P(T^{|H_0} \succeq^{H_0} t(D_{\text{obs}})) % = P(\mathcal{D}^{|H_0} \in \{D \mid t(D) \ge t(D_{\text{obs}})\}) 
$$

To fill this with life, we need to set a null hypothesis, i.e., a value $\theta_0$ of coin bias $\theta$, that we would like to collect evidence *against*. A fixed $H_0$ will directly fix $T^{|H_0}$ but we will have to put extra thought into how to conceptualize $\succeq^{H_0}$ for any given $H_0$. To make exactly this clearer is the job of this section. Specifically, we will look at what is standardly called a **two-sided $p$-value** and a **one-sided $p$-value**.

As stated in the introduction to this chapter, since this testing routine is geared to give us evidence against $H_0$, we should choose $H_0$ in such a way as to give us information about the research question that really matter to us. So, let's suppose that our research question is either one of the following:

- Is the coin fair ($\theta = 0.5$)?
- Is the coin biased towards heads ($\theta > 0.5$)?

**Research question $\theta = 0.5$.** To begin with, assume that we want to address the question of whether the coin is fair, i.e., whether $\theta = 0.5$. In this case, we identify the research question with the null hypothesis and set $\theta_0 = 0.5$. Figure \@ref(fig:ch-03-04-testing-binomial-sampling-distribution) shows the sampling distribution of the test statistic $k$. The probability of the observed value of the sampling statistic is shown in red. 

```{r ch-03-04-testing-binomial-sampling-distribution, echo = F, fig.cap = "Sampling distribution (here: Binomial distribution) and probability associated with observed data $k=7$ highlighted in red, for $N = 24$ coin flips, under the assumption of a null-hypothesis $\\theta = 0.5$."}
plotData = data.frame(x = 0:24, y = dbinom(0:24, 24, 0.5))
plotData2 = data.frame(x = 7, y = dbinom(7, 24, 0.5))
ggplot(plotData, aes(x = x , y = y )) + geom_bar(stat = "identity", fill = "gray", width = 0.35) +
  geom_bar(data = plotData2, aes(x = x, y = y), stat = "identity", fill = "firebrick", width = 0.35) +
  labs(
    x = latex2exp::TeX("test statistic $k$"),
    y = latex2exp::TeX("$Binomial(k \\, | \\, n = 24, \\theta = 0.5)$")
  )
```

The question we need to settle to obtain a $p$-value is which alternative values of $k$ would count as more extreme evidence against the chosen null hypothesis, i.e., how to interpret $\succeq^{H_0}$ for this case. The obvious approach is to use the probability of any value of the test statistic $k$ directly and say that observing $D_1$ counts as at least as extreme evidence against $H_0$ as observing $D_2$, $t(D_1) \succeq^{H_0} t(D_2)$, iff the probability of observing the test statistic associated with $D_1$ is at least as unlikely as observing $D_2$: $P(T^{|H_0} = t(D_1)) \le P(T^{|H_0} = t(D_1))$. To calculate the $p$-value in this way, we therefore need to sum up the probabilities of all values $k$ under the Binomial distribution (with parameters $N=24$ and $\theta = \theta_0 = 0.5$) that are no larger than the value of the observed $k = 7$. In mathematical language:^[Here, the bracket notation $[ \mathit{Boolean} ]$ is the [Iverson bracket](https://en.wikipedia.org/wiki/Iverson_bracket), evaluation to 1 if the Boolean expression is true and to 0 otherwise.]

$$
p(k) = \sum_{k' = 0}^{N} [\text{Binomial}(k', N, \theta_0) <= \text{Binomial}(k, N, \theta_0)] \ \text{Binomial}(k', N, \theta_0)
$$

In code, we calculate the this $p$-value as follows:

```{r}
# exact p-value for k=7 with N=24 and null-hypothesis theta = 0.5
k_obs <- 7
N <-  24
theta_0 <-  0.5
tibble( lh = dbinom(0:N, N, theta_0) ) %>% 
  filter( lh <=  dbinom(k_obs, N, theta_0) ) %>% 
  pull(lh) %>% sum %>% round(5)
```

Figure \@ref(fig:ch-03-04-testing-binomial-p-value) shows the values that need to be summed over in red.

```{r ch-03-04-testing-binomial-p-value, echo = F, fig.cap = "Sampling distribution (Binomial likelihood function) and $p$-value for the observation of $k=7$ successes in $N = 24$ coin flips, under the assumption of a null-hypothesis $\\theta = 0.5$."}
plotData = data.frame(x = 0:24, y = dbinom(0:24, 24, 0.5))
plotData2 = data.frame(x = c(0:7, 17:24), y = dbinom(c(0:7, 17:24), 24, 0.5))
ggplot(plotData, aes(x = x , y = y )) + geom_bar(stat = "identity", fill = "gray", width = 0.35) +
  geom_bar(data = plotData2, aes(x = x, y = y), stat = "identity", fill = "firebrick", width = 0.35) +
  geom_hline(yintercept=dbinom(7,24,0.5)) + 
  # geom_text(data.frame(x = 3, y = 0.05, label = paste0("p = " , round(1-sum(dbinom(8:16, 24, 0.5)),3), collapse = "")), aes(x = x, y = y, label = label))
  geom_label(x = 3, y = 0.03, label = paste0("p(k=7) = " , round(1-sum(dbinom(8:16, 24, 0.5)),5), collapse = "")) +
  labs(
    x = latex2exp::TeX("test statistic $k$"),
    y = latex2exp::TeX("$Binomial(k \\, | \\, n = 24, \\theta = 0.5)$")
  )
```


Of course, R also has a built-in function for a Binomial test. We can use it to verify that we get the same result for the $p$-value:

```{r}
binom.test(
  x = 7,     # observed successes
  n = 24,    # total nr. observations
  p = 0.5    # null hypothesis
)
```


**Research question $\theta < 0.5$.** Let's now look at the case where our research hypothesis is that $\theta > 0.5$, i.e., that there is a bias towards heads. Again we need to settle what the null hypothesis should be and how $\succeq^{H_0}$ should be interpreted. In this case, we could consider an interval-valued null hypothesis like $\theta_0 > 0.5$ (making the research hypothesis the null). But even so, we would still resort to testing a point-valed null hypothesis $\theta_0 = 0.5$ eventually. This is because point-valued null hypotheses are easy to work with.^[The problem for interval-valued null hypothesis is that we would need to specify some more information about how to rank parameters, if only to say that they are all ranked equally (plausible, *a priori*), which is something that we try to avoid in frequentist approaches.] It has therefore become customary to pick the value from the relevant interval which is most favorable of the interval-valued null-hypothesis. In case we find strong evidence *against* even the most favorable value from the interval, than this does consititute the strongest possible case *against* the whole interval-based null hypothesis.

But even though we use the the same null-value of $\theta_0 = 0.5$, the calculation of the $p$-value will be different from the case we looked at previously. The reason lies in a change to what we should consider more extreme evidence against this interval-valued null hypothesis, i.e., the interpretation of $\succeq^{H_0}$. In the case at hand, observing values of $k$ larger than 12, even if they are unlikely for the point-valued hypothesis $\theta_0 = 0.5$ do not consititute evidence against the interval-valued hypothesis we are interested in. So, therefore, we disregard the contribution of the right hand side in Figure \@ref(fig:h-03-04-testing-binomial-p-value) to arrive at a picture like in Figure \@ref(fig:h-03-04-testing-binomial-p-value-one-sided). The associated $p$-value with this, so-called **one-sided test**, is consquently:

```{r}
k_obs <- 7
N <- 24
theta_0 <- 0.5
# exact p-value for k=7 with N=24 and null-hypothesis theta > 0.5
dbinom(0:k_obs, N, theta_0) %>% sum %>% round(5)
```

We can double-check against the built-in function `binom.test` when we ask for a one-sided test:

```{r}
binom.test(
  x = 7,     # observed successes
  n = 24,    # total nr. observations
  p = 0.5,    # null hypothesis
  alternative = "less" # the alternative to compare against is theta < 0.5
)
```


```{r ch-03-04-testing-binomial-p-value-one-sided, echo = F, fig.cap = "Sampling distribution (Binomial likelihood function) and $p$-value for the observation of $k=7$ successes in $N = 24$ coin flips, under the assumption of a null-hypothesis $\\theta > 0.5$."}
plotData = data.frame(x = 0:24, y = dbinom(0:24, 24, 0.5))
plotData2 = data.frame(x = c(0:7), y = dbinom(c(0:7), 24, 0.5))
ggplot(plotData, aes(x = x , y = y )) + geom_bar(stat = "identity", fill = "gray", width = 0.35) +
  geom_bar(data = plotData2, aes(x = x, y = y), stat = "identity", fill = "firebrick", width = 0.35) +
  geom_hline(yintercept=dbinom(7,24,0.5)) + 
  # geom_text(data.frame(x = 3, y = 0.05, label = paste0("p = " , round(1-sum(dbinom(8:16, 24, 0.5)),3), collapse = "")), aes(x = x, y = y, label = label))
  geom_label(x = 3, y = 0.03, label = paste0("p(k=7) = " , round(sum(dbinom(0:7, 24, 0.5)),5), collapse = "")) +
  labs(
    x = latex2exp::TeX("test statistic $k$"),
    y = latex2exp::TeX("$Binomial(k \\, | \\, n = 24, \\theta = 0.5)$")
  )
```


## Central limit theorem {#ch-03-05-hypothesis-CLT}

The previous section expanded of the notion of a $p$-value and it showed how to calculate $p$-values for different kinds of research questions for data from repeated Bernoulli trials (= coin flips). We saw that a natural test statistic is the Binomial distribution. The Binomial distribution described the sampling distribution precisely, i.e., the sampling distribution for the frequentist Binomial Model as we set it up *is* the Binomial distribution. Unfortunately, there are models and types of data for which the sampling distribution is not known precisely. In these cases, frequentist statistics works with approximations to the true sampling distribution. These approximations get better the more data was observed, i.e., these are limit-approximations that hold in the limit when the amount of data observed goes towards infinity. For small samples, the error might be substantial. Rules of thumb have become conventional guides for judging when (not) to use a given approximation. Which (approximation for a) sampling distribution to use needs to be decided on a case-by-case basis.

To establish that a particular distribution is a good approximation of the true sampling distribution, the most important formal result is the *Central Limit Theorem* (CLT). In rough terms, the CLT says that 

### Law of Large Numbers

The Law of Large Numbers justifies why taking (large) samples from a random variable sufficiently approximates a mean (the most prominent Bayesian estimator).

```{theorem label = "Law-of-Large-Numbers", name = "Law of Large Numbers"}
Let $X_1, \dots, X_n$ be a sequence of $n$ differentiable random variables with equal mean, such that $\mathbb{E}_{X_i} = \mu_X$ for all $1 \le i \le n$.^[Though the result is more general, it is convenient to think of a natural application as the case where all $X_i$ are samples from the exact same distribution.] As the number of samples $n$ goes to infinity the mean of any tuple of samples, one from each $X_i$, convergences almost surely to $\mu_X$:
  
$$ P \left(\lim_{n \rightarrow \infty} \frac{1}{n} \sum_{i = 1}^n X_i = \mu_X \right) = 1 $$
```


Computer simulation makes the point and usefulness of this fact easier to appreciate:

```{r}
# sample from a standard normal distribution (mean = 0, sd = 1)
samples <- rnorm(100000)
# collect the mean after each 10 samples & plot
tibble(
  n = seq(100, length(samples), by = 10)
  ) %>% 
  group_by(n) %>% 
  mutate(
  mu = mean(samples[1:n])
)  %>% 
  ggplot(aes(x = n, y = mu)) +
  geom_line()
```


### Central Limit Theorem

For practical puporses, think of the Central Limit Theorem as an extension of the Law of Large Numbers. While the latter tells us that, as $n \rightarrow \infty$, the mean of repeated samples from a random variable $X$ converges to the mean of $X$, the Central Limit Theorem tells us something about the distribution of our estimate of $X$'s mean. The Central Limit Theorem tells us that the sampling distribution of the mean approximates a normal distribution for large enough sample size.

```{theorem, label = "Central-Limit-Theorem", name = "Central Limit Theorem"}
Let $X_1, \dots, X_n$ be a sequence of $n$ differentiable random variables with equal mean  $\mathbb{E}_{X_i} = \mu_X$ and equal finite variance $\text{Var}(X_i) = \sigma_X^2$ for all $1 \le i \le n$.^[As with the Law of Large Numbers, the most common application is the case where all $X_i$ are samples from the exact same distribution.] The random variable $S_n$ which captures the distribution of the sample mean for any $n$ is:

$$ S_n = \frac{1}{n} \sum_{i=1}^n X_i $$

  
As the number of samples $n$ goes to infinity the random variable $\sqrt{n} (S_n - \mu_X)$ converges in distribution to a normal distribution with mean 0 and standard deviation $\sigma_X$.
  
```


### Uniform distribution of $p$-values

```{theorem, label = "Probability-Integral-Transform", name = "Probability Integral Transform"}
If $X$ be a continuous random variable with cumulative distribution function $F_X$, the random variable $Y = F_X(X)$ is a uniformly distributed over interval $[0;1]$, i.e., $y \sim \text{Uniform}(0,1)$. 
  
```

```{proof}
Notice that the cumulative density function of a standard uniform distribution $y \sim \text{Uniform}(0,1)$ is linear line with intercept 0 and slope 1. It therefore suffices to show that $F_Y(y) = y$.
$$
\begin{aligned}
F_Y(y) & = P(Y \le y)  && [\text{def. of cumulative distribution}] \\
 & = P(F_X(X) \le y)  && [\text{by construction / assumption}] \\
 & = P(X \le F^{-1}_X(y))  && [\text{applying inverse cumulative function}] \\
 & = F_X(F^{-1}_X(y))  && [\text{def. of cumulative distribution}] \\
 & = y  && [\text{inverses cancel out}] \\
\end{aligned}
$$
```

&nbsp;

### ANOVA Sum of Squares decomposition

We have $k$ groups of metric observations. For group $1 \le j \le k$, there are $n_j$ observations. Let $x_{ij}$ be observation $1 \le i \le n_j$ for group $1 \le j \le k$. Let $\bar{x}_j = \frac{1}{n} \sum_{i = 1}^{n_j} x_{ij}$ be the mean of group $j$ and let $\bar{\bar{x}} = \frac{1}{k} \sum_{j=1}^k \frac{1}{n_j} \sum_{i=1}^{n_j} x_{ij}$ be the grand mean of all data points. We would like to show that the total sum of squares can be decomposed into two summands: the within-group sum of squares and the between-group sum of squares:

$$ 
\underbrace{
  \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{\bar{x}})^2
}_{\text{Total SS}}  = 
\underbrace{
  \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2
}_{\text{Within-Group SS}} +
\underbrace{
  \sum_{j=1}^k n_j (\bar{x}_j - \bar{\bar{x}})^2
}_{\text{Between-Group SS}}
$$ 

To show this, we first establish a lemma, which will also be useful later:

```{lemma, label = "SS-cancellation", name = "Sum of squares cancellation"}
Let $\vec{x}$ be a vector of $n$ real-valued numbers, and let $\bar{x} = \frac{1}{n} \sum_{i=i}^n x_i$ be its mean. The sum of squares around the mean is zero:
$$
  \sum_{i=1}^n (x_i - \bar{x}) = 0
$$
  
```

```{proof}
$$
\begin{aligned}
  \sum_{i=i}^n (x_i - \bar{x}) 
  & = 
  \sum_{i=i}^n (x_i - \frac{1}{n} \sum_{j=1}^n x_j) 
  && 
  [\text{by def. of mean}] 
  \\
  & = 
  \sum_{i=i}^n x_i - \frac{n}{n} \sum_{j=1}^n x_j) 
  && 
  [\text{second summand independent of } i] 
  \\ 
  & = 0
  \\ 
\end{aligned}
$$
```


&nbsp;


```{proposition, label = "ANOVA-SS-decomposition", name = "Sum of squares decomposition (ANOVA)"}
If $x_{ij}$ is observation $1 \le i \le n_j$ for group $1 \le j \le k$, $\bar{x}_j = \frac{1}{n} \sum_{i = 1}^{n_j} x_{ij}$ the mean of group $j$ and $\bar{\bar{x}} = \frac{1}{k} \sum_{j=1}^k \frac{1}{n_j} \sum_{i=1}^{n_j} x_{ij}$ be the grand mean of all data points, then:
$$ 
\underbrace{
  \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{\bar{x}})^2
}_{\text{Total SS}}  = 
\underbrace{
  \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2
}_{\text{Within-Group SS}} +
\underbrace{
  \sum_{j=1}^k n_j (\bar{x}_j - \bar{\bar{x}})^2
}_{\text{Between-Group SS}}
$$ 
  
```


```{proof}
$$
\begin{aligned}
  & 
  \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{\bar{x}})^2 
  \\
  = &  
  \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j + \bar{x}_j - \bar{\bar{x}})^2
  && 
  [- \bar{x}_j + \bar{x}_j = 0] 
  \\
  = &  
  \sum_{j=1}^k \sum_{i=1}^{n_j} \left [ (x_{ij} - \bar{x}_j)^2 + 2(x_{ij} - \bar{x}_j)(\bar{x}_j - \bar{\bar{x}}) + (\bar{x}_j - \bar{\bar{x}})^2 \right ]
  && 
  [\text{binomial theorem}] 
  \\
  = &  
  \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2 + 
  \sum_{j=1}^k \sum_{i=1}^{n_j} (\bar{x}_j - \bar{\bar{x}})^2 + 
  \sum_{j=1}^k \sum_{i=1}^{n_j} 2(x_{ij} - \bar{x}_j)(\bar{x}_j - \bar{\bar{x}})
  && 
  [\text{rearranging} ] 
  \\
  = &  
  \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2 + 
  \sum_{j=1}^k n_j (\bar{x}_j - \bar{\bar{x}})^2 + 
  2 \sum_{j=1}^k (\bar{x}_j - \bar{\bar{x}}) \underbrace{\sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)}_{\text{=0 by lemma}}
  && 
  [\text{independences} ] 
  \\
    = &  
  \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2 + 
  \sum_{j=1}^k n_j (\bar{x}_j - \bar{\bar{x}})^2
  && 
  [\text{by lemma} ] 
\end{aligned}
$$
```

&nbsp;

## Selected tests {#ch-03-05-hypothesis-testing-tests}

## Three approaches {#ch-03-05-hypothesis-testing-3-approaches}

### Fisher

Frequentist hypothesis testing is superficially similar to Popperian falsificationism. It is, however, quite the opposite when looked at more carefully. Popper famously denied that empirical observation could constitute positive evidence in favor of a research hypothesis. Research hypotheses can only be refuted, viz., when their logically consequences are logically incompatible with the observed data. In a Popperian science, what is refuted are research hypotheses; frequentist statistics instead seeks to refute null-hypotheses and counts successful refutation of a null-hypothesis as evidence in favor of a research hypothesis.

### Neyman-Pearson

A conventional threshold on $p$-values may govern a categorical decision whether to reject or not reject the null-hypothesis (in the Neyman-Pearson approach). This threshold is essentially an upper-bound on a particular kind of error, namely the error to falsely reject the null-hypothesis when it is in fact true (a so-called type-1 error or $\alpha$-error). 

### Hypbrid modern NHST
