# (PART) Preliminaries {-}

# General Introduction

This chapter lays out the learning goals of this book (Section \@ref(intro-learning-goals)) and describes how these goals are to be achieved (Section \@ref(intro-course-structure)). Section \@ref(intro-tools-methods) details which technical tools and methods are covered here, and which are not. There will be some information on the kinds of data sets we will use during the course in Section \@ref(intro-data-sets). Finally, Section \@ref(intro-installation) provides information about how to install the necessary tools for this course.


## Learning goals {#intro-learning-goals}

At the end of this course students should:

- feel confident to pick up any data set to explore in a hypothesis-driven manner
- have gained the competence to 
  - understand complex data sets, 
  - manipulate a data set, so as to
  - plot aspects of it in ways that are useful for answering a given research question
- understand the general logic statistical inference in the frequentist and the Bayesian approach
- be able to independently evaluate statistical analyses based on their adequacy for a given research question and data set
- be able to critically assess the adequacy of analyses commonly found in the literature

Notice that this is, although a lot of hard work already, sill rather modest! It doesn't actually say that we necessarily aim at the competence to *do it* or even to *do it flawlessly*! **Our main goal is understanding**, because that is the foundation of practical success *and* the foundation of an ability to learn more in the future. **We do not teach tricks! We do not share recipes!** 

## Course structure {#intro-course-structure}

The course consists of fours parts. After giving a more detailed overview of the course, Part I introduces R the main programming language that we will use. Part II covers what is often called *descriptive statistics*. It also gives us room to learn more about R when we massage data into shape, compute summary statistics and plot various different data types in various different ways.

Part III is the main theoretical part. It covers what is often called *inferential statistics*. Two aspects distinguish this course from the bulk of its cousins out there. First, we use a **dual-pronged approach**, i.e., we are going to introduce both the frequentist and the Bayesian approach to statistical inference side by siide. The motivation for this is that seeing the contrast between the two will aid our understanding of either one. Second, we will use a **computational approach**, i.e., we foster understanding of mathematical notions with computer simulations or other variants of helpful code. 

Part IV covers applications of what we have learned so far. It focuses on **generalized linear models**, a class of models that have become the new standard for analyses of experimental data in the social and psychological sciences, but are also very useful for data exploration on other domains.

There are also appendices ... [FILL ME]

## Tools and topics covered (and not covered) here {#intro-tools-methods}

The main programming language this course will use and introduce is R [instert link & citation]., in paricular the *tidyverse* (see Secion [insert ref]). We will also be using the probabilistic programming language WebPPL [insert link & citation], but only "passively" in order to quickly obtain results from probabilistic calculations that we can experiment with directly in the browser. We will not learn to write WebPPL code from scratch.

We will rely on the R package `brms` [insert link & citation] for running Bayesian generalized regression models, which itself relies on the probabilistic programming language `Stan` [insert link & citation]. We will, however, not learn about `Stan` in this course. Instead of `Stan` we will use the package `greta` to write out models and do inference with them. This is because, for current learning purposes, the language in which `greta` formulates its models is much closer to R and so, let's hope, easier to learn.

Section \@ref(intro-installation) gives information about how to install these, and other, tools necessary for this course.

The main topics that this course will cover are:

+ **data preparation**: how to clean up, and massage a data set into shape for plotting and analysis

+ **data visualization**: how to select aspects of data to visualize in informative and useful ways

+ **statistical models**: what that is, and why it's beneficial to think in terms of models, not tests

+ **statistical inference**: what that is, and how it's done in frequentist and Bayesian approaches

+ **hypothesis testing**: how Frequentists and Bayesians test scientific hypotheses

+ **generalized regression**: how to apply GRMs to different types of data sets

There is, obviously, more that we will *not* cover in this course, that what we *can* possibly cover. We will, for instance, not dwell at any length on the specifics of algorithms for computing statistical inferences or model fits. We will also scratch the surface of the history and the philosophy of science of statistics only to the extent that it helps understand the theoretical notions and practical habits that are important in the context of this course. We will also not do heavy math.

## Data sets covers {#intro-data-sets}

... say something about the data sets we will look at ...

We want to learn how to do data analysis. This is impossible without laying hands (keys?) on several data sets. But switching from one data set to another is mentally taxing. It is also difficult to focus and really care about any-old data set. This course, therefore relies, to some extent on data sets that are, hopefully, generally interesting for the target audience: students of cognitive science. Consequently, several data sets that we will use repeatedly in this class come from various psychological experiments. 

To make this even more emersive, these experiments are implemented as browser-based experiments. This makes it possible for participants to do the exact experiment whose data we are analyzing for themselves (and maybe generate some more intuitions, maybe generate some hypotheses) about the very data at hand. But it also makes it possible that we will analyze ourselves. That's why part of the exercises for this course will run additional analyses on data collected from the aspiring data analysts themselves. If you want to become an analyst, you should also have undergone analysis yourself, so to speak.

But we will also look at other data sets, beyond (rather simple, behavioral) psychology experiments. [FILL ME; DESCRIBE WHICH ONES WE USE]

[SAY HOW THE DATA SETS ARE OBTAINED?]

## Installation {#intro-installation}

... include description of how to install all the software needed for this course ...

... ideally let's write a package for the course that, upon installation, also installs all the right dependencies; this package could also include (some of) the data sets we will use ... 

what we need:
- RStudio
- an up-to-date browswer
- R
  - tidyverse
  - whatever is needed for Rmarkdown
  - devtools
  - brms (Stan -> C)
  - greta (tensorflow -> python)
  - gridExtra
  - cowplot
  - ???
- 


