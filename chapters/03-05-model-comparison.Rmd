# Model Comparison {#Chap-03-06-model-comparison}

<hr>

<div style = "float:right; width:40%;">
<img src="visuals/badge-model-comparison.png" alt="badge model comparison">  
</div>  

- AIC
- likelihood ratio test
- Bayes factor

## Case study: recall models {#Chap-03-06-model-comparison-case-study}

As a running example for this chapter, we borrow from @Myung2003:Tutorial-on-Max and consider a fictitious data set of recall rates and two models to explain this data. 

As for data, for each time point (in seconds) $t \in \{1, 3, 6, 9, 12, 18\}$, we have 100 (binary) observations of whether a previously memorized item was recalled correctly.


```{r}
# time after memorization (in seconds)
t = c(  1,   3,   6,   9,  12,  18)
# proportion (out of 100) of correct recall
y = c(.94, .77, .40, .26, .24, .16)
# number of observed correct recalls (out of 100)
obs = y*100
```

A visual representation of this data set is here:

```{r echo = F}
tibble(
  t, obs
) %>% 
  ggplot(aes(x = t, y = y)) +
  geom_point() +
  labs(
    x = "time (in seconds)",
    y = "proportion of correct recall",
    title = "(fictitious) recall data"
  ) +
  ylim(c(0,1))
  
```

We are interested in comparing two theoretically different models for this data. Models differ in their assumption about the functional relationship between recall probability and time. The **exponential model** assumes that the recall probaility $\theta_t$ at time $t$ is an exponential decay function with parameters $a$ and $b$:

$$\theta_t(a, b) = a \exp (-bt), \ \ \ \ \text{where } a,b>0 $$

In contrast, the **power model** assumes that the relationship is that of a power function:

$$\theta_t(c, d) = ct^{-d}, \ \ \ \ \text{where } c,d>0 $$

These models therefore make different (parameterized) predictions about the time course of forgetting/recall. Figure \@ref(fig:Chap-03-06-model-comparison-model-predictions) shows predictions of each model for $\theta_t$ for different parameter values:

```{r Chap-03-06-model-comparison-model-predictions, echo = F, fig.height=12, fig.cap = "examples of predictions of the exponential and the power model of forgetting for different values of each model's paramters."}
expo = function(x, c, d) return( c* exp(-x*d) )
power = function(x, a, b) return( a*x^(-b) )
myCols = project_colors[1:3]

forgetPlotExpo = ggplot(data.frame(x = c(1,20)), aes(x)) +
         stat_function(fun = function(x) expo(x, 1,1), aes(color = "a,b=1")) +
         stat_function(fun = function(x) expo(x, 2,2), aes(color = "a,b=2")) +
         stat_function(fun = function(x) expo(x, 1,0.2), aes(color = "a=1,b=0.1")) +
         scale_colour_manual("Function", breaks = c("a,b=1", "a,b=2", "a=1,b=0.1"), values = myCols) +
          ggtitle("exponential model") + geom_point(data = forgetData, aes(x = t, y = y)) + ylab("recall prob.") + xlab("time t")
forgetPlotPower = ggplot(data.frame(x = c(1,20)), aes(x)) +
         stat_function(fun = function(x) power(x, 1,1), aes(color = "c,d=1")) +
         stat_function(fun = function(x) power(x, 2,2), aes(color = "c,d=2")) +
         stat_function(fun = function(x) power(x, 2,1), aes(color = "c=2, d=1")) +
         scale_colour_manual("Function", breaks = c("c,d=1", "c,d=2", "c=2, d=1"), values = myCols) +
          ggtitle("power model") + geom_point(data = forgetData, aes(x = t, y = y)) + ylab("recall prob.") + xlab("time t")
cowplot::plot_grid(forgetPlotExpo, forgetPlotPower, nrow = 2)
```

The research question of relevance is: which of these two models is a better model for the observed data?

## Akaike Information Criterion {#Chap-03-06-model-comparison-AIC}

A wide-spread approach to model comparison used in the frequentist paradigm is to use the **Akaike information criterion (AIC)**. The AIC is the most common instance of a class of measures for model comparison known as *information criteria*, which all draw on information-theoretic notions to compare how good each model is.

If $M$ is a frequentist model, specified by (parameterized) likelihood function $P(D \mid \theta)$, with $k$ model parameters, and if $D_\text{obs}$ is the observed data, then the AIC score of model $M$ given $D_\text{obs}$ is defined as:

$$
\begin{aligned}
\text{AIC}(M, D_\text{obs}) & = 2k - 2\log P(D_\text{obs} \mid \hat{\theta}), \ \ \ \text{where} \\
\hat{\theta} & = \arg \max_{\theta} P(D_\text{obs} \mid \theta) \ \ \ \ \ \ \text{[best-fitting parameters; MLE]}
\end{aligned}
$$

The lower an AIC score the better the model (in comparison to other models for the same data $D_\text{obs}$). All else equal, the higher the number of free parameters $k$ the worse the model's AIC score. The first summand in the definition above is, therefore, a measure of **model complexity**. As for the second summand, think of $- \log P(D_\text{obs} \mid \hat{\theta})$ as a measure of (information-theoretic) surprisal: how surprising is the observed data $D_\text{obs}$ from the point of view of model $M$ under the most favorable circumstances (that is, the MLE of $\theta$). The higher the probability $P(D_\text{obs} \mid \hat{\theta})$, the better the model's AIC score, all else equal.

To apply AIC-based model comparison to the recall models, we first need to compute the MLE of each model (see Chapter \@ref(ch-03-03-estimation-frequentist)). Here are functions that return the negative log-likelihood of each model, for any (suitable) pair of parameter values:

```{r}
# generic neg-log-LH function (covers both models)
nLL_generic <- function(par, model_name) {
  w1 <- par[1]
  w2 <- par[2]
  # make sure paramters are in acceptable range
  if (w1 < 0 | w2 < 0 | w1 > 20 | w2 > 20) {
    return(NA)
  }
  # calculate predicted recall rates for given parameters
  if (model_name == "exponential") {
    theta <- w1*exp(-w2*t)  # exponential model
  } else {
    theta <- w1*t^(-w2)     # power model
  }
  # avoid edge cases of infinite log-likelihood
  theta[theta <= 0.0] <- 1.0e-4
  theta[theta >= 1.0] <- 1-1.0e-4
  # return negative log-likelihood of data
  - sum(dbinom(x = obs, prob = theta, size = 100, log = T))
}
# negative log likelihood of exponential model
nLL_exp <- function(par) {nLL_generic(par, "exponential")}
# negative log likelihood of power model
nLL_pow <- function(par) {nLL_generic(par, "power")}
```

These functions are then optimized by using built-in function `optim`. The results are shown in the table below

```{r}
# getting the best fitting values
bestExpo <- optim(nLL_exp, par = c(1,0.5))
bestPow  <- optim(nLL_pow, par = c(0.5,0.2))
MLEstimates = data.frame(model = rep(c("expo", "power"), each = 2),
                         parameter = c("a", "b", "c", "d"),
                         value = c(bestExpo$par, bestPow$par))
knitr::kable(MLEstimates)
```

The MLE-predictions of each model are shown in Figure \@ref(fig:Chap-03-06-model-comparison-MLE-fits) below, alongside the observed data.

```{r Chap-03-06-model-comparison-MLE-fits, echo = F, fig.cap = "Predictions of the exponential and the power model under best-fitting parameter values."}
a = bestExpo$par[1]
b = bestExpo$par[2]
c = bestPow$par[1]
d = bestPow$par[2]
forgetPlotBest <-  ggplot(data.frame(x = c(1,20)), aes(x)) +
         stat_function(fun = function(x) expo(x, a, b), aes(color = "exponential")) +
         stat_function(fun = function(x) power(x, c, d), aes(color = "power")) +
         scale_colour_manual("Function", breaks = c("exponential", "power"), values = project_colors[1:2]) +
          ggtitle("MLE fits") + geom_point(data = forgetData, aes(x = t, y = y)) +
  labs(
    x = "time",
    y = "recall rate"
  )
forgetPlotBest
```


It is impossible to say by visual inspection of Figure \@ref(fig:Chap-03-06-model-comparison-MLE-fits) which model should be preferred. Numbers might help see more fine-grained differences:

```{r}
predExp = expo(t,a,b)
predPow = power(t,c,d)
modelStats = tibble(
  model = c("expo", "power"),
  `log likelihood` = round(c(-bestExpo$value, -bestPow$value),3),
  probability = signif(exp(c(-bestExpo$value, -bestPow$value)),3),
  # sum of squared errors
  SS = round(c( sum((predExp-y)^2), sum((predPow-y)^2)),3)
)
modelStats
```

The exponential model has a higher log-likelihood, a higher probability, and a lower sum of squares. This suggests that the exponential model is better. 

The AIC-score of these models is a direct function of the negative log-likelihood. Since both models have the same number of parameters, we also arrive at the same verdict: based on comparison of AIC-scores, the exponential model is the better model.

```{r}
get_AIC <- function(optim_fit) {
  2 * length(optim_fit$par) - 2 * optim_fit$value
}
tibble(
  AIC_exponential = get_AIC(bestExpo),
  AIC_power = get_AIC(bestPow)
)
```


## Likelihood-Ratio Test {#Chap-03-06-model-comparison-LR-test}

## Bayes factors {#Chap-03-06-model-comparison-BF}

## Snippets / Results

### Bayes factors for ROPE-d hypotheses through encompassing models

Suppose that instead of addressing the point-valued hypothesis $\theta = \theta^*$, we are able (e.g., through prior research or *a priori* conceptual considerations) to specify a reasonable *region of practical equivalence (=ROPE)* around the parameter value of interest and address what we may call the **ROPE-d hypothesis** $\theta \in [\theta^* - \epsilon\ ;\ \theta^* + \epsilon]$, or $\theta = \theta^* \pm \epsilon$ for short.

The Savage-Dickey method can be generalized to cover also interval-valued hypotheses in general, and therefore also ROPE-d hypotheses in particular. The previous literature has focused on inequality-based intervals (like $\theta \ge 0.5$) [@KlugkistKato2005:Bayesian-model,@WetzelsGrasman2010:An-encompassing,@Oh2014:Bayesian-compar]. Here, we show that this method also extends to arbitrary intervals.

[@KlugkistKato2005:Bayesian-model]
[@WetzelsGrasman2010:An-encompassing;@Oh2014:Bayesian-compar]

**Encompassing Priors**

```{r}
tibble(
  delta = seq(-3,3,length.out = 1000),
  encompassing = dnorm(delta)
) %>% 
  mutate(
    null = ifelse(-0.1 <= delta & delta <= 0.1,
                  encompassing, 0),
    alt  = ifelse(-0.1 >= delta | delta >= 0.1,
                  encompassing, 0)
  ) %>% 
  mutate(
    encompassing = encompassing / sum(encompassing),
    null = null / sum(null),
    alt = alt / sum(alt)
  ) %>% 
  pivot_longer(cols = -delta, names_to = "model", values_to = "density") %>% 
  mutate(model = factor(model, ordered = T, levels = c("encompassing", "null", "alt"))) %>% 
  ggplot(aes(x = delta, y = density)) +
  geom_line() + 
  geom_area(fill = "lightgray", alpha = 0.6) +
  facet_wrap(. ~ model, nrow = 3, scales = "free") +
  labs(
    x = "", y = ""
  ) +
  guides(fill = F) +
  theme(
    axis.text.y  = element_blank(),
    axis.ticks.y = element_blank()
  )
```


**Theorem statement**

Fix a Bayesian model $M$ with pior $P_M(\theta, \omega)$ and likelihood function $P_M(D \mid \theta, \omega)$, where $\theta$ is the parameter of interest and $\omega$ is a vector of other (nuisance) parameters. Assume that priors over $\theta$ are independent of the nuisance parameters $\omega$. For an interal-valued hypothesis $H_0$ $\theta \in [a,b]$, the Bayes Factor in favor of this hypothesis over its negation $H_a$ $\theta \not \in [a,b]$ can be expressed as:

$$ \text {BF}_{01} = \frac{\text{posterior-odds of } H_0}{\text{prior-odds of } H_0} $$

**Proof**

```{theorem, "BF-ROPED-hypotheses"}
FILL ME

```

```{proof}
TBD
```

### Example (binomial 24/7)

Using the Savage-Dickey method, the Bayes factor in favor of point-valued hypothesis $\theta = 0.5$ is:

```{r}
bf_point_hypothesis <- dbeta(0.5,8,18) / dunif(0.5, 0, 1)
bf_point_hypothesis
```

Using the ROPE-d method to compute the interval-valued hypothesis $\theta = 0.5 \pm \epsilon$ is:

```{r}
# set the scene
theta_null <- 0.5
epsilon <- 0.01          # epsilon margin for ROPE
upper <- theta_null + epsilon   # upper bound of ROPE
lower <- theta_null - epsilon   # lower bound of ROPE
# calculate prior odds of the ROPE-d hypothesis
prior_of_hypothesis <- qbeta(upper, 1, 1) - qbeta(lower, 1, 1)
prior_odds <- prior_of_hypothesis / (1 - prior_of_hypothesis)
# calculate posterior odds of the ROPE-d hypothesis
posterior_of_hypothesis <- qbeta(upper, 8, 18) - qbeta(lower, 8, 18)
posterior_odds <- posterior_of_hypothesis / (1 - posterior_of_hypothesis)
# calculate Bayes Factor
bf_ROPEd_hypothesis <- posterior_odds / prior_odds
bf_ROPEd_hypothesis
```

