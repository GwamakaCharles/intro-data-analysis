# Summary statistics {#Chap-02-03-summary-statistics}

<hr>

<div style = "float:right; width:45%;">
<img src="visuals/badge-summary-statistics.png" alt="badge summary statistics">  
</div>  

- counts and frequencies
- one-dimensional data
  - measures of central tendency
    - mean/mode/median/
  - meaures of dispersion
    - variance, standard deviation, quantiles
  - non-parametric estimates of confidence
    - bootstraped CI of mean
  - ...
- multi-dimensional data
  - covariance
  - Bravais-Pearson correlation
  - Spearman/Kendall
  

## Exploring numerical data in a vector {#02-02-exploring}

The first part of the Mental Chronometry experiment is a simple reaction time task.

```{r}
# read a data set of reaction times (as a tibble)
# RTs <- read_csv("https://tinyurl.com/y4jo8mox")
RT <- read_csv("data_sets/ch-03-set-01-MC-RTs.csv") %>% pull(RT)

# check its content
glimpse(RT)
```

### Bootstrapped 95% confidence intervals

[Bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) is an elegant way to obtain measures of confidence for summary statistics. These measures of confidence can be used for parameter inference, too. We will discuss parameter inference at length in Section \@ref(03-04-intro-learning-goals). In this course, we will not use bootstrapping as an alternative approach to parameter inference. We will, however, follow a common practice (at least in some areas of Cognitive Psychology) to use **bootstrapped 95% confidence intervals of the mean** as part of descriptive statistics, i.e., in summaries and plots of the data.

The bootstrap is a method from a more general class of algorithms, namely so-called **resampling methods**. The general idea is, roughly put, that we treat the data at hand as the true representation of reality. We then imagine that we run an experiment on that (restricted, hypothetical) reality. We then ask ourselves: what would we estimate (e.g., as a mean) in any such hypothetical experiment. The more these hypothetical measures derived from hypothetical experiments based on a hypthetical reality differ, the less confident we are in the estimate. Sounds weird, but is mindblowingly elegant.

An algorithm for constructing a 95% confidence interval of the mean of vector $D$ of numeric data with length $k$ looks as follows:

1. take $k$ samples from $D$ with replacement, call this $D^{\textrm{rep}}$^[$D^{\textrm{rep}}$ is short for "repeated Data". We will use this concept more later on. The idea is that we consider "hypothetical data" which we have not perceived, but which we might have. Repeated data is (usually) of the same shape and form as the original, observed data, which is also sometimes noted as $D^{\textrm{obs}}$ for clarity in comparison to $D^{\textrm{rep}}$.]
2. calculate the mean $\mu(D^{\textrm{rep}})$ of the newly sampled data
3. repeat steps 1 and 2 to gather $r$ means of different resamples of $D$; call the result vector $\mu_{\textrm{sampled}}$
4. the boundaries of the 95% inner quantile of $\mu_{\textrm{sampled}}$ are the bootstrapped 95% confidence interval of the mean

The higher $r$, i.e., the more samples we take, the better the estimate. The higher $k$, i.e., the more observations we have to begin with, the less variable the means $\mu(D^{\textrm{rep}})$ of the resampled data will usually be. Hence, usually, the higher $k$ the smaller the bootstrapped 95% confidence interval of the mean.

Here is a convenience function that we will use throughout the book to produce bootstrapped 95% confidence intervals of the mean:

```{r}
## takes a vector of numbers and returns bootstrapped 95% ConfInt
## for the mean, based on `n_resamples` re-samples (default: 1000)
bootstrapped_CI <-  function(data_vector, n_resamples = 1000) {
  resampled_means <- map_dbl(1:n_resamples, function(i) {
       mean(sample(x = data_vector, 
                   size = length(data_vector), 
                   replace = T)
       )
    }
  )
  tibble(
    'lower' = quantile(resampled_means, 0.025),
    'mean'  = mean(data_vector),
    'upper' = quantile(resampled_means, 0.975)
  ) 
}
```

Applying this method to the vector of reaction times from above we get:

```{r}
bootstrapped_CI(RT)
```

Notice that, since `RT` has length `r length(RT)`, i.e., we have $k = `r length(RT)`$ observations in the data, the bootstrapped 95% confidence interval is rather narrow. Compare this against a case of $k = 30$:

```{r}
# 30 samples from a standard normal
smaller_data = rnorm(n = 30, mean = 0, sd = 1)
bootstrapped_CI(smaller_data)
```

To obtain summary statistics for different groups of a variable, we can use the function `bootstrapped_CI` conveniently in concert with nested tibbles, as demonstrated here on the Mental Chronometry data set (see Appendix \@ref(app-93-data-sets-mental-chronometry):

```{r}
mc_data_cleaned = read_csv("data_sets/mental-chrono-data_cleaned.csv") 
mc_data_cleaned %>%
  group_by(block) %>%
  nest() %>% 
  summarise(
    CIs = map(data, function(d) bootstrapped_CI(d$RT))
  ) %>% 
  unnest(CIs)
```

<!-- Homework: -->
<!-- - calculate bs 95% CI for vector [1,2,3] by hand! -->


## Co-variance & correlation

Covariance:

$$cov(x,y) = \frac{1}{n-1} \ \sum_{i=1}^n (x_i - \bar{x}) \ (y_i - \bar{y})$$

There is a visually intuitive geometric interpretation of covariance. To see this, let's look at a short contrived example. 

```{r}
contrived_example <- 
  tribble(
    ~x,   ~y,
    2,    2,
    2.5,  4,
    3.5,  2.5,
    4,    3.5
  )
```

First, notice that the mean of `x` and `y` is 3:

```{r}
means_contr_example <- map_df(contrived_example, mean)
means_contr_example
```

We can then compute the covariance as follows:

```{r}
contrived_example <- 
  contrived_example %>% 
  mutate(
    area_rectangle =               (x-mean(x)) * (y - mean(y)),
    covariance     =  1/ (n()-1) * sum((x-mean(x)) * (y - mean(y)))
  )
contrived_example
```


```{r}
contrived_example %>% 
  ggplot(aes(x, y)) +
  geom_rect(
    aes(
      xmin = map_dbl(x, function(i) {min(i, means_contr_example$x)}),
      ymin = map_dbl(y, function(i) {min(i, means_contr_example$y)}),
      xmax = map_dbl(x, function(i) {max(i, means_contr_example$x)}),
      ymax = map_dbl(y, function(i) {max(i, means_contr_example$y)}),
      fill = area_rectangle
    )
  ) +
  geom_point(color = "darkorange", size = 4) +
  geom_point(data = means_contr_example, color = "white", size = 10) +
  theme(legend.position = "none") +
  ggtitle("Rectangular areas contributing to the computation of covariance")
```

We can, of course, also calculate covariance just with a built-in base R function, `cov`:

```{r}
with(contrived_example, cov(x,y))
```

