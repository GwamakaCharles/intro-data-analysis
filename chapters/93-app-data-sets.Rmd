# Data sets used in the book

Several data sets are used throughout the book as 'running examples'. They occur in different places to illustrate different things. This chapter centrally describes each data set, together with the most important visualizations and analyses.

## Mental Chronometry


### Nature, origin and rationale of the data

[Francis Donders](https://en.wikipedia.org/wiki/Franciscus_Donders) is remembered as one of, if not the first experimental cognitive psychologists. He famously introduced the **subtraction logic** which looks at difference in reaction times across different tasks to infer difference in the complexity of the mental processes involved in these tasks. The Mental Chronometry data set presents the results of an online replication of one such subtraction-experiment.

#### The experiment

50 participants were recruited using the crowd-sourcing platform [Prolific](https://www.prolific.co) and paid for their participation.

In each experiment trial, participants see either a blue square or a blue circle appear on the screen and are asked to respond as quickly as possible. The experiment consists of three parts, presented to all participants in the same order (see below). The parts differ in the adequate response to the visual stimuli.

1. **Reaction task**

	The participant presses the space bar whenever there is a stimulus (square or circle)

	*Recorded*: reaction time

2. **Go/No-Go task**

	The participant presses the space bar whenever their target (one of the two stimuli) is on the screen

	*Recorded*: the reaction time and the response

3. **Discrimination task**

	The participant presses the **F** key on the keybord when there is one of the stimuli and the **J** key when there is the other one of the stimuli on the screen.

	*Recorded*: the reaction time and the response

The **reaction time** measurement starts from the onset of the visual stimuli to the button press. The **response** variable records whether the reaction was correct or incorrect.

For each participant, the experiment randomly allocates one shape (circle or square) as the target to be used in both the second and the third task.

The experiment was realized using [_magpie](https://magpie-ea.github.io/magpie-site/index.html) and can be tried out [here](https://magpie-exp-mental-chronometry.netlify.com).

#### Theoretical motivation & hypotheses

We expect that reaction times of correct responses are lowest in the reaction task, higher in the Go/No-Go task, and highest in the discrimination task.


### Loading and preprocessing the data

The raw data produced by the online experiment is not particularly tidy. It needs substantial massages before plotting and analysis.

```{r}
d_raw <- read_csv('data_sets/mental-chrono-data_raw.csv')
glimpse(d_raw)
```

The most pressing problem is that entries in the column `trial_type` contain two logically separate pieces of information: the block (reaction, go/no-go, discrimination) *and* whether the data comes from a practice trial (which we want to discard) or a main trial (which we want to analyze). We therefore separate this information, and perform some other massages, to finally select a preprocessed data set for further analysis:

```{r}
block_levels <- c("reaction", "goNoGo", "discrimination") # ordering of blocks for plotting, etc. 

d_preprocessed <- d_raw %>% 
  separate(trial_type, c("block", "stage"), sep = "_", remove = FALSE) %>%
  mutate(comments = ifelse(is.na(comments), "non given", comments)) %>% 
  filter(stage == "main") %>% 
  mutate(
    block = factor(block, ordered = T, levels = block_levels),
    response = ifelse(is.na(response), "none", response)
  ) %>%
  filter(response != "wait") %>% 
  rename(
    handedness = languages, # variable name is simply wrong
    total_time_spent = timeSpent
  ) %>% 
  select(
    submission_id, 
    trial_number, 
    block, 
    stimulus, 
    RT, 
    handedness, 
    gender, 
    total_time_spent,
    comments
  )

# write_csv(d_preprocessed, 'mental-chrono-data_preprocessed.csv')
```


### Cleaning the data

Remeber that the criteria for data exclusion should ideally be defined before data collection (or at least inspection). They should definitely never be chosen in such a way as to maximize the "desirability" of an analysis. Data cleaning is not a way of making sure that your favorite research hypothesis "wins".

Although we have not preregistered any data cleaning regime or analyses for this data set, we demonstrate a frequently used cleaning scheme for reaction time data, which does depend on the data in some sense, but does not require precise knowledge of the data. In particular, we are going to do this:

1. We remove remove the data from an individual participant $X$ if there is an experimental condition $C$ such that the mean RT of $X$ for condition $C$ is more than 2 standard deviations away from the overal mean RT for condition $C$.
2. From the remaining data, we then remove any individual trial $Y$ if the RT of $Y$ is more than 2 standard deviations away from the mean of experimental condition $C$ (where $C$ is the condition of $Y$, of course).

Notice that in the case at hand, the experimental conditions are the three types of tasks.

#### Cleaning by-participant

Our rule for removing data from outlier participants is this:

> We remove remove the data from an individual participant $X$ if there is an experimental condition $C$ such that the mean RT of $X$ for condition $C$ is more than 2 standard deviations away from the overal mean RT for condition $C$. We also remove all trials with reaction times below 100ms.

This procedure is implemented in this code:

```{r}

# summary stats (means) for participants
d_sum_stats_participants <- d_preprocessed %>% 
  group_by(submission_id, block) %>% 
  summarise(
    mean_P = mean(RT)
  )

# summary stats (means and SDs) for conditions
d_sum_stats_conditions <- d_preprocessed %>% 
  group_by(block) %>% 
  summarise(
    mean_C = mean(RT),
    sd_C   = sd(RT)
  )
  
d_sum_stats_participants <- 
  full_join(
    d_sum_stats_participants,
    d_sum_stats_conditions,
    by = "block"
  ) %>% 
  mutate(
    outlier_P = abs(mean_P - mean_C) > 2 * sd_C
  )

# show outlier participants
d_sum_stats_participants %>% filter(outlier_P == 1) %>% show()
```

When plotting the data for this condition and this participant, we see that the high overall mean is not just caused by a single outlier, but several trials that took longer than 1 second.

```{r}
d_preprocessed %>% 
  semi_join(
    d_sum_stats_participants %>% filter(outlier_P == 1), 
    by = c("submission_id")
  ) %>% 
  ggplot(aes(x = trial_number, y = RT)) +
  geom_point()
```

We are then going to exclude this participant's entire data from all subsequent analysis:^[This may seem a harsh step, but when data acquisition is cheap, it's generally not a bad strategy to be very strict in exclusion criteria, and to apply rules that are not strongly context-dependent.]

```{r}
d_cleaned <- d_preprocessed %>% 
  filter(submission_id != d_sum_stats_participants$submission_id[1] )
```


#### Cleaning by-trial

Our rule for exclusing data from individual trials is:

> From the remaining data, we then remove any individual trial $Y$ if the RT of $Y$ is more than 2 standard deviations away from the mean of experimental condition $C$ (where $C$ is the condition of $Y$, of course). We also remove all trials with reaction times below 100ms.

The following code implements this:

```{r}

# mark individual trials as outliers
d_cleaned <- d_cleaned %>% 
  full_join(
    d_sum_stats_conditions,
    by = "block"
  ) %>% 
  mutate(
    trial_type = case_when(
      abs(RT - mean_C) > 2 * sd_C ~ "too far from mean",
      RT < 100 ~ "< 100ms",
      TRUE ~ "acceptable"
    ) %>% factor(levels = c("acceptable", "< 100ms", "too far from mean")),
    trial = 1:nrow(d_cleaned)
  )

# visualize outlier trials

d_cleaned %>% 
  ggplot(aes(x = trial, y = RT, color = trial_type)) +
  geom_point(alpha = 0.4) + facet_grid(~block) + 
  geom_point(alpha = 0.9, data = filter(d_cleaned, trial_type != "acceptable"))

```

So, we remove `r filter(d_cleaned, trial_type != "acceptable") %>% nrow()` individual trials.

```{r}
d_cleaned <- d_cleaned %>% 
  filter(trial_type == "acceptable")
```


### Exploration: summary stats & plots

What's the distribution of `total_time_spent`, i.e., the time each participant took to complete the whole study?

```{r}
d_cleaned %>% 
  select(submission_id, total_time_spent) %>% 
  unique() %>% 
  ggplot(aes(x = total_time_spent)) +
  geom_histogram()
```

There are two participants who took noticably longer than all the others, but we need not necessarily be concerned about this, because it is not unusual for participants of online experiments to open the experiment and wait before actually starting.

Here are summary statistics for the reaction time measures for each condition (= block).

```{r}
## TODO make pretty and find proper place for convenience function for bootstrap

## takes a vector of numbers and returns bootstrapped 
## credible interval (95% per default), based on `n_resamples` re-samples
bootstrapped_CI <-  function(data_vector, n_resamples = 1000) {
  resampled_means <- map_dbl(1:n_resamples, function(i) {
       mean(sample(x = data_vector, 
                   size = length(data_vector), 
                   replace = T)
       )
    }
  )
  tibble(
    'lower' = quantile(resampled_means, 0.025),
    'mean'  = mean(data_vector),
    'upper' = quantile(resampled_means, 0.975)
  ) 
}

## TODO use nesting
d_sum_stats_blocks_cleaned <- d_cleaned %>%
  group_by(block) %>%
  nest() %>% 
  summarise(
    CIs = map(data, function(d) bootstrapped_CI(d$RT))
  ) %>% 
  unnest(CIs)

d_sum_stats_blocks_cleaned

```

And a plot of the summary:

```{r}
d_sum_stats_blocks_cleaned %>% 
  ggplot(aes(x = block, y = mean, fill = block)) +
  geom_col() +
  geom_errorbar(aes(ymin = lower, ymax = upper), size = 0.3, width = 0.2 ) +
  ylab("mean reaction time") + xlab("") +
  scale_fill_manual(values = project_colors) +
  theme(legend.position = "none") 
```


<!-- To do: -->
<!-- - give alternative to barplot with individual points -->

### Data analysis 

We are interested in seeing whether the mean RTs are smallest for the 'reaction' task, higher for the 'go/no-go' task, and highest for the 'discrimination' task. We test this with a hierarchical Bayesian regression model, taking participant-level variation of intercets and slopes for factor `block` into account. We make 'go/no-go' the default level of the `block` factor, so that we can directly test our directed hypothesis, using posterior parameter inference.

<!-- TODO: write convenience function for this kind of releveling -->

```{r}
# making 'go/no-go' the reference level
## TODO: write convenience function for this kind of releveling
reflevel <- "goNoGo"
reflevel_index <- which(levels(d_cleaned$block) == reflevel)
contrasts(d_cleaned$block) <- contr.treatment(
  nlevels(d_cleaned$block), 
  reflevel_index
)
colnames(contrasts(d_cleaned$block)) <- str_c("_",levels(d_cleaned$block)[-reflevel_index])
  
regression_model_ME <- brm(
  formula = RT ~ block + (1 + block | submission_id),
  data = d_cleaned
)

## TODO tidy and concise output
regression_model_ME
```

We see that the value zero lies clearly outsie of the 95% credible interval for block 'reaction' and for block 'discrimination'. The deviation from the intercept (zero point) is in the expected direction. We may conclude that, as hypothesized, reaction times in the 'reaction' condition are lowest, higher in the 'go/no-go' condition, and highest in the 'discrimination' condition.

- if applicable: test the research hypotheses 
- use the methods appropriate for the task
  - ideally, always also include a (Bayesian) regression analysis
- interpret the findings

## World Values Survey (wave 6 | 2010-2014)

### Nature, origin and rationale of the data

The [World Values Survey](www.worldvaluessurvey.org) (WVS) aims to study *changing values and their impact on social and political life*. The WVS consists of nationally representative surveys conducted in almost 100 countries which contain almost 90 percent of the world's population, using a common questionnaire. The WVS is the largest non-commercial, cross-national, time series investigation of human beliefs and values. 

It currently includes interviews with almost *400,000 respondents*. Respondents are people in the age 18 and older residing within private households in each country, regardless of their nationality, citizenship or language. 

The main method of data collection in the WVS survey is *face-to-face interview* at respondent's home / place of residence.

#### The questionnaire

The survey was conducted by using a *structured* [questionnaire](../4_WV6_Official_Questionnaire.pdf), consisting of 250 questions (here: variables) ([overview of all variables](../3_WV6_Codebook.pdf)) that describe 10 thematic sub-sections:

1. Perceptions of life,
2. Environment,
3. Work,
4. Family,
5. Politics and Society,
6. Religion and Morale,
7. National Identity,
8. Security,
9. Science, and
10. Socio-demographics.

(The document ["variable description"](../6_variable-description-wvs.csv) contains the assigment of variables to topics.)

#### Theoretical motivation & hypotheses

**Inglehart's Concept of Postmaterialism**
**Schwartz-Value-Scale**


[WORKOUT POSSIBLE HYPOTHESES/ANALYSIS]

### Loading and preprocessing the data

```{r}
d_raw_wvs <- readRDS('../data_sets/data-sets/4_world-values-survey/5_WV6.rds')
head(d_raw_wvs)
```


#### Inglehart's Concept of Postmaterialism

Inglehart (1977) has suggested that value orientations are organized hierarchically on a uni-dimensional continuum from material to postmaterial values [REFERENCE].

The 12-item measure is based on the variables(questions) V60 to V65 in the questionnaire:

People sometimes talk about what the aims of this country should be for the next ten years. On this card are listed some of the goals which different people would give top priority. 

**V60** Would you please say which one of these you, yourself, consider the *most important*? 
**V61** And which would be the *next most important*? 

1. A high level of economic growth (materialist)
2. Making sure this country has strong defense forces (materialist)
3. Seeing that people have more say about how things are done at their jobs and in their communities (post-materialist)
4. Trying to make our cities and countryside more beautiful (post-materialist)

**V62** If you had to choose, which one of the things on this card would you say is *most important*? 
**V63** And which would be the *next most important*? 

1. Maintaining order in the nation (materialist)
2. Giving people more say in important government decisions (post-materialist)
3. Fighting rising prices (materialist)
4. Protecting freedom of speech (post-materialist)

**V64** Here is **another list**. In your opinion, which one of these is *most important*? 
**V65** And what would be the *next most important*? 

1. A stable economy (materialist)
2. Progress toward a less impersonal and more humane society (post-materialist)
3. Progress toward a society in which ideas count more than money (post-materialist)
4. The fight against crime (materialist)

```{r}
# select variables for Postmaterialism-Index
d_post_raw <- select(d_raw_wvs, 
                       "V60":"V65",    # scale items
                       "V2",           # country
                       "V240",         # gender participant
                       "V238"          # working-class participant
                       ) %>% 
  filter(
    V2 %in% c(276,840,392,710)         #276-Germany,840-United States,392-Japan,710-South Africa
  ) %>% 
  as_tibble() %>% 
  print()
```

```{r}
library(naniar)
# change values c(-5,-4,-3,-2,-1) to NAs
d_post_clean <- replace_with_na_all(d_post_raw, condition = ~.x %in% c(-5,-4,-3,-2,-1))

# inspect missing values per country and variable (item)
d_post_clean %>% 
  group_by(V2) %>% 
  summarize(
    V60 = scales::percent(sum(is.na(V60))/length(V60)),
    V61 = scales::percent(sum(is.na(V61))/length(V61)),
    V62 = scales::percent(sum(is.na(V62))/length(V62)),
    V63 = scales::percent(sum(is.na(V63))/length(V63)),
    V64 = scales::percent(sum(is.na(V64))/length(V64)),
    V65 = scales::percent(sum(is.na(V65))/length(V65)),
    gender = scales::percent(sum(is.na(V240))/length(V240)),
    workingclass = scales::percent(sum(is.na(V238))/length(V238))
  )

```

```{r}
# prepare data set
d_post_clean <- 
 mutate(d_post_clean,
    country = factor(d_post_clean$V2, levels = c(276,840,392,710), labels = c("Germany","United States","Japan","South Africa")),
    gender = factor(d_post_clean$V240, levels = c(1,2), labels = c("female", "male")),
    working_class = factor(d_post_clean$V238, levels = c(1,2,3,4,5), labels = c("Upper class","Upper middle class","Lower middle class","Working class","Lower class"))
  ) %>% 
  print()
```


```{r}
# calculating the postmaterialism-index (12-items)
post_index <- mutate(d_post_clean, "post_materialist" =  # postmaterialist: first choice = 2/ second choice = 1
    ifelse(V60 %in% c(1,2),0,2)+
    ifelse(V61 %in% c(1,2),0,1)+
    ifelse(V62 %in% c(1,3),0,2)+
    ifelse(V63 %in% c(1,3),0,1)+
    ifelse(V64 %in% c(1,4),0,2)+
    ifelse(V65 %in% c(1,4),0,1)) %>% 
mutate("post_index" = ifelse(
      post_materialist == 9 , 1,
      ifelse(post_materialist %in% c(8,7), 2,
      ifelse(post_materialist %in% c(6,5), 3,
      ifelse(post_materialist %in% c(4,3), 4,
      ifelse(post_materialist %in% c(2,1), 5,
      6
    )))))) %>%  
mutate("label" = ifelse(
  post_index == 1, "postmaterialist",
  ifelse(post_index == 2, "rather postmaterialist",
  ifelse(post_index == 3, "mixed postmaterialist",
  ifelse(post_index == 4, "mixed materialist",
  ifelse(post_index == 5, "rather materialist",
  "materialist"
)))))) %>% 
  print()
```

```{r}
# distribution of postmaterialists-types in population 
group_by(post_index, `post_index`, label) %>% 
  summarize(count = n(),
            percentage = scales::percent(count/10252)
            ) %>% 
  as_tibble() 
```


```{r}
# plotting
post_index %>% 
ggplot(mapping = aes(x = post_index, y = ..prop.., fill = country)) +
  geom_bar(position = "fill") +
  scale_x_continuous(name = "Postmaterialism-Index", breaks =c(1,2,3,4,5,6), labels = c("postmaterialist","rather post.", "mixed post.", "mixed mat.", "rather mat.", "materialist")) +
  ylab("Percentage") +
  ggtitle("Postmaterialism-Index: Comparision between countries")
  
```

```{r}
# variable: "gender" with 1 - female and 2 - male
post_index %>% 
  filter(country == "Germany") %>%
ggplot(mapping = aes(x = post_index, y = ..prop.., fill = gender)) +
  geom_bar(position = "dodge") +
  scale_x_continuous(name = "Postmaterialism-Index", breaks =c(1,2,3,4,5,6), labels = c("postmaterialist","rather post.", "mixed post.", "mixed mat.", "rather mat.", "materialist")) +
  ylab("Percentage") +
  ggtitle("Postmaterialism-Index: Germany - gender difference")
```


```{r}
# variable: "working-class" with 1 - upper class to 5 - lower class
post_index %>% 
  filter(country == "Germany") %>%
ggplot(mapping = aes(x = post_index, y = ..prop..)) +
  geom_bar(position = "dodge") +
  scale_x_continuous(name = "Postmaterialism-Index", breaks =c(1,2,3,4,5,6), labels = c("post.","2", "3", "4", "5", "mat.")) +
  facet_wrap(~ working_class, nrow = 2) +
  ylab("Percentage") +
  ggtitle("Postmaterialism-Index: Germany - working-class difference")
```

#### Schwartz's Value Scale

Schwartz identifies *ten different values* which can be summarized in two fundamental polarities along which these values cluster: *egoism* versus *altruism* (in Schwartz's terminology: self-enhancement vs. self-transcendence) and *conformism* versus *individualism* (conservation vs. openness to change). The first dimension includes values such as *power* and *achievement* (egoism) and benevolence and universalism (al-truism); stimulation and self-direction (individualism) and security and conformity (con-formism) form the second dimension.

**Schwartz Value Inventory (SVI) items**:

Question: Would you please indicate for each description whether that person is very much like you, like you, somewhat like you, a little like you, not like you, or not at all like you? (6-point Likert-scale)

1. V70: It is important to this person to think up new ideas and be creative; to do things one's own way. (Self-Direction)
2. V71: It is important to this person to be rich; to have a lot of money and expensive things. (Power)
3. V72: Living in secure surroundings is important to this person; to avoid anything that might be dangerous. (Security)
4. V73: It is important to this person to have a good time; to "spoil" oneself. (Hedonism)
5. V74: It is important to this person to do something for the good of society. (Benevolence)
6. V74B: It is important for this people to help the people nearby; to care for their well-being (Benevolence)
7. V75: Being very successful is important to this person; to have people recognize one's achievements. (Achievement)
8. V76: Adventure and taking risks are important to this person; to have an exciting life. (Stimulation)
9. V77: It is important to this person to always behave properly; to avoid doing anything people would say is wrong. (Conformity)
10. V78: Looking after the environment is important to this person; to care for nature and save life resources. (Universalism)
11. V79: Tradition is important to this person; to follow the customs handed down by one's religion or family. (Tradition)

```{r}
# select variables for Schwartz-value-scale
d_value_raw <- select(d_raw_wvs, 
                       "V70":"V79",    # scale items
                       "V2",           # country
                       "V240",         # gender participant
                       "V238"          # working-class participant
                       ) %>% 
  filter(
    V2 %in% c(276,840,392,710)         #276-Germany,840-United States,392-Japan,710-South Africa
  ) %>% 
  as_tibble() %>% 
  print()
```

```{r}
# change values c(-5,-4,-3,-2,-1) to NAs
d_value_clean <- replace_with_na_all(d_value_raw, condition = ~.x %in% c(-5,-4,-3,-2,-1))

# inspect missing values per country and variable (item)
d_value_clean %>% 
  group_by(V2) %>% 
  summarize(
    V70 = scales::percent(sum(is.na(V70))/length(V70)),
    V71 = scales::percent(sum(is.na(V71))/length(V71)),
    V72 = scales::percent(sum(is.na(V72))/length(V72)),
    V73 = scales::percent(sum(is.na(V73))/length(V73)),
    V74 = scales::percent(sum(is.na(V74))/length(V74)),
    V74B = scales::percent(sum(is.na(V74B))/length(V74B)),
    V75 = scales::percent(sum(is.na(V75))/length(V75)),
    V76 = scales::percent(sum(is.na(V76))/length(V76)),
    V77 = scales::percent(sum(is.na(V77))/length(V77)),
    V78 = scales::percent(sum(is.na(V78))/length(V78)),
    V79 = scales::percent(sum(is.na(V79))/length(V79)),
    gender = scales::percent(sum(is.na(V240))/length(V240)),
    workingclass = scales::percent(sum(is.na(V238))/length(V238))
  )
```
```{r}
# use V74 or V74B, depending on NA
d_value_clean <- d_value_clean %>% 
  mutate(V74_new = ifelse(is.na(V74)==FALSE,V74,V74B)) %>% 
  print()
```

```{r}
# prepare tibble 
d_value_clean1 <- 
 transmute(d_value_clean,
    self_direction =  factor(V70, levels = c(1,2,3,4,5,6), labels = c("very much", "like me","somewhat","a little","not like me","not at all")),
    power = factor(V71, levels = c(1,2,3,4,5,6), labels = c("very much", "like me","somewhat","a little","not like me","not at all")),
    security = factor(V72, levels = c(1,2,3,4,5,6), labels = c("very much", "like me","somewhat","a little","not like me","not at all")),
    hedonism = factor(V73, levels = c(1,2,3,4,5,6), labels = c("very much", "like me","somewhat","a little","not like me","not at all")),
    benevolence = factor(V74_new, levels = c(1,2,3,4,5,6), labels = c("very much", "like me","somewhat","a little","not like me","not at all")),
    achievement = factor(V75, levels = c(1,2,3,4,5,6), labels = c("very much", "like me","somewhat","a little","not like me","not at all")),
    stimulation = factor(V76, levels = c(1,2,3,4,5,6), labels = c("very much", "like me","somewhat","a little","not like me","not at all")),
    conformity = factor(V77, levels = c(1,2,3,4,5,6), labels = c("very much", "like me","somewhat","a little","not like me","not at all")),
    universalism = factor(V78, levels = c(1,2,3,4,5,6), labels = c("very much", "like me","somewhat","a little","not like me","not at all")),
    tradition = factor(V79, levels = c(1,2,3,4,5,6), labels = c("very much", "like me","somewhat","a little","not like me","not at all")),
    country = factor(V2, levels = c(276,840,392,710), labels = c("Germany","United States","Japan","South Africa")),
    gender = factor(V240, levels = c(1,2), labels = c("female", "male")),
    working_class = factor(V238, levels = c(1,2,3,4,5), labels = c("Upper class","Upper middle class","Lower middle class","Working class","Lower class"))
  ) %>% 
  print()
```

```{r}
# rearrange tibble for plotting
d_value_clean1 <- d_value_clean1 %>% 
  pivot_longer(
    cols = self_direction:tradition,
    names_to = "value",
    values_to = "rating"
  ) %>% 
  print()
```


```{r}
# flipped bar chart
d_value_clean1 %>% 
  na.omit() %>% 
  filter(country == "Germany") %>%
ggplot(mapping = aes(x = rating, y = (..count..)/sum(..count..), fill = rating)) +
  geom_bar(
    show.legend = FALSE,
    width = 1
  ) +
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL) +
  coord_flip() +
  facet_wrap(~ value, nrow = 2) +
  ggtitle("Value scale: Germany (1-very much to 6-not at all)")
```

```{r}
# Bar chart and Coxcomb chart
d_value_clean1 %>% 
  na.omit() %>% 
  filter(country == "Germany") %>%
ggplot(mapping = aes(x = rating,y = (..count..)/sum(..count..), fill = rating)) +
  geom_bar(
    show.legend = FALSE,
    width = 1
  ) +
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL) +
  coord_polar() +
  facet_wrap(~ value, nrow = 2) +
  ggtitle("Value scale: Germany")
```

```{r}
# bar chart - grouped by countries and values
d_value_clean1 %>%
  na.omit() %>% 
  filter(country %in% c("Germany", "Japan", "South Africa")) %>%
ggplot(mapping = aes(x = rating, y = (..count..)/sum(..count..), fill = country)) +
  geom_bar() +
  facet_wrap(~ value, nrow = 3) +
  scale_x_discrete(labels = c(1,2,3,4,5,6)) +
  ylab("percentage") +
  ggtitle("Value scale: Germany, Japan, South Africa (1-very much to 6-not at all)")
```

