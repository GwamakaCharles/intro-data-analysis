# Parameter estimation {#ch-03-04-parameter-inference}

<hr>

<div style = "float:right; width:40%;">
<img src="visuals/badge-estimation.png" alt="badge estimation">  
</div>  

Based on a model $M$ with parameters $\theta$, parameter estimation addresses the question of which values of $\theta$ are good estimates, given some data $D$. Bayesian and frequentist approaches differ in what they consider a good, or the best estimate. Bayesian approaches take the prior into account, frequentist approaches do not (unsurprisingly). 

Parameter estimation is traditionally governed by two measures: (i) a point-estimate for the best parameter value, and (ii) an interval-estimate for a range of values that are considered "good enough". Table \@ref(tab:ch-03-03-estimation-overview) gives the most salient answers that each approach gives.

```{r ch-03-03-estimation-overview, echo = F}
table_data <- tribble(
  ~estimate, ~Bayesian, ~frequentist,
  "best value", "mean of posterior  posterior", "maximum likelihood estimate",
  "interval range", "credible interval (HDI)", "confidence interval"
)
knitr::kable(
  table_data,
  escape = F,
  caption = "Common methods of obtaining point-valued and interval-range estimates for parameters, given some data, in frequentist and Bayesian approaches.", 
  booktabs = TRUE
)
```


```{block, type='infobox'}
The learning goals for this chapter are:

- understand how Bayes rule applies to parameter estimation
  - role of prior and likelihood
  - understand notion of conjugate prior
- become familiar with and able to compute point-valued estimators
  - frequentist: MLE
  - Bayes: mean of posterior
- become familiar with interval-range estimators
  - frequentist: confidence intervals
  - Bayesian: credible intervals
- become able to compute Bayesian cred.intervals from posterior samples
- understand conceptual differences and relative advantages of both approaches
- 
```


FILL ME: structure / learning goals

- MLE vs posterior
- confidence intervals
- credible intervals
- algorithms for MLE & Bayesian inference
  - OLS vs MLE (regression)
  - conjugate priors
  - variational Bayes
  - mcmc

## Bayes rule of parameter estimation

Fix a Bayesian model $M$ with likelihood $P(D \mid \theta)$ for observed data $D$ and prior over parameters $P_M(\theta)$. We then update our prior beliefs $P(\theta)$ to obtained posterior beliefs by Bayes rule:^[Since parameter estimation is only about one model, it should do not harm to omit the index $M$ in the probability notation. Moreover, since in many contexts the meaning will be  clear enough, we follow common practice and write $P(D \mid \theta)$ as a shortcut for $P(\mathcal{D} = D \mid \Theta = \theta)$. Here $\mathcal{D}$ is the class of all relevant observable data and $\Theta$ is the range of a possibly high-dimensional vector of parameter values. We diverge from common practice of using capital roman letters for random variables and small roman letters for values from these random variables, because parameter vectors are traditionally written as $\theta$ and the small letter $\textrm{d}$ (albeit non-italic) is reserved fro differentials.]

$$P(\theta \mid D) = \frac{P(D \mid \theta) \ P(\theta)}{P(D)}$$

The ingredients of this equation are:

- the **posterior distribution** $P(\theta \mid D)$ specifying our beliefs about how
  likely each value of $\theta$ is given fixed $k$;
- the **likelihood function** $P(D \mid \theta)$ specifying how likely each observation
  of $k$ is for a fixed $\theta$ (here given by the binomial distribution);
- the **prior distribution** $P(\theta)$ specifying our initial (aka.~*a priori*)
  beliefs about how likely each value of $\theta$ might be;
- the **marginal likelihood** $P(D) = \int P(D \mid \theta) \ P(\theta) \ \text{d}\theta$
  specifying how likely an observation of $k$ is under our prior beliefs about $\theta$.

A frequently used shorthand notation for probabilities is this:

$$\underbrace{P(\theta \, | \, D)}_{posterior} \propto \underbrace{P(\theta)}_{prior} \ \underbrace{P(D \, | \, \theta)}_{likelihood}$$

where the "proportional to" sign $\propto$ indicates that the probabilities on the LHS are defined in terms of the quantity on the RHS after normalization. So, if $F \colon x \mapsto \mathbb{R}^+$ is a positive function of non-normalized probabilities, $P(x) \propto F(x)$ is equivalent to $P(x) = \frac{F(x)}{\sum_{x'} F(x')}$.

This notation makes particularly clear that the posterior distribution is a "mix" of prior and likelihood. Let's explore this first, before worrying how to compute posteriors concretely.

### The effects of prior and likelihood on the posterior

We consider the case of flipping a coin with unknown bias $\theta$ a total of $N$ times and observig $k$ heads (= successes). This is modeled with the  **Binomial Model** (see Section \@ref(Chap-03-03-models-examples)), using priors expressed with a [Beta distribution](app-91-distributions-beta), giving us a model specification as: 

$$ 
\begin{aligned}
k & \sim \text{Binomial}(N, \theta) \\ 
\theta & \sim \text{Beta}(\alpha, \beta)
\end{aligned}
$$

<div style = "float:right; width:8%;">
<img src="visuals/skull_king.png" alt="badge-data-wrangling">  
</div>  

To study the impact of the likelihood function, we compare two data sets. The first one is the contrived "24/7" example where $N = 24$ and $k = 7$. The second example uses a much larger naturalistic data set stemming from the [King of France](app-93-data-sets-king-of-france) example, namely $k = 109$ for $N=311$. These numbers are the count of "true" responses for all conditions except for Condition 1, which did not involve a presupposition. 


```{r, echo = F}
data_KoF_cleaned <- read_csv('data_sets/king-of-france_data_cleaned.csv')
```

```{r, eval = F}
data_KoF_cleaned <- read_csv(url('https://raw.githubusercontent.com/michael-franke/intro-data-analysis/master/data_sets/king-of-france_data_raw.csv'))
```

```{r}
data_KoF_cleaned %>% 
  filter(condition != "Condition 1") %>% 
  group_by(response) %>% 
  dplyr::count()
```


We can plot the likelihood function for both data sets like so:

```{r, echo = F, fig.cap = "Likelihood for the two examples of bimoial data."}
lh_tibble <- 
  tibble(
  theta = seq(0,1, length.out = 401),
  `24/5` = dbinom(7, 24, theta),
  `King of France` = dbinom(109, 311, theta)
) 

lh_tibble %>% 
  pivot_longer(
    cols = -1,
    names_to = "example",
    values_to = "likelihood"
  ) %>% 
  ggplot(aes(x = theta, y = likelihood, color = example)) +
  geom_line(size = 3) +
  facet_wrap(.~example, nrow = 2, scales = "free") +
  guides(color = F) +
    labs(
    x = latex2exp::TeX("Coin bias $\\theta$"),
    y = latex2exp::TeX("Prior probability $P_{M_i}(\\theta)$"),
    title = latex2exp::TeX("Binomial likelihood function for different data sets.")
  )
```


Picking up the example from Section \@ref(Chap-03-02-models-priors), we willl consider the four types of priors show below:

```{r ch-03-03-estimation-types-of-priors, echo = F, fig.cap = "Examples of different kinds of Bayesian priors for the Binomial Model."}
prior_tibble <- tibble(
  theta = seq(0, 1, length.out = 401),
  objective = dbeta(theta, 0.5, 0.5),
  uninformative = dbeta(theta, 1, 1),
  `weakly informative` = dbeta(theta, 5, 2),
  `strongly informative` = dbeta(theta, 50, 20)
) 

prior_tibble %>% 
    pivot_longer(
    cols = -1,
    names_to = "prior_type",
    values_to = "prior"
  ) %>% 
  ggplot(aes(x = theta, y = prior)) +
  geom_line(size = 2) +
  facet_wrap(~ prior_type, ncol = 2, scales = "free") +
  labs(
    x = latex2exp::TeX("Coin bias $\\theta$"),
    y = latex2exp::TeX("Prior probability $P_{M_i}(\\theta)$"),
    title = latex2exp::TeX("Different kinds of priors over bias $\\theta$ (Binomial Model family).")
  ) 

```

Combining the four different priors and the two different data sets, we see that the posterior is indeed a mix of prior and likelihood. In particular we see that the strong informative prior has only little effect if there are many data points (the KoF data).

```{r ch-03-03-estimation-posteriors-comparison, echo = F, fig.cap = "Posterior beliefs over coin biases under different priors and different data sets."}
full_join(lh_tibble, prior_tibble, by = 'theta') %>% 
    pivot_longer(
    cols = 2:3,
    names_to = "example",
    values_to = "likelihood"
  ) %>% 
  pivot_longer(
    cols = 2:5,
    names_to = "prior_type",
    values_to = "prior"
  ) %>% 
  mutate(
    posterior = likelihood * prior
  ) %>%  
  filter( posterior != Inf) %>% 
  group_by( example, prior_type) %>% 
  mutate(
    posterior = posterior/sum(posterior)
  ) %>% 
  ggplot(aes(x = theta, y = posterior, color = example)) +
  geom_line(size = 2) +
  guides(color = F) +
  facet_grid(example ~ prior_type, scales = "free") +
  labs(
    x = latex2exp::TeX("Coin bias $\\theta$"),
    y = latex2exp::TeX("Posterior probability $P_{M_i}(\\theta \\, | \\, D)$"),
    title = latex2exp::TeX("Posterior beliefs in $\\theta$ for different priors and data.")
  ) 
```

### Posterior means and credible intervals

Let's consider the "24/7" example with a flat prior again, concisely repeated in Figure \@ref(fig:ch-03-03-estimation-24-7-overview).

```{r ch-03-03-estimation-24-7-overview, echo = F, fig.cap = "Prior (uninformative), likelihood and posterior for the 24/7 example.", fig.height = 12}

prior_plot_24_7 <- 
  tibble(
    theta = seq(0.01,1, by = 0.01),
    prior = dbeta(seq(0.01,1, by = 0.01), 1, 1 )
  ) %>% 
  ggplot(aes(x = theta, y = prior)) + 
  xlim(0,1) + 
  geom_line(color = project_colors[1], size = 2) + 
  labs(
    x = latex2exp::TeX("Coin bias $\\theta$"),
    y = latex2exp::TeX("Prior probability $P_{M}(\\theta)$"),
    title = "Prior"
  )

lh_plot_24_7 <- 
  tibble(
    theta = seq(0.01,1, by = 0.01),
    lh = dbinom(x = 7, size = 24, prob = theta)
  ) %>% 
  ggplot(aes(x = theta, y = lh)) + 
  xlim(0,1) + 
  geom_line(color = project_colors[2], size = 2) + 
  labs(
    x = latex2exp::TeX("Coin bias $\\theta$"),
    y = latex2exp::TeX("Likelihood $P_{M}(D \\, | \\, \\theta)$"),
    title = "Likelihood"
  )

hdi = HDInterval::hdi(qbeta , shape1 = 8 , shape2 = 18 )
hdiData <- tibble(
  theta = rep(hdi, each = 2),
  post = c(0,dbeta(hdi, 8, 18), 0)
)
expData <- tibble(
  theta = c(8/26,8/26),
  post = c(0,dbeta(8/26, 8, 18 ))
)

posterior_plot_24_7 <- 
  tibble(
    theta = seq(0.01,1, by = 0.01),
    posterior = dbeta(seq(0.01,1, by = 0.01), 8, 18 )
  ) %>% 
  ggplot(aes(x = theta, y = posterior)) + 
  xlim(0,1) + 
  labs(
    x = latex2exp::TeX("Coin bias $\\theta$"),
    y = latex2exp::TeX("Posterior probability $P_{M}(\\theta \\, | \\, D)$"),
    title = "Posterior"
  ) +
  geom_line(data = hdiData, aes(x = theta, y = post), color = "firebrick", size = 1.5) +
  geom_label(x = 0.7, y = 0.5, label = "Cred.Int.: 0.14 - 0.48", color = "firebrick", size = 5) +
  geom_line(data = expData, aes(x = theta, y = post), color = "darkblue", size = 1.5) +
  geom_label(x = 0.52, y = dbeta(8/26, 8, 18 ), label = "expectation: 0.308", color = "darkblue", size = 5) +
  geom_line(color = "black", size = 2)

cowplot::plot_grid(
  prior_plot_24_7,
  lh_plot_24_7,
  posterior_plot_24_7,
  ncol = 1
)
```


The posterior probability distribution in Figure \@ref(fig:ch-03-03-estimation-24-7-overview) contains rich information. It specifies how likely each value of $\theta$ is, obtained by updating the original prior beliefs with the observed data. Such rich information is difficult to process and communicate in language. It is therefore convenient to have conventional means of summarizing the rich information carried in a probability distribution like in Figure \@ref(fig:ch-03-03-estimation-24-7-overview). Customarily, we summarize in terms of a point-estimate and/or an interval estimate. The *point estimate* gives information about a "best value", i.e., a salient point, such as the expectation (in Bayesian approaches) or the most likely value (in frequentist approaches (see below)). The *interval estimate* gives, usually, an indication of how closely other *good values* are scattered around the *best value*.

A common Bayesian point estimate of coin bias parameter $\theta$ is **the mean of the posterior distribution**. It gives the value of $\theta$ which we would expect to see, when basing out expectations on the posterior distribution:
$$
\begin{aligned}
  \mathbb{E}_{P(\theta \mid D)} = \int \theta \ P(\theta \mid D) \ \text{d}\theta
\end{aligned}
$$
If we start with flat beliefs, the expected value of $\theta$ after $k$ successes in $N$ flips
can be calculated rather easily as $\frac{k+1}{n+2}$.^[This is also known as *Laplace's
  rule*, or the *rule of succession*.] For our example case, we calculate the expected value of
$\theta$ as $\frac{8}{26} \approx 0.308$ (see also Figure \@ref(fig:ch-03-03-estimation-24-7-overview)).

```{block2, type="infobox"}
**Remark: Maximum _a posteriori_** Another salient point-estimate to summarize a Bayesian posterior distribution is the MAP (maximum _a posteriori_). The map is the parameter value (tuple) that maximizes the posterior distribution:
  
$$ \text{MAP}(P(\theta \mid D)) =  \arg \max_\theta P(\theta \mid D) $$
  
While the mean of the posterior is "holisitic" in the sense that it depends on the whole distribution, the MAP does not. The mean is therefore more faithful to the Bayesian ideal of taking the full posterior distribution into account. Moreover, depending on how Bayesian posteriors are computed / approximated, the estimation of a mean can be more reliable than that of a MAP.
```

A common Bayesian interval estimate of coin bias parameter $\theta$ is a **credible interval**.^[Also frequently called "highest-density intervals", even when we are dealing not with density but probability mass.] An interval $[l;u]$ is a $\gamma\%$ credible interval for a random variable $X$ if two conditions hold, namely
$$
\begin{aligned}
  P(l \le X \le u) = \frac{\gamma}{100}
\end{aligned}
$$
and, secondly, for every $x \in[l;u]$ and $x' \not \in[l;u]$ we have $P(X=x) > P(X = x')$. Intuitively, a $95\%$ credible interval gives the range of values in which we believe with
relatively high certainty that the true value resides. Figure \@ref(fig:ch-03-03-estimation-24-7-overview) indicates the $95\%$ credible interval, based on the posterior distribution $P(\theta \mid D)$ of $\theta$, for the 24/7 example.^[Not all random variables have a credible interval for a given $\gamma$, according to this definition. A bimodal distribution might not, for example. A bi-modal distribution has two regions of high probability. We can therefore generalize the concept to a finite set of disjoint convex *credible regions*, all of which have the second property of the definition above and all of which conjointly are realized with $\gamma\%$ probability. Unfortunately, common parlor uses the term "credible interval" to refer to credible regions as well. The same disaster occurs with alternative terms, such as "$\gamma\%$ highest-density intervals", which also often refers to what should better be called "highest-density regions".]

### Computing Bayesian posteriors with conjugate priors

Bayesian posterior distributions can be hard to compute. Usually, the prior $P(\theta)$ is easy to compute (otherwise we might choose a different one for practicality). Usually, the likelihood function $P(D \mid \theta)$ is also fast to compute. Everything seems innocuous when we just write:

$$\underbrace{P(\theta \, | \, D)}_{posterior} \propto \underbrace{P(\theta)}_{prior} \ \underbrace{P(D \, | \, \theta)}_{likelihood}$$

But the real pain is the normalizing constant, i.e., the marginalize likelihood a.k.a. the "integral of doom", which to compute can be intractable, especially if the parameter space is large and not well-behaved:

$$P(D) = \int P(D \mid \theta) \ P(\theta) \ \text{d}\theta$$

Section \@ref(Ch-03-03-estimation-algorithms) will therefore enlarge on methods to compute or approximate the posterior distribution efficiently.

Foortunately, computing Bayesian posterior distributions need not always be intractable. If the prior and the likelihood function cooperate, so to speak, the computation of the posterior can be as simple as sleep. The nature of the data often prescribes which likelihood function is plausible. But we have more wiggle room in the choice of the priors. If prior $P(\theta)$ and posterior $P(\theta \, | \, D)$ are of the same family, i.e., if they are the same kind of distribution albeit possibly with different parameterizations, we say that they **conjugate**. In that case, the prior $P(\theta)$ is called **conjugate prior** for the likelihood function $P(D \, | \, \theta)$. from which the posterior $P(\theta \, | \, D)$ is derived.

```{theorem, "Conjugacy-beta-binomial"}
The Beta distribution is the conjugate prior of binomial likelihood. For $\theta \sim \text{Beta}(a,b)$ as prior and data $k$ and $N$, the posterior is $\theta \sim \text{Beta}(a+k, b+ N-k)$.
```


```{proof}
By construction, the posterior is:
$$P(\theta \mid \langle{k, n \rangle}) \propto \text{Binomial}(k ; n, \theta) \ \text{Beta}(\theta \, | \, a, b) $$


We extend the RHS by definitions:

$$
\begin{aligned} 
\text{Binomial}(k ; n, \theta) \ \text{Beta}(\theta \, | \, a, b) & = \theta^{k} \, (1-\theta)^{n-k} \, \theta^{a-1} \, (1-\theta)^{b-1}  \\  & = \theta^{k + a - 1} \, (1-\theta)^{n-k +b -1}
\end{aligned}
$$

This latter expression is the non-normalized Beta-distribution for parameters $a + k$ and $b + N - k$, so that we conclude with what was to be shown:

$$
\begin{aligned} 
P(\theta \mid \tuple{k, n}) & = \text{Beta}(\theta \, | \, a + k, b+ N-k)
\end{aligned}
$$
```

&nbsp;

### Sequential updating

Ancient wisdom has coined the widely popular proverb: "Today's posterior is tomorrow's prior." Suppose we collected data from an experiment, like $k = 7$ in $N = 24$. Using uninformative priors at the outset, our posterior belief after the experiment is $\theta \sim \text{Beta}(8,18)$. But now consider what happened at half-time. After half the experiment, we had $k = 2$ and $N = 12$, so our beliefs followed $\theta \sim \text{Beta}(3, 10)$ at this moment in time. But using these beliefs as priors, and observing the rest of the data would consequently result in updating the prior $\theta \sim \text{Beta}(3, 10)$ with another set of observations $k = 5$ and $N = 12$, giving us the same posterior belief as what we would have gotten if we updated in one swoop. Figure \@ref(fig:ch-03-03-estimation-sequential-updates) shows steps through belief space, starting uninformed, and observing one piece of data at a time (going right for each outcome of heads, down for each outcome of tails).

```{r ch-03-03-estimation-sequential-updates, echo = F, fig.cap = "Beta distributions for different parameters. Starting from an uninformative prior (top left), we arrive at the posterior distribution in the bottom left, in any sequence of sequentially updating with the data.", fig.height = 8}
expand_grid(
    theta = seq(0,1,length.out = 41),
    a = 1:7,
    b = 1:4
  ) %>% 
  as_tibble() %>% 
  mutate(
    probability = dbeta(theta, a, b),
    parameters = str_c("a=", a, " b=", b)
  ) %>% 
  ggplot(aes(x = theta, y = probability)) +
  geom_line() +
  facet_wrap(~parameters, scales = "free", nrow = 7) +
  theme(
    axis.title.x=element_blank(),
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank(),
    axis.title.y=element_blank(),
    axis.text.y=element_blank(),
    axis.ticks.y=element_blank()
  )
```


This sequential updating is not a peculiarity of the Beta-Binomial case or of conjugacy. It holds in general for Bayesian inference. Sequential updating is a very intuitive property, but it is not shared by all other forms of inference from data. That Bayesian inference is sequential and commutative follows from commutativity of multiplication of likelihoods (and the definition of Bayes rule).

```{theorem, "Sequential-update"}
Bayesian posterior inference is sequential and commutative in the sense that for a data set $D$ which is comprised of two mutually exclusive subsets $D_1$ and D_2$ such that $D_1 \cup D_2 = D$,we have:

$$ P(\theta \mid D ) \propto P(\theta \mid D_1) \ P(D_2 \mid \theta) $$  

```

```{proof}
$$
\begin{aligned}
P(\theta \mid D) & = \frac{P(\theta) \ P(D \mid \theta)}{ \int P(\theta') \ P(D \mid \theta') \text{d}\theta'} \\
& = \frac{P(\theta) \ P(D_1 \mid \theta) \ P(D_2 \mid \theta)}{ \int P(\theta') \ P(D_1 \mid \theta') \ P(D_2 \mid \theta') \text{d}\theta'} & \text{[from multiplicativity of likelihood]} \\
& = \frac{P(\theta) \ P(D_1 \mid \theta) \ P(D_2 \mid \theta)}{ \frac{k}{k} \int P(\theta') \ P(D_1 \mid \theta') \ P(D_2 \mid \theta') \text{d}\theta'} & \text{[for random positive k]} \\
& = \frac{\frac{P(\theta) \ P(D_1 \mid \theta)}{k} \ P(D_2 \mid \theta)}{\int \frac{P(\theta') \ P(D_1 \mid \theta')}{k} \ P(D_2 \mid \theta') \text{d}\theta'} & \text{[rules of integration & basic calculus]} \\
& = \frac{P(\theta \mid D_1) \ P(D_2 \mid \theta)}{\int P(\theta' \mid D_1) \ P(D_2 \mid \theta') \text{d}\theta'} & \text{[Bayes rule with } k = \int P(\theta) P(D_1 \mid \theta) \text{d}\theta ]\\
\end{aligned}
$$
```




&nbsp;

## Algorithms for parameter estimation {#Ch-03-03-estimation-algorithms}

- MLE
- MCMC, variational etc.