# Parameter inference {#ch-03-04-parameter-inference}

- MLE vs posterior
- confidence intervals
- credible intervals
- algorithms for MLE & Bayesian inference
  - OLS vs MLE (regression)
  - conjugate priors
  - variational Bayes
  - mcmc



## Prior & Posterior

The clearest difference between frequentist and Bayesian methods is the incorporation of *prior information* in the Bayesian framework. The posterior distribution results by combining the likelihood with the prior information:

$$Posterior \propto Prior \cdot Likelihood,$$
or
$$P(B|A) \propto P(B) \cdot P(A|B).$$

When an uninformative (e.g. uniform) prior $P(B)$ is used then the posterior $P(B|A)$ is completely dependent on the data (likelihood) $P(A|B)$. The influence of the prior on the posterior depends on their relative weighting. Remember, the posterior is the conditional distribution of the parameter given the data. The mathematical procedure behind it is depicted by *Bayes' Theorem*.

Consider for example a beta distribution with the parameters a and b: $\theta \sim Beta(a,b)$ for expressing prior knowledge. Assume further that the observed data are derived from a coin flip experiment, thus, the likelihood function is a bernoulli likelihood. The parameters of a beta distribution can be interpreted as $a=$no. of heads and $b = $no. of tails, or in other words: $n=a+b$. Consequently, Beta(1,1) has a lower weight and thus influence on the posterior than Beta(5,5).

```{r ch-03-03-prior-distributions, echo = FALSE, fig.cap = "Uninformative vs. informative beta prior distribution."}
tibble(
  theta = theta,
  y1 = dbeta(theta, shape1 = 1, shape2 = 1),
  y2 = dbeta(theta, shape1 = 10, shape2 = 10)
) %>%
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>%
  mutate(
    parameter = case_when(parameter == "y1" ~ "(1,1)",
                          parameter == "y2" ~ "(10,10)")
  ) %>%

ggplot(aes(theta, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X ~ Beta distribution", y = "Density", x = expression(theta))
```

```{r ch-03-03-posterior-distributions, echo = FALSE, fig.cap = "Beta posterior distributions for different beta prior distributions and bernoulli likelihood."}

tibble(
  theta = theta,
  y_prior1 = dbeta(theta, shape1 = 1, shape2 = 1),
  y_prior2 = dbeta(theta, shape1 = 10, shape2 = 10),
  likelihood = likelihood_bern$likelihood_norm,
  y_likelihood = likelihood_bern$likelihood_norm, # only for legend
  y_posterior1 = dbeta(theta, shape1 = likelihood_bern$k+1, shape2 = likelihood_bern$n-likelihood_bern$k+1),
  y_posterior2 = dbeta(theta, shape1 = likelihood_bern$k+10, shape2 = likelihood_bern$n-likelihood_bern$k+10)
) %>%
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>%
  mutate(
    parameter = case_when(parameter == "y_prior1" ~ "Beta(1,1) - prior1",
                          parameter == "y_prior2" ~ "Beta(10,10) - prior2",
                          parameter == "y_likelihood" ~ "Bernoulli(18) - likelihood",
                          parameter == "y_posterior1" ~ "Beta(19,13) - posterior1",
                          parameter == "y_posterior2" ~ "Beta(28,22) - posterior2")
  ) %>%

ggplot() +
  geom_line(aes(theta, y, color = parameter), size = 2) +
  geom_line(aes(theta, likelihood), linetype = "dashed", size = 2, color = project_colors[2]) +
  labs(x = expression(theta), y = "Density")

```

A lot of effort has been done in the area of "Objective Bayesian data analysis" in order to develop "uninformative prior distributions", because a lot of people feel uncomfortable using informative priors --- seeing them as biased or unscientific. On the contrary, most people interpret results based on their prior experiences and in this light, prior information is just a way to quantify this [@dobson2008]. It is "just" important that the definition of the prior distributions make sense (at every stage) in the model.
