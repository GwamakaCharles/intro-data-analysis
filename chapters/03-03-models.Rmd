# Models

## Likelihood, Prior, \& Posterior

Before the topic of is introduced, some conceptual notions regarding likelihood, prior and posterior are necessary.

### Probability density function vs. Likelihood function

As already introduced, for a coin flip the probability of each outcome can be modeled with the *Bernoulli distribution*, because two discrete outcomes (head or tail) and a constant probability $\theta$ exist:
$$p([X=x]|\theta)=\theta^{[x]}(1-\theta)^{(1-[x])}$$

where

- $\theta$ is the probability of coin-flip-outcome "head", and
- the bracket [ ] indicates that the particular parameter is treated as unknown.

With the formula above the probability of $\theta$ is treated as "known". Accordingly, the distribution of possible outcomes can be derived.

But often the contrary is the case, that is one is interested in the value of $\theta$ by a given data set. Then $\theta$ is unknown and the data are observed. Treating $\theta$ as parameter instead of $x$ leads to the *likelihood function* --- a mathematical formula that specifies the plausibility of the data. It states the probability of any possible observation:
$$p(X=x|[\theta])=[\theta]^x(1-[\theta])^{(1-x)}$$
```{r ch-03-03-likelihood-distribution, echo = FALSE, fig.cap= "Bernoulli likelihood for simulated observed coin-flip data."}
# x-axis
stepsize <- 100
theta = seq(from = 0, to = 1, by = 1/stepsize)

# beroulli likelihood
likelihood_bern <- tibble(
  theta = theta,
  n = 30,                                    # number of observations
  k = 18,                                    # number of heads
  likelihood = (theta^k)*(1-theta)^(n-k)     # bernoulli likelihood
) %>%
mutate(likelihood_norm = (likelihood / sum(likelihood)*stepsize)
           )
# plot likelihood
ggplot(likelihood_bern, aes(x = theta, y = likelihood_norm)) +
  geom_line(size = 2) +
  labs(x = expression(theta), y = "Likelihood")
```

Please be aware that through exchanging the roles of $x$ and $\theta$ in the second equation (likelihood function) this function is no longer a probability distribution and thus does not integrate to 1.

### Prior & Posterior

The clearest difference between frequentist and Bayesian methods is the incorporation of *prior information* in the Bayesian framework. The posterior distribution results by combining the likelihood with the prior information:

$$Posterior \propto Prior \cdot Likelihood,$$
or
$$P(B|A) \propto P(B) \cdot P(A|B).$$

When an uninformative (e.g. uniform) prior $P(B)$ is used then the posterior $P(B|A)$ is completely dependent on the data (likelihood) $P(A|B)$. The influence of the prior on the posterior depends on their relative weighting. Remember, the posterior is the conditional distribution of the parameter given the data. The mathematical procedure behind it is depicted by *Bayes' Theorem*.

Consider for example a beta distribution with the parameters a and b: $\theta \sim Beta(a,b)$ for expressing prior knowledge. Assume further that the observed data are derived from a coin flip experiment, thus, the likelihood function is a bernoulli likelihood. The parameters of a beta distribution can be interpreted as $a=$no. of heads and $b = $no. of tails, or in other words: $n=a+b$. Consequently, Beta(1,1) has a lower weight and thus influence on the posterior than Beta(5,5).

```{r ch-03-03-prior-distributions, echo = FALSE, fig.cap = "Uninformative vs. informative beta prior distribution."}
tibble(
  theta = theta,
  y1 = dbeta(theta, shape1 = 1, shape2 = 1),
  y2 = dbeta(theta, shape1 = 10, shape2 = 10)
) %>%
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>%
  mutate(
    parameter = case_when(parameter == "y1" ~ "(1,1)",
                          parameter == "y2" ~ "(10,10)")
  ) %>%

ggplot(aes(theta, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X ~ Beta distribution", y = "Density", x = expression(theta))
```

```{r ch-03-03-posterior-distributions, echo = FALSE, fig.cap = "Beta posterior distributions for different beta prior distributions and bernoulli likelihood."}

tibble(
  theta = theta,
  y_prior1 = dbeta(theta, shape1 = 1, shape2 = 1),
  y_prior2 = dbeta(theta, shape1 = 10, shape2 = 10),
  likelihood = likelihood_bern$likelihood_norm,
  y_likelihood = likelihood_bern$likelihood_norm, # only for legend
  y_posterior1 = dbeta(theta, shape1 = likelihood_bern$k+1, shape2 = likelihood_bern$n-likelihood_bern$k+1),
  y_posterior2 = dbeta(theta, shape1 = likelihood_bern$k+10, shape2 = likelihood_bern$n-likelihood_bern$k+10)
) %>%
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>%
  mutate(
    parameter = case_when(parameter == "y_prior1" ~ "Beta(1,1) - prior1",
                          parameter == "y_prior2" ~ "Beta(10,10) - prior2",
                          parameter == "y_likelihood" ~ "Bernoulli(18) - likelihood",
                          parameter == "y_posterior1" ~ "Beta(19,13) - posterior1",
                          parameter == "y_posterior2" ~ "Beta(28,22) - posterior2")
  ) %>%

ggplot() +
  geom_line(aes(theta, y, color = parameter), size = 2) +
  geom_line(aes(theta, likelihood), linetype = "dashed", size = 2, color = project_colors[2]) +
  labs(x = expression(theta), y = "Density")

```

A lot of effort has been done in the area of "Objective Bayesian data analysis" in order to develop "uninformative prior distributions", because a lot of people feel uncomfortable using informative priors --- seeing them as biased or unscientific. On the contrary, most people interpret results based on their prior experiences and in this light, prior information is just a way to quantify this [@dobson2008]. It is "just" important that the definition of the prior distributions make sense (at every stage) in the model.

#### Exursos: Prior predictive distribution

So far we have seen that the \emph{prior distribution} over parameters captures the initial assumptions or state of knowledge about the psychological variables they represent [@leeWagen2014], in the above example this variable is $\theta$.

Considering these initial assumptions in terms of prior distributions allow to make predictions about what data we would expect given the model and current state of knowledge. This distribution is called *prior predictive distribution*. It is a distribution over data, and gives the relative probability of different observable outcomes before any data have been seen [@leeWagen2014].

For the coin flip model we consider a flat prior distribution: Beta(1,1). The prior predictive distribution would therefore look like:

```{r ch-03-03-prior-predictive-distribution, echo = FALSE, fig.cap = "Prior predictive distribution with Beta(1,1) prior for coin flip model"}

tibble(
  n = 1e5,
  p = rbeta(n = n, shape1 = 1, shape2 = 1),
  x = rbern(n = n, prob = p)
) %>%
  ggplot() +
  geom_histogram(aes(x, y = (..count..)/sum(..count..))) +
  scale_x_continuous(name = "number of heads", breaks = c(0.0,1.0), labels = c("0","1"), limits = c(-0.5,1.5)) +
  scale_y_continuous(name = "Probability", limits = c(0,1))
```

## Modeling

### Introductory example

As introductory example we considered a coin flip experiment and asked if a particular coin is *biased*. In order to investigate this question a coin is flipped $x$ times (= trials) and the number of success (i.e. numbers of "head") $k$ is recorded. This is repeated $n$ times (=observations).

```{r ch-03-03-coin-flip data set, echo = FALSE, fig.cap = "Simulating a coin flip experiment with n observations, x trials and k number of heads."}
#simulate coin flip data set
sample.space <- c(0, 1)
theta <- 0.5              # "true" probability of a success (here: head)
X <- 30                   # number of trials in the experiment
n <- 10                   # number of observations
k <- 0                    # number of heads [initialization]

## repeat experiment N-times
for (i in 1: n) {
  k[i] <- sum(sample(sample.space, size = X, replace = TRUE,
                     prob = c(theta, 1 - theta)))
}

## show results in a tibble
coin.flip <- tibble("n" = seq(from=1, to=n, by=1),
                    "k" = k,
                    "x" = X
) %>%
  print()
```

The above table shows the observed outcome, but how the underlying probability of coming up *heads* can be derived from that data set?

### Steps of Data Analysis

The approach described here is based on [@mcelreath2015; @kruschke2015]. Although the approach is introduced in a Bayesian context, it can be used as a general guideline (with some caveats):

* Identify the relevant variables according to the hypothesis (Measurement scales, predicted vs. predictor variables).
* Define the descriptive model for the relevant variables.
  + likelihood distribution (distribution of each outcome variable that defines the plausibility of individual observations)
  + parameters (define and name all parameters of the model in order to relate the likelihood to the predictor variable(s))
* Bayesian context: Specify prior distribution(s).

Further steps that will be subject of later chapters:

* Inference and interpretation of the results.
* Model checking (Is the defined model adequate?)

*In the following we are interested in the question if a certain coin is biased.*

**First step** is to *identify the relevant variables*. For the coin flip experiment a coin is flipped $n$ times, whereby each observation consists of $x$ trials. Imagine for example 10 people flip a coin 30 times, then $n=10$ and $x=30$. The variable *coin flip* $Y$ is dichotomous with the possible outcomes "head" and "tail". For each observation the outcome is recorded: "0" for coming up tail and "1" for coming up head. The data are summarized for each observation. The variable $k$ indicates the number of heads coming up in $x$ trials.

In **the second step** a *descriptive model for the identified variables* has to be defined. An underlying probability $\theta$ is assumed, indicating the probability of heads coming up $p(y=1)$. The probability that the outcome is head, given a value of parameter $\theta$, is the value of $\theta$ [@kruschke2015, p.109]. Formally, this can be written as
$$p(y=1|\theta)=\theta$$
As only two outcomes of $Y$ exists, the probability that the outcome is tail is the complementary probability $1-\theta$. Both probabilities can be combined in one probability expression:
$$Pr(Y|n,\theta)=\frac{n!}{y!(n-y)!}\theta^{y}(1-\theta)^{n-y}.$$
This probability distribution is called the **Binomial distribution**. The fracture at the beginning indicates how many ordered sequences of $n$ outcomes a count $y$ have.

```{r ch-03-03-binomial-distribution-coin-flip-example, echo = FALSE, fig.cap = "Binomial-distribution for different coin biases"}
# how many trials
trials = 30

rv_binom <- tibble(
  x = seq(0, trials),
  y1 = dbinom(x, size = trials, p = 0.2),
  y2 = dbinom(x, size = trials, p = 0.5),
  y3 = dbinom(x, size = trials, p = 0.8)
) %>%
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>%
  mutate(
    parameter = case_when(parameter == "y1" ~ "(n,0.2)",
                          parameter == "y2" ~ "(n,0.5)",
                          parameter == "y3" ~ "(n,0.8)")
  )

# dist plot
ggplot(rv_binom, aes(x, y, fill = parameter)) +
  geom_col(position = "identity", alpha = 0.8) +
  labs(fill = "X ~ Binomial", y = "Probability")

```

When the coin is flipped only once, then the probability can be written as:
$$Pr(Y|\theta)=\theta^{y}(1-\theta)^{1-y}.$$
This special variant of the Binomial distribution is the so-called **Bernoulli distribution**. To see the connection to the first considerations: When the outcome "head" is observed the equation reduces to $Pr(y=1|\theta)=\theta$ and when the outcome "tail" is observed the equation results in $Pr(y=0|\theta)=(1-\theta).$

Accordingly, for the introductory example it can be noted that the coin flip variable $Y$ *is distributed as* Binomial distribution. (Note: For Bayes' rule the *likelihood function* is needed. Remember, the likelihood function treats $\theta$ as unknow and the data as known. This role of parameter is exchanged in a probability distribution.)

```{r ch-03-03-binomial-likelihood-coin-flip-example, echo = FALSE, fig.cap = "Binomial likelihoods for different observed coin flip outcomes."}

binomial.likelihood <- function(n, k, theta){theta^k*(1-theta)^(n-k)}

tibble(
  n = 10,
  theta = seq(from=0, to=1, by=0.01),
  y_1 = binomial.likelihood(n,2,theta),
  y_2 = binomial.likelihood(n,5,theta),
  y_3 = binomial.likelihood(n,8,theta)
) %>%
pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>%
  mutate(
    parameter = case_when(parameter == "y_1" ~ "(n=10,x=20,p)",
                          parameter == "y_2" ~ "(n=10,x=50,p)",
                          parameter == "y_3" ~ "(n=10,x=80,p)")
  ) %>%
ggplot(aes(theta, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X ~ Binomial", y = "Likelihood", x = expression(theta))
```

**The third step** is solely a *Bayesian idea*, that is the *incorporation of prior knowledge*. What do we believe about the coin bias $\theta$ before seeing the data? Assuming that no expectation about $\theta$ exists a priori, indicating that all values of $\theta$ between 0 and 1 are equally probable. This can be modeled by a uniform distribution or as already visualized as Beta distribution with parameters a=1 and b=1 (see following figure).

```{r ch-03-03-prior-distribution-coin-flip-example, echo = FALSE, fig.cap = "Using uninformative prior distributions: The Uniform(0,1) and Beta(1,1) prior."}

tibble(
  x = seq(from = -0.01, to = 1.01, by = 0.01),
  y_1 = dunif(x, min = 0, max = 1),
  y_2 = dbeta(x, shape1 = 1, shape2 = 1), # only for legend
  beta = dbeta(x, shape1 = 1, shape2 = 1)
) %>%
  pivot_longer(cols = starts_with("y"),
               names_to  = "prior",
               values_to = "y") %>%
  mutate(
    prior = case_when(prior == "y_1" ~ "Uniform(0,1)",
                      prior == "y_2" ~ "Beta(1,1)")
  ) %>%
ggplot() +
  geom_line(aes(x, y, color = prior), size = 2) +
  geom_line(aes(x, beta), color = project_colors[2], size = 2, linetype = "dashed") +
  labs(y = "Density", x = expression(theta)) +
  ylim(0,1.5)
```

So far, the coin flip model is defined conceptually. In the following some notational considerations have to be made.

### Notation

#### Textual notation

In the textual notation, first the prior assumptions (if the Bayesian perspective is taken) are described. For the coin flip example this is:
$$\theta \sim Beta(1,1).$$
The symbol "$\sim$" means "is distributed as", thus, the above equation says before seeing the data all possible values of $\theta$ between 0 and 1 are assumed to be equally likely.

Subsequently, the descriptive model for the data has to be defined. As already described in the section above, it is assumed that the observed data (upcoming of heads $k$) are distributed as Binomial distribution with given $n$ (number of observations) and unknown $\theta$. This relation is denoted symbolically as
$$k\sim Binomial(\theta|n).$$
To summarize the current model (whereby the prior knowledge is only considered from a Bayesian perspective):
$$\theta \sim Beta(1,1),$$
$$k\sim Binomial(\theta|n).$$

#### Graphical notation

When models get very complex and incorporate many parameters it can be difficult to tease out all relations between the model components. In such a situation a graphical notation of a model might be helpful. In the following the convention described in Wagenmakers and Lee's *Bayesian Cognitive Modelling* (2014) is used: The graph structure is used to indicate dependencies between the variables, with children depending on their parents [@leeWagen2014]. General conventions:

* Nodes - problem relevant variables,
* shaded nodes - observed variables,
* unshaded nodes - unobserved variables,
* circular nodes - continuous variables,
* square nodes - discrete variables,
* single line - stochastic dependency, and
* double line - deterministic dependency.

For the introductory example this indicates:

* relevant variables: number of trials ($n$), number of success ($k$) and probability for a success ($\theta$),
* observed variables: $n$ and $k$,
* unobserved variables: $\theta$,
* continuous variable: $\theta$,
* discrete variables: $n$ and $k$.

In the next step the dependencies have to be determined:

The number of success $k$ depends on the probability of a success $\theta$ as well as on the number of trials $n$.

Finally, the graphical structure together with the textual notation can be represented:

![Graphical notation Beta-Binomial Model - One group](chapters/images/Graph-binom.png)

### An outlook: Hierarchical models

Often data can be considered as part of an overall structure. Single observations can be modelled belonging into different groups. These groups in turn are part of a superordinate group etc. Such information are presented in a model in form of a hierarchy.

For example, consider again the coin flip experiment. The outcome of *head* is influenced by the probability $\theta$. Further, $\theta$ is assumed to be distributed as Beta(1,1). Remember that the parameter a and b of a Beta-distribution can be considered in this context as: $a=$number of heads and $b=$ number of tails, consequently, $n=a+b$.

#### Reparameterization of a Beta distribution

Probability distributions can be described by their *central tendency* and *spread* (or dispersion). The *mode* of a Beta distribution is defined as:

$\omega=\frac{a-1}{a+b-2}$,

and the concentration as:

$\kappa=a+b.$

The nice thing is, that the definition of the *mode* as well as of the *concentration* consists solely of the parameters a and b. Therefore, it is possible to re-express the parameters of a Beta density in terms of $\omega$ and $\kappa$, such that:

$$Beta(a,b)=Beta\left(\omega(\kappa -2)+1, (1-\omega)(\kappa -2)+1\right).$$

*Why this is useful? And what is its value in connection with hierarchical modeling?*

Return back to the coin flip experiment. So far, the parameters of the prior on $\theta$ are fixed: $a=1$ and $b=1$. Assume that we get further information: The manufacturing process of the coins has a bias near $\omega$ (example taken from [@kruschke2015]). But how to incorporate this additional knowledge in the model?

At this point, the hierarchy and the reparameterization come into play. Hierarchy because a further assumption is placed on top of the existing model and reparameterization, because we want to express the prior in terms of the mode $\omega$.

Such that the model can be assumed as follows:

![Graphical notation hierarchical Beta-Binomial Model - One group](chapters/images/binom-oneLevel.png)


Now, the parameters of the hyperpriors (Gamma and Beta) are fixed, but they can be treated as parameters as well ... as such hierarchical models can be created with any degree of complexity:

![Graphical notation hierarchical Beta-Binomial Model - One group](chapters/images/hierarchical-model.png)

## Further examples

### Difference between two groups

In the introductory example we asked for the underlying probability $\theta$ of a single coin that was flipped repeatedly.
Consider now, that a second coin $y_2$ is introduced. One question that arises might be for example: *How different are the biases of the two coins?*

```{r}
#simulate flips of two coins
sample.space <- c(0,1)
##First coin:
theta1 <- 0.5              # probability of a success (here: head)
X1 <- 30                   # number of trials in the experiment
n1 <- 100                  # number of observations
k1 <- 0                    # number of heads [initialization]

for (i in 1: n1) {
  k1[i] <- sum(sample(sample.space, size = X1, replace = TRUE,
                     prob = c(theta1, 1 - theta1)))
}
##Second coin:
theta2 <- 0.7              # probability of a success (here: head)
X2 <- 30                   # number of trials in the experiment
n2 <- 100                  # number of observations
k2 <- 0                    # number of heads [initialization]

## repeat experiment N-times
for (i in 1: n2) {
  k2[i] <- sum(sample(sample.space, size = X2, replace = TRUE,
                     prob = c(theta2, 1 - theta2)))
}

## show results in a tibble
coin.flip2 <- tibble("coin" = c(replicate(n1,"coin1"), replicate(n2,"coin2")),
                     "n" = c(seq(from=1, to=n1, by=1), seq(from=1, to=n2, by=1)),
                     "k" = c(k1,k2),
                     "x" = c(replicate(n1,X1), replicate(n2,X2))
) %>%
  print()
```

```{r}
#Plotting the observed results
ggplot(data=coin.flip2,mapping = aes(x=k, fill=coin ))+
          geom_histogram()
```

#### Conceptual steps for modeling

We suppose that the underlying probabilities of the two coins correspond to *different* latent variables $\theta_1$ and $\theta_2$.

**First step** is again the *identification of the relevant variables* according to the research question. As already indicated for the "one coin" example we have:

* the observed number of heads $k_1$ and $k_2$ (for each coin, respectively), which is influenced by
* the number of observations $n_1$ and $n_2$ and by
* the underlying probabilities $\theta_1$ and $\theta_2$.

Furthermore, from a conceptional perspective, we are interested in the *difference between the coin biases*. Therefore a further variable will be introduced $\delta$, defined by:
$$\delta =\theta_1 - \theta_2.$$

The *distributional assumptions*, according to the **second and third step**, can be adopted from the "one coin" example, such that the graphical notation (including the textual notation) can be denoted as follows:

#### Notation Beta-Binomial Model - Two Groups

![Graphical notation Beta-Binomial Model - Two groups](chapters/images/Graph-Factorial.png)

### Simple linear regression with one metric predictor

The following example originates from a data set in which speed of cars and the distance taken to stop was recorded. It is a simple data set good for introducing the basic ideas for simple linear regression.
```{r}
#The "cars" data set
data(cars)
#take a look at the variables included in the data set
str(cars)
```
One possible question could be how much the stopping distance increases when the speed of a car increases.

#### Conceptual steps for modeling

First step is to **identify the relevant variables**. In this case these are "speed" measured in mph and "distance" measured in ft, thus, both variables are metric variables. As distance will be predicted from speed. The *predicted variable* is "distance" and the *predictor variable* is "speed". A scatter plot can visualize a possible relationship between both variables.
```{r}
plot(x=cars$speed,y=cars$dist, type="p", main="scatter plot of cars data set",
     ylab="distance in ft", xlab="speed in mph")
```

Next step is to define a **descriptive model of the data**. According to the scatter plot it is not too absurd to think that distance might be proportional to speed. Therefore, a linear relationship between both variables can be assumed, where speed is used i order to predict distance. But how can the distribution of the predicted variable "distance" be described? The following plot shows in blue the density of the actual distance values.

```{r, eval=FALSE}
#density of distance values in blue
#(in black simulation of a normal distribution)
dens(cars$dist, col="blue", norm.comp = TRUE, main="Distribution of distance",
     xlab="distance in ft")
```

Although the distribution of "distance" values is not identical to the corresponding normal distribution, it can be assumed that the values follow a *normal distribution*. The underlying consideration is that the distance values $y_i$ are distributed randomly according to a normal distribution around the predicted value $\hat{y}$ and with a standard deviation denoted with $\sigma$. This can be denoted as:
$$y_i\sim Normal(\mu, \sigma).$$
The index $i$ indicates each element (i.e. car) of the list $y$, which in turn is the list of distances.

In the third step, a Bayesian perspective is taken the **prior knowlege** (before seeing the data) has to be defined. The parameters of the current model are the predicted value $\mu$ and the standard deviation $\sigma$. For the parameter $\mu$ a normal distribution can be assumend with parameters that reflect the estimated values from the sample.

```{r}
#descriptive statistics from the sample
tibble(variables=c("speed", "distance"),
       mean=c(mean(cars$speed),mean(cars$dist)),
       sigma = c(sd(cars$speed), sd(cars$dist)))
```

$$\mu\sim Normal(43,26)$$

For the standard deviation $\sigma$ a uniform distribution is assumed:
$$\sigma\sim Uniform(0,40)$$

#### Excursus: Identically and independently distributed (*iid*)

The short model description $y_i\sim Normal(\mu, \sigma)$ incorporates often already an assumption about the distribution of distance-values: They are *identically and independently distributed*. Often the abbreviation *iid* can be found for this assumption:
$$y_i\overset{\text{iid}}{\sim} Normal(\mu, \sigma).$$

The abbreviation *iid* indicates that each value $y_i$ has the same probability function, independent of the other $y$ values and using the same parameters [@mcelreath2015]. This is hardly ever true (why hierarchical modeling is very attractive). For example, thinking about the cars in the current example data set. Some cars may be of different types or even the same type but different batches. But the question is: Is this underlying dependency relevant for the model? If yes, this information has to be added in the model (e.g. in form of a hierarchical model). Janyes states it as follows: "*The onus is always on the user to make sure that all information, which his common sense tells him is relevant to the problem, is actually incorporated into the equations, (...).*"[@jaynes2003,p.339]. But if one do not know any relevant underlying relationships the most conservative distribution to use is *iid*. Note, that the stated assumptions define how the model represents a problem and not how the world should be understood. For example, there might exist underlying correlations but on the overall distribution there influence tends towards zero. In such cases it remains usefull to assume iid [@mcelreath2015].

### Notation Simple Regression model
![Graphical notation Simplre Regression model](chapters/images/Graph-SimpReg.png)

## Further elaboration on modeling (in anticipation of the topic "estimation")

### Beta-Binomial model - one group (revisited)

Sofar the existence of the underlying probability $\theta$ for observing head as outcome of a coin flip has been discussed. But the estimation of $\theta$ has been ignored until yet. Although "estimation" will be topic of next chapter, it is helpful at this point to discuss the introduced models further. In order to estimate $\theta$ *parameter(s)* are needed. When it comes to estimation exactly this/these parameter(s) will be the result(s), therefore is is important to see already the connection to the models that were developed in this chapter.

For the coin flip example the value of interest is the underlying probability, thus, only one parameter is needed:$\beta_0$. (Note: Latin letters are used when we refer to the sample, Greek letters are used when we refer to the population.)

How is $\beta_0$ linked to the latent variable $\theta$?

Considering for example the simplest case: a *linear relationship* (see next plot left side). The problem which arises at this point is that $\theta$ represents a probability, and is therefore bounded to the range 0-1 (grey shaded area).

```{r}
#Different relationships between the parameter and expected value
x <- seq(from=-4, to=4, length.out = 100)
y <- x                 #linear relationship
y.log <- inv_logit_scaled(x)   #logistic relationship

par(mfrow = c(1, 2))   #set both plot beside each other
plot(x, y, type="l", ylab=expression(theta), xlab=expression(beta[0]))
rect(-5, 0, 5, 1, col = rgb(0.5, 0.5, 0.5, 1/4), border = NA)
plot(x, y.log, type="l", ylab=expression(logit~(theta)), xlab=expression(beta[0]))
rect(-5, 0, 5, 1, col = rgb(0.5, 0.5, 0.5, 1/4), border = NA)
```

A mathematical transformation is needed such that the parameter $\beta_0$ can take any value while $\theta$ is bounded to the range 0-1. One transformation that offers exactly this possibility is the *logit link function* (see above plot right side)

$$logit(\theta) = \beta_0.$$
As the underlying assumption maps the parameter to the latent variable $\theta$ (and not the other way around) from a conceptional point of view the *inverse link function* is more appropriate, which is the *logistic link* in this case:

$$\theta = logistic(\beta_0).$$
It is defined as
$$\theta=\frac{exp(\beta_0)}{1+exp(\beta_0)}.$$

Both expression, *logit* and *logistic* link achieve mathematically the same result but it is conceptually just a different matter of emphasis [@kruschke2015].

#### Notation of beta-binomial model - one group (revisited)

The current descriptive model incorporates the idea that parameter $\beta_0$ is estimated from the given sample. It defines the latent variable $\theta$. The parameter is mapped to $\theta$ by a logistic link function. The underlying probability $\theta$ designates the observed number of upcoming heads. The number of upcoming heads in turn, is assumed to be distributed as Binomial distribution.

<!-- [here graphical notation including textual notation... but I don't know how to write it properly down... what is the right order????] -->

### Beta-Binomial model - two groups (revisited)

In the above model for two coins the latent variable $\delta$ was already introduced. It is defined by the difference between the underlying probabilities $\theta_1-\theta_2$.
Which parameters should be used in order to estimate the difference between both groups? As we will see, it turns out that the same mathematical form can be used, as one would use for simple linear regression:

$$\theta_j=\beta_0+\beta_1*X_{Group_j},$$
$$\textrm{with } X_{Group_j}=\begin{cases}
0, \textrm{if coin 2,}\\
1, \textrm{if coin 1.}
\end{cases}$$

Considering *coin 2*, the above equation would result in
$$\theta_2 = \beta_0,$$
which is the *intercept* and indicates the proportion of head coming up for coin 2.

Considering by contrast coin 1, then the equation would result in:
$$\theta_1 = \beta_0 + \beta_1.$$
The proportion of coming up head for coin 1 has to be calculated by summing up the *intercept* $\beta_0$ and the *slope* $\beta_1$.

Taken together: *What is the interpretation of the slope $\beta_1$?*
The difference $\delta=\theta_1-\theta2$ is
$$\theta_1 - \theta_2 = (\beta_0+\beta_1)-\beta_0=\beta_1=\delta,$$
the slope $\beta_1$, thus, we can see that this parameterization enables us to estimate the difference between two groups. When in comes to estimation and interpretation the results will be the intercept $b_0$ and the slope $b_1$.

### Simple linear regression model (revisited)
