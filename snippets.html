<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>14.7 Snippets | Introduction to Data Analysis</title>
  <meta name="description" content="Course Material for I2DA-2019 in Cognitive Science at University Osnabrück" />
  <meta name="generator" content="bookdown 0.17.2 and GitBook 2.6.7" />

  <meta property="og:title" content="14.7 Snippets | Introduction to Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course Material for I2DA-2019 in Cognitive Science at University Osnabrück" />
  <meta name="github-repo" content="michael-franke/intro-data-analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="14.7 Snippets | Introduction to Data Analysis" />
  
  <meta name="twitter:description" content="Course Material for I2DA-2019 in Cognitive Science at University Osnabrück" />
  

<meta name="author" content="Michael Franke" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="testing-coefficients.html"/>
<link rel="next" href="logistic-regression.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!--<link rel="stylesheet" href="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-editor-1.0.9.css">-->
<link rel="stylesheet" href="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-viz-0.7.11.css">

<script src="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-editor-1.0.9.js"></script>
<script src="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-viz-0.7.11.js"></script>
<script src="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-v0.9.13.js" defer async></script>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span. { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />
<link rel="stylesheet" href="webppl-editor.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li class="chapter" data-level="1" data-path="general-introduction.html"><a href="general-introduction.html"><i class="fa fa-check"></i><b>1</b> General Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="Chap-01-00-intro-learning-goals.html"><a href="Chap-01-00-intro-learning-goals.html"><i class="fa fa-check"></i><b>1.1</b> Learning goals</a></li>
<li class="chapter" data-level="1.2" data-path="Chap-01-00-intro-course-structure.html"><a href="Chap-01-00-intro-course-structure.html"><i class="fa fa-check"></i><b>1.2</b> Course structure</a></li>
<li class="chapter" data-level="1.3" data-path="Chap-01-00-intro-tools-methods.html"><a href="Chap-01-00-intro-tools-methods.html"><i class="fa fa-check"></i><b>1.3</b> Tools and topics covered (and not covered) here</a></li>
<li class="chapter" data-level="1.4" data-path="Chap-01-00-intro-data-sets.html"><a href="Chap-01-00-intro-data-sets.html"><i class="fa fa-check"></i><b>1.4</b> Data sets covered</a></li>
<li class="chapter" data-level="1.5" data-path="Chap-01-00-intro-installation.html"><a href="Chap-01-00-intro-installation.html"><i class="fa fa-check"></i><b>1.5</b> Installation</a><ul>
<li class="chapter" data-level="1.5.1" data-path="Chap-01-00-intro-installation.html"><a href="Chap-01-00-intro-installation.html#Chap-01-00-intro-installation-VirtualBox"><i class="fa fa-check"></i><b>1.5.1</b> VirtualBox Setup</a></li>
<li class="chapter" data-level="1.5.2" data-path="Chap-01-00-intro-installation.html"><a href="Chap-01-00-intro-installation.html#Chap-01-00-intro-installation-Manual"><i class="fa fa-check"></i><b>1.5.2</b> Manual installation</a></li>
<li class="chapter" data-level="1.5.3" data-path="Chap-01-00-intro-installation.html"><a href="Chap-01-00-intro-installation.html#Chap-01-00-intro-installation-Updating"><i class="fa fa-check"></i><b>1.5.3</b> Updating the course package</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap-01-01-R.html"><a href="Chap-01-01-R.html"><i class="fa fa-check"></i><b>2</b> Basics of R</a><ul>
<li class="chapter" data-level="2.1" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html"><i class="fa fa-check"></i><b>2.1</b> First steps</a><ul>
<li class="chapter" data-level="2.1.1" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#functions"><i class="fa fa-check"></i><b>2.1.1</b> Functions</a></li>
<li class="chapter" data-level="2.1.2" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#variables"><i class="fa fa-check"></i><b>2.1.2</b> Variables</a></li>
<li class="chapter" data-level="2.1.3" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#literate-coding"><i class="fa fa-check"></i><b>2.1.3</b> Literate coding</a></li>
<li class="chapter" data-level="2.1.4" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#objects"><i class="fa fa-check"></i><b>2.1.4</b> Objects</a></li>
<li class="chapter" data-level="2.1.5" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#packages"><i class="fa fa-check"></i><b>2.1.5</b> Packages</a></li>
<li class="chapter" data-level="2.1.6" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#Chap-01-01-R-help"><i class="fa fa-check"></i><b>2.1.6</b> Getting help</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="ch1-data-types.html"><a href="ch1-data-types.html"><i class="fa fa-check"></i><b>2.2</b> Data types</a><ul>
<li class="chapter" data-level="2.2.1" data-path="ch1-data-types.html"><a href="ch1-data-types.html#numeric-vectors-matrices"><i class="fa fa-check"></i><b>2.2.1</b> Numeric vectors &amp; matrices</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch1-data-types.html"><a href="ch1-data-types.html#booleans"><i class="fa fa-check"></i><b>2.2.2</b> Booleans</a></li>
<li class="chapter" data-level="2.2.3" data-path="ch1-data-types.html"><a href="ch1-data-types.html#special-values"><i class="fa fa-check"></i><b>2.2.3</b> Special values</a></li>
<li class="chapter" data-level="2.2.4" data-path="ch1-data-types.html"><a href="ch1-data-types.html#characters-strings"><i class="fa fa-check"></i><b>2.2.4</b> Characters (= strings)</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch1-data-types.html"><a href="ch1-data-types.html#factors"><i class="fa fa-check"></i><b>2.2.5</b> Factors</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch1-data-types.html"><a href="ch1-data-types.html#lists-data-frames-tibbles"><i class="fa fa-check"></i><b>2.2.6</b> Lists, data frames &amp; tibbles</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="Chap-01-01-functions.html"><a href="Chap-01-01-functions.html"><i class="fa fa-check"></i><b>2.3</b> Functions</a><ul>
<li class="chapter" data-level="2.3.1" data-path="Chap-01-01-functions.html"><a href="Chap-01-01-functions.html#some-important-built-in-functions"><i class="fa fa-check"></i><b>2.3.1</b> Some important built-in functions</a></li>
<li class="chapter" data-level="2.3.2" data-path="Chap-01-01-functions.html"><a href="Chap-01-01-functions.html#defining-your-own-functions"><i class="fa fa-check"></i><b>2.3.2</b> Defining your own functions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="ch-01-01-loops-and-maps.html"><a href="ch-01-01-loops-and-maps.html"><i class="fa fa-check"></i><b>2.4</b> Loops and maps</a></li>
<li class="chapter" data-level="2.5" data-path="Chap-01-01-piping.html"><a href="Chap-01-01-piping.html"><i class="fa fa-check"></i><b>2.5</b> Piping</a></li>
<li class="chapter" data-level="2.6" data-path="ch-01-01-Rmarkdown.html"><a href="ch-01-01-Rmarkdown.html"><i class="fa fa-check"></i><b>2.6</b> Rmarkdown</a></li>
</ul></li>
<li class="part"><span><b>II Data</b></span></li>
<li class="chapter" data-level="3" data-path="Chap-02-01-data.html"><a href="Chap-02-01-data.html"><i class="fa fa-check"></i><b>3</b> Data, variables &amp; experimental designs</a><ul>
<li class="chapter" data-level="3.1" data-path="Chap-02-01-data-kinds-of-data.html"><a href="Chap-02-01-data-kinds-of-data.html"><i class="fa fa-check"></i><b>3.1</b> Different kinds of data</a></li>
<li class="chapter" data-level="3.2" data-path="Chap-02-01-data-variables.html"><a href="Chap-02-01-data-variables.html"><i class="fa fa-check"></i><b>3.2</b> On the notion of “variables”</a></li>
<li class="chapter" data-level="3.3" data-path="Chap-02-01-data-exp-design.html"><a href="Chap-02-01-data-exp-design.html"><i class="fa fa-check"></i><b>3.3</b> Basics of experimental design</a><ul>
<li class="chapter" data-level="3.3.1" data-path="Chap-02-01-data-exp-design.html"><a href="Chap-02-01-data-exp-design.html#what-to-analyze-dependent-variables"><i class="fa fa-check"></i><b>3.3.1</b> What to analyze? – Dependent variables</a></li>
<li class="chapter" data-level="3.3.2" data-path="Chap-02-01-data-exp-design.html"><a href="Chap-02-01-data-exp-design.html#conditions-trials-items"><i class="fa fa-check"></i><b>3.3.2</b> Conditions, trials, items</a></li>
<li class="chapter" data-level="3.3.3" data-path="Chap-02-01-data-exp-design.html"><a href="Chap-02-01-data-exp-design.html#sample-size"><i class="fa fa-check"></i><b>3.3.3</b> Sample size</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data-wrangling.html"><a href="data-wrangling.html"><i class="fa fa-check"></i><b>4</b> Data Wrangling</a><ul>
<li class="chapter" data-level="4.1" data-path="Chap-02-02-data-IO.html"><a href="Chap-02-02-data-IO.html"><i class="fa fa-check"></i><b>4.1</b> Data in, data out</a></li>
<li class="chapter" data-level="4.2" data-path="Chap-02-02-data-tidy-data.html"><a href="Chap-02-02-data-tidy-data.html"><i class="fa fa-check"></i><b>4.2</b> Tidy data</a><ul>
<li class="chapter" data-level="4.2.1" data-path="Chap-02-02-data-tidy-data.html"><a href="Chap-02-02-data-tidy-data.html#running-example"><i class="fa fa-check"></i><b>4.2.1</b> Running example</a></li>
<li class="chapter" data-level="4.2.2" data-path="Chap-02-02-data-tidy-data.html"><a href="Chap-02-02-data-tidy-data.html#definition-of-tidy-data"><i class="fa fa-check"></i><b>4.2.2</b> Definition of <em>tidy data</em></a></li>
<li class="chapter" data-level="4.2.3" data-path="Chap-02-02-data-tidy-data.html"><a href="Chap-02-02-data-tidy-data.html#excursion-non-redundant-data"><i class="fa fa-check"></i><b>4.2.3</b> Excursion: non-redundant data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html"><i class="fa fa-check"></i><b>4.3</b> Data manipulation: the basics</a><ul>
<li class="chapter" data-level="4.3.1" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html#pivoting"><i class="fa fa-check"></i><b>4.3.1</b> Pivoting</a></li>
<li class="chapter" data-level="4.3.2" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html#subsetting-row-columns"><i class="fa fa-check"></i><b>4.3.2</b> Subsetting row &amp; columns</a></li>
<li class="chapter" data-level="4.3.3" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html#Chap-02-02-tidy-selection"><i class="fa fa-check"></i><b>4.3.3</b> Tidy selection of column names</a></li>
<li class="chapter" data-level="4.3.4" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html#adding-changing-and-renaming-columns"><i class="fa fa-check"></i><b>4.3.4</b> Adding, changing and renaming columns</a></li>
<li class="chapter" data-level="4.3.5" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html#splitting-and-uniting-columns"><i class="fa fa-check"></i><b>4.3.5</b> Splitting and uniting columns</a></li>
<li class="chapter" data-level="4.3.6" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html#sorting-a-data-set"><i class="fa fa-check"></i><b>4.3.6</b> Sorting a data set</a></li>
<li class="chapter" data-level="4.3.7" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html#combining-tibbles"><i class="fa fa-check"></i><b>4.3.7</b> Combining tibbles</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="Chap-02-02-data-grouping-nesting.html"><a href="Chap-02-02-data-grouping-nesting.html"><i class="fa fa-check"></i><b>4.4</b> Grouped operations</a></li>
<li class="chapter" data-level="4.5" data-path="Chap-02-02-data-case-study-KoF.html"><a href="Chap-02-02-data-case-study-KoF.html"><i class="fa fa-check"></i><b>4.5</b> Case study: the King of France</a><ul>
<li class="chapter" data-level="4.5.1" data-path="Chap-02-02-data-case-study-KoF.html"><a href="Chap-02-02-data-case-study-KoF.html#cleaning-the-data"><i class="fa fa-check"></i><b>4.5.1</b> Cleaning the data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap-02-03-summary-statistics.html"><a href="Chap-02-03-summary-statistics.html"><i class="fa fa-check"></i><b>5</b> Summary statistics</a><ul>
<li class="chapter" data-level="5.1" data-path="Chap-02-03-summary-statistics-counts.html"><a href="Chap-02-03-summary-statistics-counts.html"><i class="fa fa-check"></i><b>5.1</b> Counts and proportions</a><ul>
<li class="chapter" data-level="5.1.1" data-path="Chap-02-03-summary-statistics-counts.html"><a href="Chap-02-03-summary-statistics-counts.html#loading-and-inspecting-the-data"><i class="fa fa-check"></i><b>5.1.1</b> Loading and inspecting the data</a></li>
<li class="chapter" data-level="5.1.2" data-path="Chap-02-03-summary-statistics-counts.html"><a href="Chap-02-03-summary-statistics-counts.html#obtaining-counts-with-n-count-and-tally"><i class="fa fa-check"></i><b>5.1.2</b> Obtaining counts with <code>n</code>, <code>count</code> and <code>tally</code></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="Chap-02-03-summary-statistics-1D.html"><a href="Chap-02-03-summary-statistics-1D.html"><i class="fa fa-check"></i><b>5.2</b> Central tendency and dispersion</a><ul>
<li class="chapter" data-level="5.2.1" data-path="Chap-02-03-summary-statistics-1D.html"><a href="Chap-02-03-summary-statistics-1D.html#the-data-for-the-remainder-of-the-chapter"><i class="fa fa-check"></i><b>5.2.1</b> The data for the remainder of the chapter</a></li>
<li class="chapter" data-level="5.2.2" data-path="Chap-02-03-summary-statistics-1D.html"><a href="Chap-02-03-summary-statistics-1D.html#measures-of-central-tendency"><i class="fa fa-check"></i><b>5.2.2</b> Measures of central tendency</a></li>
<li class="chapter" data-level="5.2.3" data-path="Chap-02-03-summary-statistics-1D.html"><a href="Chap-02-03-summary-statistics-1D.html#measures-of-dispersion"><i class="fa fa-check"></i><b>5.2.3</b> Measures of dispersion</a></li>
<li class="chapter" data-level="5.2.4" data-path="Chap-02-03-summary-statistics-1D.html"><a href="Chap-02-03-summary-statistics-1D.html#quantifying-confidence-with-bootstrapping"><i class="fa fa-check"></i><b>5.2.4</b> Quantifying confidence with bootstrapping</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="Chap-02-03-summary-statistics-2D.html"><a href="Chap-02-03-summary-statistics-2D.html"><i class="fa fa-check"></i><b>5.3</b> Co-variance &amp; correlation</a><ul>
<li class="chapter" data-level="5.3.1" data-path="Chap-02-03-summary-statistics-2D.html"><a href="Chap-02-03-summary-statistics-2D.html#covariance"><i class="fa fa-check"></i><b>5.3.1</b> Covariance</a></li>
<li class="chapter" data-level="5.3.2" data-path="Chap-02-03-summary-statistics-2D.html"><a href="Chap-02-03-summary-statistics-2D.html#correlation"><i class="fa fa-check"></i><b>5.3.2</b> Correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap-02-02-visualization.html"><a href="Chap-02-02-visualization.html"><i class="fa fa-check"></i><b>6</b> Data Visualization</a><ul>
<li class="chapter" data-level="6.1" data-path="Chap-02-04-Anscombe-example.html"><a href="Chap-02-04-Anscombe-example.html"><i class="fa fa-check"></i><b>6.1</b> Motivating example: Anscombe’s quartet</a></li>
<li class="chapter" data-level="6.2" data-path="Chap-02-04-good-visualization.html"><a href="Chap-02-04-good-visualization.html"><i class="fa fa-check"></i><b>6.2</b> Visualization: the good, the bad and the info-graphic</a></li>
<li class="chapter" data-level="6.3" data-path="Chap-02-04-ggplot.html"><a href="Chap-02-04-ggplot.html"><i class="fa fa-check"></i><b>6.3</b> Basics of <code>ggplot</code></a><ul>
<li class="chapter" data-level="6.3.1" data-path="Chap-02-04-ggplot.html"><a href="Chap-02-04-ggplot.html#incrementally-composition-of-a-plot"><i class="fa fa-check"></i><b>6.3.1</b> Incrementally composition of a plot</a></li>
<li class="chapter" data-level="6.3.2" data-path="Chap-02-04-ggplot.html"><a href="Chap-02-04-ggplot.html#elements-in-the-layered-grammar-of-graphs"><i class="fa fa-check"></i><b>6.3.2</b> Elements in the layered grammar of graphs</a></li>
<li class="chapter" data-level="6.3.3" data-path="Chap-02-04-ggplot.html"><a href="Chap-02-04-ggplot.html#layers-and-groups"><i class="fa fa-check"></i><b>6.3.3</b> Layers and groups</a></li>
<li class="chapter" data-level="6.3.4" data-path="Chap-02-04-ggplot.html"><a href="Chap-02-04-ggplot.html#grouping"><i class="fa fa-check"></i><b>6.3.4</b> Grouping</a></li>
<li class="chapter" data-level="6.3.5" data-path="Chap-02-04-ggplot.html"><a href="Chap-02-04-ggplot.html#example-of-a-customized-plot"><i class="fa fa-check"></i><b>6.3.5</b> Example of a customized plot</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html"><i class="fa fa-check"></i><b>6.4</b> A rendezvous with popular geoms</a><ul>
<li class="chapter" data-level="6.4.1" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html#scatter-plots-with-geom_point"><i class="fa fa-check"></i><b>6.4.1</b> Scatter plots with <code>geom_point</code></a></li>
<li class="chapter" data-level="6.4.2" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html#smooth"><i class="fa fa-check"></i><b>6.4.2</b> Smooth</a></li>
<li class="chapter" data-level="6.4.3" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html#line"><i class="fa fa-check"></i><b>6.4.3</b> Line</a></li>
<li class="chapter" data-level="6.4.4" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html#barplot"><i class="fa fa-check"></i><b>6.4.4</b> Barplot</a></li>
<li class="chapter" data-level="6.4.5" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html#plotting-distributions-histograms-boxplots-densities-and-violins"><i class="fa fa-check"></i><b>6.4.5</b> Plotting distributions: histograms, boxplots, densities and violins</a></li>
<li class="chapter" data-level="6.4.6" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html#rugs"><i class="fa fa-check"></i><b>6.4.6</b> Rugs</a></li>
<li class="chapter" data-level="6.4.7" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html#annotation"><i class="fa fa-check"></i><b>6.4.7</b> Annotation</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="Chap-02-04-faceting.html"><a href="Chap-02-04-faceting.html"><i class="fa fa-check"></i><b>6.5</b> Faceting</a></li>
<li class="chapter" data-level="6.6" data-path="Chap-02-04-customization.html"><a href="Chap-02-04-customization.html"><i class="fa fa-check"></i><b>6.6</b> Customization etc.</a><ul>
<li class="chapter" data-level="6.6.1" data-path="Chap-02-04-customization.html"><a href="Chap-02-04-customization.html#themes"><i class="fa fa-check"></i><b>6.6.1</b> Themes</a></li>
<li class="chapter" data-level="6.6.2" data-path="Chap-02-04-customization.html"><a href="Chap-02-04-customization.html#guides"><i class="fa fa-check"></i><b>6.6.2</b> Guides</a></li>
<li class="chapter" data-level="6.6.3" data-path="Chap-02-04-customization.html"><a href="Chap-02-04-customization.html#axes-ticks-and-tick-labels"><i class="fa fa-check"></i><b>6.6.3</b> Axes, ticks and tick labels</a></li>
<li class="chapter" data-level="6.6.4" data-path="Chap-02-04-customization.html"><a href="Chap-02-04-customization.html#labels"><i class="fa fa-check"></i><b>6.6.4</b> Labels</a></li>
<li class="chapter" data-level="6.6.5" data-path="Chap-02-04-customization.html"><a href="Chap-02-04-customization.html#combining-arranging-plots"><i class="fa fa-check"></i><b>6.6.5</b> Combining &amp; arranging plots</a></li>
<li class="chapter" data-level="6.6.6" data-path="Chap-02-04-customization.html"><a href="Chap-02-04-customization.html#latex-expressions-in-plot-labels"><i class="fa fa-check"></i><b>6.6.6</b> LaTeX expressions in plot labels</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Models and inferences</b></span></li>
<li class="chapter" data-level="7" data-path="Chap-03-01-probability.html"><a href="Chap-03-01-probability.html"><i class="fa fa-check"></i><b>7</b> Basics of Probability Theory</a><ul>
<li class="chapter" data-level="7.1" data-path="Chap-03-01-probability-basics.html"><a href="Chap-03-01-probability-basics.html"><i class="fa fa-check"></i><b>7.1</b> Probability</a><ul>
<li class="chapter" data-level="7.1.1" data-path="Chap-03-01-probability-basics.html"><a href="Chap-03-01-probability-basics.html#outcomes-events-observations"><i class="fa fa-check"></i><b>7.1.1</b> Outcomes, events, observations</a></li>
<li class="chapter" data-level="7.1.2" data-path="Chap-03-01-probability-basics.html"><a href="Chap-03-01-probability-basics.html#probability-distributions"><i class="fa fa-check"></i><b>7.1.2</b> Probability distributions</a></li>
<li class="chapter" data-level="7.1.3" data-path="Chap-03-01-probability-basics.html"><a href="Chap-03-01-probability-basics.html#interpretations-of-probability"><i class="fa fa-check"></i><b>7.1.3</b> Interpretations of probability</a></li>
<li class="chapter" data-level="7.1.4" data-path="Chap-03-01-probability-basics.html"><a href="Chap-03-01-probability-basics.html#urns-frequencies-distributions-as-samples"><i class="fa fa-check"></i><b>7.1.4</b> Urns, frequencies &amp; distributions as samples</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="Chap-03-01-probability-marginal.html"><a href="Chap-03-01-probability-marginal.html"><i class="fa fa-check"></i><b>7.2</b> Structured events &amp; marginal distributions</a><ul>
<li class="chapter" data-level="7.2.1" data-path="Chap-03-01-probability-marginal.html"><a href="Chap-03-01-probability-marginal.html#probability-table-for-a-flip-and-draw-scenario"><i class="fa fa-check"></i><b>7.2.1</b> Probability table for a flip-and-draw scenario</a></li>
<li class="chapter" data-level="7.2.2" data-path="Chap-03-01-probability-marginal.html"><a href="Chap-03-01-probability-marginal.html#structured-events-and-joint-probability-distributions"><i class="fa fa-check"></i><b>7.2.2</b> Structured events and joint-probability distributions</a></li>
<li class="chapter" data-level="7.2.3" data-path="Chap-03-01-probability-marginal.html"><a href="Chap-03-01-probability-marginal.html#marginalization"><i class="fa fa-check"></i><b>7.2.3</b> Marginalization</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="Chap-03-01-probability-conditional.html"><a href="Chap-03-01-probability-conditional.html"><i class="fa fa-check"></i><b>7.3</b> Conditional probability</a><ul>
<li class="chapter" data-level="7.3.1" data-path="Chap-03-01-probability-conditional.html"><a href="Chap-03-01-probability-conditional.html#bayes-rule"><i class="fa fa-check"></i><b>7.3.1</b> Bayes rule</a></li>
<li class="chapter" data-level="7.3.2" data-path="Chap-03-01-probability-conditional.html"><a href="Chap-03-01-probability-conditional.html#Chap-03-01-probability-independence"><i class="fa fa-check"></i><b>7.3.2</b> Stochastic (in-)dependence</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="Chap-03-01-probability-random-variables.html"><a href="Chap-03-01-probability-random-variables.html"><i class="fa fa-check"></i><b>7.4</b> Random variables</a><ul>
<li class="chapter" data-level="7.4.1" data-path="Chap-03-01-probability-random-variables.html"><a href="Chap-03-01-probability-random-variables.html#notation-terminology"><i class="fa fa-check"></i><b>7.4.1</b> Notation &amp; terminology</a></li>
<li class="chapter" data-level="7.4.2" data-path="Chap-03-01-probability-random-variables.html"><a href="Chap-03-01-probability-random-variables.html#cumulative-distribution-functions-mass-density"><i class="fa fa-check"></i><b>7.4.2</b> Cumulative distribution functions, mass &amp; density</a></li>
<li class="chapter" data-level="7.4.3" data-path="Chap-03-01-probability-random-variables.html"><a href="Chap-03-01-probability-random-variables.html#expected-value-variance"><i class="fa fa-check"></i><b>7.4.3</b> Expected value &amp; variance</a></li>
<li class="chapter" data-level="7.4.4" data-path="Chap-03-01-probability-random-variables.html"><a href="Chap-03-01-probability-random-variables.html#composite-random-variables"><i class="fa fa-check"></i><b>7.4.4</b> Composite random variables</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="Chap-03-01-probability-R.html"><a href="Chap-03-01-probability-R.html"><i class="fa fa-check"></i><b>7.5</b> Probability distributions in R</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap-03-03-models.html"><a href="Chap-03-03-models.html"><i class="fa fa-check"></i><b>8</b> Models</a><ul>
<li class="chapter" data-level="8.1" data-path="Chap-03-03-models-general.html"><a href="Chap-03-03-models-general.html"><i class="fa fa-check"></i><b>8.1</b> Probabilistic models in statistics</a><ul>
<li class="chapter" data-level="8.1.1" data-path="Chap-03-03-models-general.html"><a href="Chap-03-03-models-general.html#Chap-03-03-models-general-urn-example"><i class="fa fa-check"></i><b>8.1.1</b> Example 1: a single draw from an urn</a></li>
<li class="chapter" data-level="8.1.2" data-path="Chap-03-03-models-general.html"><a href="Chap-03-03-models-general.html#example-2-avocado-prices-by-type"><i class="fa fa-check"></i><b>8.1.2</b> Example 2: avocado prices by type</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="Chap-03-03-models-parameters-priors.html"><a href="Chap-03-03-models-parameters-priors.html"><i class="fa fa-check"></i><b>8.2</b> Parameters, priors, probability and predictions</a><ul>
<li class="chapter" data-level="8.2.1" data-path="Chap-03-03-models-parameters-priors.html"><a href="Chap-03-03-models-parameters-priors.html#whats-a-model-parameter"><i class="fa fa-check"></i><b>8.2.1</b> What’s a model parameter?</a></li>
<li class="chapter" data-level="8.2.2" data-path="Chap-03-03-models-parameters-priors.html"><a href="Chap-03-03-models-parameters-priors.html#Chap-03-02-models-priors"><i class="fa fa-check"></i><b>8.2.2</b> Priors over parameters</a></li>
<li class="chapter" data-level="8.2.3" data-path="Chap-03-03-models-parameters-priors.html"><a href="Chap-03-03-models-parameters-priors.html#two-notions-of-probability-revisited"><i class="fa fa-check"></i><b>8.2.3</b> Two notions of probability (revisited)</a></li>
<li class="chapter" data-level="8.2.4" data-path="Chap-03-03-models-parameters-priors.html"><a href="Chap-03-03-models-parameters-priors.html#prior-predictions"><i class="fa fa-check"></i><b>8.2.4</b> Prior predictions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="Chap-03-03-models-three-pillars.html"><a href="Chap-03-03-models-three-pillars.html"><i class="fa fa-check"></i><b>8.3</b> Three pillars of data analysis</a></li>
<li class="chapter" data-level="8.4" data-path="Chap-03-03-models-representation.html"><a href="Chap-03-03-models-representation.html"><i class="fa fa-check"></i><b>8.4</b> Notation &amp; graphical representation</a><ul>
<li class="chapter" data-level="8.4.1" data-path="Chap-03-03-models-representation.html"><a href="Chap-03-03-models-representation.html#formula-notation"><i class="fa fa-check"></i><b>8.4.1</b> Formula notation</a></li>
<li class="chapter" data-level="8.4.2" data-path="Chap-03-03-models-representation.html"><a href="Chap-03-03-models-representation.html#graphical-notation"><i class="fa fa-check"></i><b>8.4.2</b> Graphical notation</a></li>
<li class="chapter" data-level="8.4.3" data-path="Chap-03-03-models-representation.html"><a href="Chap-03-03-models-representation.html#multiple-observations"><i class="fa fa-check"></i><b>8.4.3</b> Multiple observations</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="Chap-03-03-models-examples.html"><a href="Chap-03-03-models-examples.html"><i class="fa fa-check"></i><b>8.5</b> Strolling the zoo of models</a><ul>
<li class="chapter" data-level="8.5.1" data-path="Chap-03-03-models-examples.html"><a href="Chap-03-03-models-examples.html#Chap-03-03-models-examples-binomial"><i class="fa fa-check"></i><b>8.5.1</b> The Binomial Model</a></li>
<li class="chapter" data-level="8.5.2" data-path="Chap-03-03-models-examples.html"><a href="Chap-03-03-models-examples.html#flip-and-draw-model"><i class="fa fa-check"></i><b>8.5.2</b> Flip-and-Draw Model</a></li>
<li class="chapter" data-level="8.5.3" data-path="Chap-03-03-models-examples.html"><a href="Chap-03-03-models-examples.html#flip-and-draw-hypergeometric-model"><i class="fa fa-check"></i><b>8.5.3</b> Flip-and-Draw-Hypergeometric Model</a></li>
<li class="chapter" data-level="8.5.4" data-path="Chap-03-03-models-examples.html"><a href="Chap-03-03-models-examples.html#t-test-model-comparing-two-groups"><i class="fa fa-check"></i><b>8.5.4</b> T-Test Model: comparing two groups</a></li>
<li class="chapter" data-level="8.5.5" data-path="Chap-03-03-models-examples.html"><a href="Chap-03-03-models-examples.html#simple-linear-regression-model"><i class="fa fa-check"></i><b>8.5.5</b> Simple Linear Regression Model</a></li>
<li class="chapter" data-level="8.5.6" data-path="Chap-03-03-models-examples.html"><a href="Chap-03-03-models-examples.html#linear-regression-with-two-groups"><i class="fa fa-check"></i><b>8.5.6</b> Linear Regression with Two Groups</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="Chap-03-03-models-hypotheses.html"><a href="Chap-03-03-models-hypotheses.html"><i class="fa fa-check"></i><b>8.6</b> Expressing hypotheses with models</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-03-04-parameter-inference.html"><a href="ch-03-04-parameter-inference.html"><i class="fa fa-check"></i><b>9</b> Parameter estimation</a><ul>
<li class="chapter" data-level="9.1" data-path="ch-03-03-estimation-bayes.html"><a href="ch-03-03-estimation-bayes.html"><i class="fa fa-check"></i><b>9.1</b> Bayes rule of parameter estimation</a><ul>
<li class="chapter" data-level="9.1.1" data-path="ch-03-03-estimation-bayes.html"><a href="ch-03-03-estimation-bayes.html#the-effects-of-prior-and-likelihood-on-the-posterior"><i class="fa fa-check"></i><b>9.1.1</b> The effects of prior and likelihood on the posterior</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-03-03-estimation-bayes.html"><a href="ch-03-03-estimation-bayes.html#posterior-means-and-credible-intervals"><i class="fa fa-check"></i><b>9.1.2</b> Posterior means and credible intervals</a></li>
<li class="chapter" data-level="9.1.3" data-path="ch-03-03-estimation-bayes.html"><a href="ch-03-03-estimation-bayes.html#computing-bayesian-posteriors-with-conjugate-priors"><i class="fa fa-check"></i><b>9.1.3</b> Computing Bayesian posteriors with conjugate priors</a></li>
<li class="chapter" data-level="9.1.4" data-path="ch-03-03-estimation-bayes.html"><a href="ch-03-03-estimation-bayes.html#sequential-updating"><i class="fa fa-check"></i><b>9.1.4</b> Sequential updating</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-03-03-estimation-frequentist.html"><a href="ch-03-03-estimation-frequentist.html"><i class="fa fa-check"></i><b>9.2</b> A frequentist approach to parameter estimation</a><ul>
<li class="chapter" data-level="9.2.1" data-path="ch-03-03-estimation-frequentist.html"><a href="ch-03-03-estimation-frequentist.html#maximum-likelihood-estimate"><i class="fa fa-check"></i><b>9.2.1</b> Maximum likelihood estimate</a></li>
<li class="chapter" data-level="9.2.2" data-path="ch-03-03-estimation-frequentist.html"><a href="ch-03-03-estimation-frequentist.html#confidence-intervals"><i class="fa fa-check"></i><b>9.2.2</b> Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ch-03-03-estimation-testing.html"><a href="ch-03-03-estimation-testing.html"><i class="fa fa-check"></i><b>9.3</b> Addressing point-valued hypotheses with parameter estimation</a></li>
<li class="chapter" data-level="9.4" data-path="ch-03-03-estimation-comparison.html"><a href="ch-03-03-estimation-comparison.html"><i class="fa fa-check"></i><b>9.4</b> Comparing Bayesian and frequentist estimates</a></li>
<li class="chapter" data-level="9.5" data-path="Ch-03-03-estimation-algorithms.html"><a href="Ch-03-03-estimation-algorithms.html"><i class="fa fa-check"></i><b>9.5</b> Algorithms for parameter estimation</a><ul>
<li class="chapter" data-level="9.5.1" data-path="Ch-03-03-estimation-algorithms.html"><a href="Ch-03-03-estimation-algorithms.html#optimizing-functions"><i class="fa fa-check"></i><b>9.5.1</b> Optimizing functions</a></li>
<li class="chapter" data-level="9.5.2" data-path="Ch-03-03-estimation-algorithms.html"><a href="Ch-03-03-estimation-algorithms.html#approximating-posterior-distributions"><i class="fa fa-check"></i><b>9.5.2</b> Approximating posterior distributions</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="ch-03-03-estimation-greta.html"><a href="ch-03-03-estimation-greta.html"><i class="fa fa-check"></i><b>9.6</b> Probabilistic modeling with <code>greta</code></a><ul>
<li class="chapter" data-level="9.6.1" data-path="ch-03-03-estimation-greta.html"><a href="ch-03-03-estimation-greta.html#basics-of-greta"><i class="fa fa-check"></i><b>9.6.1</b> Basics of <code>greta</code></a></li>
<li class="chapter" data-level="9.6.2" data-path="ch-03-03-estimation-greta.html"><a href="ch-03-03-estimation-greta.html#binomial-model"><i class="fa fa-check"></i><b>9.6.2</b> Binomial Model</a></li>
<li class="chapter" data-level="9.6.3" data-path="ch-03-03-estimation-greta.html"><a href="ch-03-03-estimation-greta.html#t-test-model-for-mental-chronometry"><i class="fa fa-check"></i><b>9.6.3</b> T-Test Model for Mental Chronometry</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-03-05-hypothesis-testing.html"><a href="ch-03-05-hypothesis-testing.html"><i class="fa fa-check"></i><b>10</b> Hypothesis Testing</a><ul>
<li class="chapter" data-level="10.1" data-path="ch-03-05-hypothesis-p-values.html"><a href="ch-03-05-hypothesis-p-values.html"><i class="fa fa-check"></i><b>10.1</b> <em>p</em>-values</a><ul>
<li class="chapter" data-level="10.1.1" data-path="ch-03-05-hypothesis-p-values.html"><a href="ch-03-05-hypothesis-p-values.html#binomial-model---frequentist-version"><i class="fa fa-check"></i><b>10.1.1</b> Binomial Model - frequentist version</a></li>
<li class="chapter" data-level="10.1.2" data-path="ch-03-05-hypothesis-p-values.html"><a href="ch-03-05-hypothesis-p-values.html#p-values-for-the-binomial-model"><i class="fa fa-check"></i><b>10.1.2</b> <em>p</em>-values for the Binomial Model</a></li>
<li class="chapter" data-level="10.1.3" data-path="ch-03-05-hypothesis-p-values.html"><a href="ch-03-05-hypothesis-p-values.html#statistical-significance"><i class="fa fa-check"></i><b>10.1.3</b> Statistical significance</a></li>
<li class="chapter" data-level="10.1.4" data-path="ch-03-05-hypothesis-p-values.html"><a href="ch-03-05-hypothesis-p-values.html#p-values-and-alpha-errors"><i class="fa fa-check"></i><b>10.1.4</b> <em>p</em>-values and <span class="math inline">\(\alpha\)</span>-errors</a></li>
<li class="chapter" data-level="10.1.5" data-path="ch-03-05-hypothesis-p-values.html"><a href="ch-03-05-hypothesis-p-values.html#relation-of-p-values-to-confidence-intervals"><i class="fa fa-check"></i><b>10.1.5</b> Relation of <em>p</em>-values to confidence intervals</a></li>
<li class="chapter" data-level="10.1.6" data-path="ch-03-05-hypothesis-p-values.html"><a href="ch-03-05-hypothesis-p-values.html#distribution-of-p-values"><i class="fa fa-check"></i><b>10.1.6</b> Distribution of <span class="math inline">\(p\)</span>-values</a></li>
<li class="chapter" data-level="10.1.7" data-path="ch-03-05-hypothesis-p-values.html"><a href="ch-03-05-hypothesis-p-values.html#how-not-to-interpret-p-values"><i class="fa fa-check"></i><b>10.1.7</b> How (not) to interpret <em>p</em>-values</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="ch-03-05-hypothesis-testing-CLT.html"><a href="ch-03-05-hypothesis-testing-CLT.html"><i class="fa fa-check"></i><b>10.2</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="10.3" data-path="ch-03-05-hypothesis-testing-tests.html"><a href="ch-03-05-hypothesis-testing-tests.html"><i class="fa fa-check"></i><b>10.3</b> Selected tests</a><ul>
<li class="chapter" data-level="10.3.1" data-path="ch-03-05-hypothesis-testing-tests.html"><a href="ch-03-05-hypothesis-testing-tests.html#ch-03-05-hypothesis-testing-Pearsons-Chi"><i class="fa fa-check"></i><b>10.3.1</b> Pearson’s <span class="math inline">\(\chi^2\)</span>-tests</a></li>
<li class="chapter" data-level="10.3.2" data-path="ch-03-05-hypothesis-testing-tests.html"><a href="ch-03-05-hypothesis-testing-tests.html#ch-03-05-hypothesis-testing-z-test"><i class="fa fa-check"></i><b>10.3.2</b> <em>z</em>-test</a></li>
<li class="chapter" data-level="10.3.3" data-path="ch-03-05-hypothesis-testing-tests.html"><a href="ch-03-05-hypothesis-testing-tests.html#ch-03-05-hypothesis-testing-t-test"><i class="fa fa-check"></i><b>10.3.3</b> <em>t</em>-tests</a></li>
<li class="chapter" data-level="10.3.4" data-path="ch-03-05-hypothesis-testing-tests.html"><a href="ch-03-05-hypothesis-testing-tests.html#ch-03-05-hypothesis-testing-ANOVA"><i class="fa fa-check"></i><b>10.3.4</b> ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="ch-03-05-hypothesis-testing-3-approaches.html"><a href="ch-03-05-hypothesis-testing-3-approaches.html"><i class="fa fa-check"></i><b>10.4</b> Three approaches</a><ul>
<li class="chapter" data-level="10.4.1" data-path="ch-03-05-hypothesis-testing-3-approaches.html"><a href="ch-03-05-hypothesis-testing-3-approaches.html#fisher"><i class="fa fa-check"></i><b>10.4.1</b> Fisher</a></li>
<li class="chapter" data-level="10.4.2" data-path="ch-03-05-hypothesis-testing-3-approaches.html"><a href="ch-03-05-hypothesis-testing-3-approaches.html#neyman-pearson"><i class="fa fa-check"></i><b>10.4.2</b> Neyman-Pearson</a></li>
<li class="chapter" data-level="10.4.3" data-path="ch-03-05-hypothesis-testing-3-approaches.html"><a href="ch-03-05-hypothesis-testing-3-approaches.html#hypbrid-modern-nhst"><i class="fa fa-check"></i><b>10.4.3</b> Hypbrid modern NHST</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ch-03-05-hypothesis-testing-3-model-checking.html"><a href="ch-03-05-hypothesis-testing-3-model-checking.html"><i class="fa fa-check"></i><b>10.5</b> Relation to model checking Section</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="Chap-03-06-model-comparison.html"><a href="Chap-03-06-model-comparison.html"><i class="fa fa-check"></i><b>11</b> Model Comparison</a><ul>
<li class="chapter" data-level="11.1" data-path="Chap-03-06-model-comparison-case-study.html"><a href="Chap-03-06-model-comparison-case-study.html"><i class="fa fa-check"></i><b>11.1</b> Case study: recall models</a></li>
<li class="chapter" data-level="11.2" data-path="Chap-03-06-model-comparison-AIC.html"><a href="Chap-03-06-model-comparison-AIC.html"><i class="fa fa-check"></i><b>11.2</b> Akaike Information Criterion</a></li>
<li class="chapter" data-level="11.3" data-path="Chap-03-06-model-comparison-LR-test.html"><a href="Chap-03-06-model-comparison-LR-test.html"><i class="fa fa-check"></i><b>11.3</b> Likelihood-Ratio Test</a></li>
<li class="chapter" data-level="11.4" data-path="Chap-03-06-model-comparison-BF.html"><a href="Chap-03-06-model-comparison-BF.html"><i class="fa fa-check"></i><b>11.4</b> Bayes factors</a><ul>
<li class="chapter" data-level="11.4.1" data-path="Chap-03-06-model-comparison-BF.html"><a href="Chap-03-06-model-comparison-BF.html#grid-approximation"><i class="fa fa-check"></i><b>11.4.1</b> Grid approximation</a></li>
<li class="chapter" data-level="11.4.2" data-path="Chap-03-06-model-comparison-BF.html"><a href="Chap-03-06-model-comparison-BF.html#naive-monte-carlo"><i class="fa fa-check"></i><b>11.4.2</b> Naive Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="outlook.html"><a href="outlook.html"><i class="fa fa-check"></i><b>11.5</b> Outlook</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-03-07-hypothesis-testing-Bayes.html"><a href="ch-03-07-hypothesis-testing-Bayes.html"><i class="fa fa-check"></i><b>12</b> Bayesian hypothesis testing</a><ul>
<li class="chapter" data-level="12.1" data-path="data-and-models-for-this-chapter.html"><a href="data-and-models-for-this-chapter.html"><i class="fa fa-check"></i><b>12.1</b> Data and models for this chapter</a><ul>
<li class="chapter" data-level="12.1.1" data-path="data-and-models-for-this-chapter.html"><a href="data-and-models-for-this-chapter.html#section"><i class="fa fa-check"></i><b>12.1.1</b> 24/7</a></li>
<li class="chapter" data-level="12.1.2" data-path="data-and-models-for-this-chapter.html"><a href="data-and-models-for-this-chapter.html#eco-sensitivity-fictitious"><i class="fa fa-check"></i><b>12.1.2</b> Eco-sensitivity (fictitious)</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="testing-as-posterior-estimation.html"><a href="testing-as-posterior-estimation.html"><i class="fa fa-check"></i><b>12.2</b> Testing as posterior estimation</a><ul>
<li class="chapter" data-level="12.2.1" data-path="testing-as-posterior-estimation.html"><a href="testing-as-posterior-estimation.html#example-247"><i class="fa fa-check"></i><b>12.2.1</b> Example: 24/7</a></li>
<li class="chapter" data-level="12.2.2" data-path="testing-as-posterior-estimation.html"><a href="testing-as-posterior-estimation.html#example-eco-sensitivity"><i class="fa fa-check"></i><b>12.2.2</b> Example: Eco-sensitivity</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="the-savage-dickey-method.html"><a href="the-savage-dickey-method.html"><i class="fa fa-check"></i><b>12.3</b> The Savage-Dickey method</a><ul>
<li class="chapter" data-level="12.3.1" data-path="the-savage-dickey-method.html"><a href="the-savage-dickey-method.html#nested-bayesian-models"><i class="fa fa-check"></i><b>12.3.1</b> Nested (Bayesian) models</a></li>
<li class="chapter" data-level="12.3.2" data-path="the-savage-dickey-method.html"><a href="the-savage-dickey-method.html#savage-dickey-theorem"><i class="fa fa-check"></i><b>12.3.2</b> Savage-Dickey theorem</a></li>
<li class="chapter" data-level="12.3.3" data-path="the-savage-dickey-method.html"><a href="the-savage-dickey-method.html#example-247-1"><i class="fa fa-check"></i><b>12.3.3</b> Example: 24/7</a></li>
<li class="chapter" data-level="12.3.4" data-path="the-savage-dickey-method.html"><a href="the-savage-dickey-method.html#example-eco-sensitivity-1"><i class="fa fa-check"></i><b>12.3.4</b> Example: eco-sensitivity</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="bayes-factors-for-rope-d-hypotheses-through-encompassing-models.html"><a href="bayes-factors-for-rope-d-hypotheses-through-encompassing-models.html"><i class="fa fa-check"></i><b>12.4</b> Bayes factors for ROPE-d hypotheses through encompassing models</a><ul>
<li class="chapter" data-level="12.4.1" data-path="bayes-factors-for-rope-d-hypotheses-through-encompassing-models.html"><a href="bayes-factors-for-rope-d-hypotheses-through-encompassing-models.html#example-247-2"><i class="fa fa-check"></i><b>12.4.1</b> Example: 24/7</a></li>
<li class="chapter" data-level="12.4.2" data-path="bayes-factors-for-rope-d-hypotheses-through-encompassing-models.html"><a href="bayes-factors-for-rope-d-hypotheses-through-encompassing-models.html#example-eco-sensitivity-2"><i class="fa fa-check"></i><b>12.4.2</b> Example: eco-sensitivity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap-03-08-model-criticism.html"><a href="Chap-03-08-model-criticism.html"><i class="fa fa-check"></i><b>13</b> Model criticism</a></li>
<li class="part"><span><b>IV Appliied (generalized) linear modeling</b></span></li>
<li class="chapter" data-level="14" data-path="Chap-04-01-simple-linear-regression.html"><a href="Chap-04-01-simple-linear-regression.html"><i class="fa fa-check"></i><b>14</b> Simple linear regression</a><ul>
<li class="chapter" data-level="14.1" data-path="data-set-murder-data.html"><a href="data-set-murder-data.html"><i class="fa fa-check"></i><b>14.1</b> Data set: murder data</a></li>
<li class="chapter" data-level="14.2" data-path="what-is-a-simple-linear-regression.html"><a href="what-is-a-simple-linear-regression.html"><i class="fa fa-check"></i><b>14.2</b> What is a (simple) linear regression?</a><ul>
<li class="chapter" data-level="14.2.1" data-path="what-is-a-simple-linear-regression.html"><a href="what-is-a-simple-linear-regression.html#prediction-without-any-further-information"><i class="fa fa-check"></i><b>14.2.1</b> Prediction without any further information</a></li>
<li class="chapter" data-level="14.2.2" data-path="what-is-a-simple-linear-regression.html"><a href="what-is-a-simple-linear-regression.html#prediction-with-knowledge-of-unemployment-rate"><i class="fa fa-check"></i><b>14.2.2</b> Prediction with knowledge of unemployment rate</a></li>
<li class="chapter" data-level="14.2.3" data-path="what-is-a-simple-linear-regression.html"><a href="what-is-a-simple-linear-regression.html#simple-linear-regression-general-problem-formulation"><i class="fa fa-check"></i><b>14.2.3</b> Simple linear regression: general problem formulation</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="ordinary-least-squares-regression.html"><a href="ordinary-least-squares-regression.html"><i class="fa fa-check"></i><b>14.3</b> Ordinary least-squares regression</a><ul>
<li class="chapter" data-level="14.3.1" data-path="ordinary-least-squares-regression.html"><a href="ordinary-least-squares-regression.html#finding-optimal-parameters-with-optim"><i class="fa fa-check"></i><b>14.3.1</b> Finding optimal parameters with <code>optim</code></a></li>
<li class="chapter" data-level="14.3.2" data-path="ordinary-least-squares-regression.html"><a href="ordinary-least-squares-regression.html#fitting-ols-regression-lines-with-lm"><i class="fa fa-check"></i><b>14.3.2</b> Fitting OLS regression lines with <code>lm</code></a></li>
<li class="chapter" data-level="14.3.3" data-path="ordinary-least-squares-regression.html"><a href="ordinary-least-squares-regression.html#finding-optimal-parameter-values-with-math"><i class="fa fa-check"></i><b>14.3.3</b> Finding optimal parameter values with math</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="a-maximim-likelihood-approach.html"><a href="a-maximim-likelihood-approach.html"><i class="fa fa-check"></i><b>14.4</b> A maximim-likelihood approach</a></li>
<li class="chapter" data-level="14.5" data-path="a-bayesian-approach.html"><a href="a-bayesian-approach.html"><i class="fa fa-check"></i><b>14.5</b> A Bayesian approach</a></li>
<li class="chapter" data-level="14.6" data-path="testing-coefficients.html"><a href="testing-coefficients.html"><i class="fa fa-check"></i><b>14.6</b> Testing coefficients</a></li>
<li class="chapter" data-level="14.7" data-path="snippets.html"><a href="snippets.html"><i class="fa fa-check"></i><b>14.7</b> Snippets</a><ul>
<li class="chapter" data-level="14.7.1" data-path="snippets.html"><a href="snippets.html#preliminaries"><i class="fa fa-check"></i><b>14.7.1</b> Preliminaries</a></li>
<li class="chapter" data-level="14.7.2" data-path="snippets.html"><a href="snippets.html#definitions"><i class="fa fa-check"></i><b>14.7.2</b> Definitions</a></li>
<li class="chapter" data-level="14.7.3" data-path="snippets.html"><a href="snippets.html#theorem-1"><i class="fa fa-check"></i><b>14.7.3</b> Theorem 1</a></li>
<li class="chapter" data-level="14.7.4" data-path="snippets.html"><a href="snippets.html#theorem-2"><i class="fa fa-check"></i><b>14.7.4</b> Theorem 2</a></li>
<li class="chapter" data-level="14.7.5" data-path="snippets.html"><a href="snippets.html#theorem-3"><i class="fa fa-check"></i><b>14.7.5</b> Theorem 3</a></li>
<li class="chapter" data-level="14.7.6" data-path="snippets.html"><a href="snippets.html#theorem-4"><i class="fa fa-check"></i><b>14.7.6</b> Theorem 4</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>15</b> Logistic regression</a><ul>
<li class="chapter" data-level="15.0.1" data-path="logistic-regression.html"><a href="logistic-regression.html#beta-binomial-model---one-group-revisited"><i class="fa fa-check"></i><b>15.0.1</b> Beta-Binomial model - one group (revisited)</a></li>
<li class="chapter" data-level="15.0.2" data-path="logistic-regression.html"><a href="logistic-regression.html#beta-binomial-model---two-groups-revisited"><i class="fa fa-check"></i><b>15.0.2</b> Beta-Binomial model - two groups (revisited)</a></li>
<li class="chapter" data-level="15.0.3" data-path="logistic-regression.html"><a href="logistic-regression.html#simple-linear-regression-model-revisited"><i class="fa fa-check"></i><b>15.0.3</b> Simple linear regression model (revisited)</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="multinomial-regression.html"><a href="multinomial-regression.html"><i class="fa fa-check"></i><b>16</b> Multinomial regression</a></li>
<li class="chapter" data-level="17" data-path="ordinal-regression.html"><a href="ordinal-regression.html"><i class="fa fa-check"></i><b>17</b> Ordinal regression</a></li>
<li class="chapter" data-level="18" data-path="ch-05-05-hierarchical-modeling.html"><a href="ch-05-05-hierarchical-modeling.html"><i class="fa fa-check"></i><b>18</b> Hierarchical regression</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="app-90-further-material.html"><a href="app-90-further-material.html"><i class="fa fa-check"></i><b>A</b> Further useful material</a><ul>
<li class="chapter" data-level="A.1" data-path="material-on-introduction-to-probability.html"><a href="material-on-introduction-to-probability.html"><i class="fa fa-check"></i><b>A.1</b> Material on <em>Introduction to Probability</em>:</a></li>
<li class="chapter" data-level="A.2" data-path="material-on-bayesian-data-analysis.html"><a href="material-on-bayesian-data-analysis.html"><i class="fa fa-check"></i><b>A.2</b> Material on <em>Bayesian Data Analysis</em>:</a></li>
<li class="chapter" data-level="A.3" data-path="material-on-frequentist-statistics.html"><a href="material-on-frequentist-statistics.html"><i class="fa fa-check"></i><b>A.3</b> Material on <em>frequentist statistics</em>:</a></li>
<li class="chapter" data-level="A.4" data-path="material-on-r-tidyverse-etc-.html"><a href="material-on-r-tidyverse-etc-.html"><i class="fa fa-check"></i><b>A.4</b> Material on <em>R, tidyverse, etc.</em>:</a></li>
<li class="chapter" data-level="A.5" data-path="further-information-for-rstudio.html"><a href="further-information-for-rstudio.html"><i class="fa fa-check"></i><b>A.5</b> Further information for RStudio</a></li>
<li class="chapter" data-level="A.6" data-path="resources-on-webppl.html"><a href="resources-on-webppl.html"><i class="fa fa-check"></i><b>A.6</b> Resources on WebPPL</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="app-91-distributions.html"><a href="app-91-distributions.html"><i class="fa fa-check"></i><b>B</b> Common probability distributions</a><ul>
<li class="chapter" data-level="B.1" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html"><i class="fa fa-check"></i><b>B.1</b> Selected continuous distributions of random variables</a><ul>
<li class="chapter" data-level="B.1.1" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-normal"><i class="fa fa-check"></i><b>B.1.1</b> Normal distribution</a></li>
<li class="chapter" data-level="B.1.2" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-chi2"><i class="fa fa-check"></i><b>B.1.2</b> Chi-squared distribution</a></li>
<li class="chapter" data-level="B.1.3" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html#f-distribution"><i class="fa fa-check"></i><b>B.1.3</b> F distribution</a></li>
<li class="chapter" data-level="B.1.4" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-students-t"><i class="fa fa-check"></i><b>B.1.4</b> Student’s <em>t</em>-distribution</a></li>
<li class="chapter" data-level="B.1.5" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-beta"><i class="fa fa-check"></i><b>B.1.5</b> Beta distribution</a></li>
<li class="chapter" data-level="B.1.6" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html#uniform-distribution"><i class="fa fa-check"></i><b>B.1.6</b> Uniform distribution</a></li>
<li class="chapter" data-level="B.1.7" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-dirichlet"><i class="fa fa-check"></i><b>B.1.7</b> Dirichlet distribution</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="selected-discrete-distributions-of-random-variables.html"><a href="selected-discrete-distributions-of-random-variables.html"><i class="fa fa-check"></i><b>B.2</b> Selected discrete distributions of random variables</a><ul>
<li class="chapter" data-level="B.2.1" data-path="selected-discrete-distributions-of-random-variables.html"><a href="selected-discrete-distributions-of-random-variables.html#app-91-distributions-binomial"><i class="fa fa-check"></i><b>B.2.1</b> Binomial distribution</a></li>
<li class="chapter" data-level="B.2.2" data-path="selected-discrete-distributions-of-random-variables.html"><a href="selected-discrete-distributions-of-random-variables.html#app-91-distributions-multinomial"><i class="fa fa-check"></i><b>B.2.2</b> Multinomial distribution</a></li>
<li class="chapter" data-level="B.2.3" data-path="selected-discrete-distributions-of-random-variables.html"><a href="selected-discrete-distributions-of-random-variables.html#app-91-distributions-bernoulli"><i class="fa fa-check"></i><b>B.2.3</b> Bernoulli distribution</a></li>
<li class="chapter" data-level="B.2.4" data-path="selected-discrete-distributions-of-random-variables.html"><a href="selected-discrete-distributions-of-random-variables.html#app-91-distributions-categorical"><i class="fa fa-check"></i><b>B.2.4</b> Categorical distribution</a></li>
<li class="chapter" data-level="B.2.5" data-path="selected-discrete-distributions-of-random-variables.html"><a href="selected-discrete-distributions-of-random-variables.html#app-91-distributions-beta-binomial"><i class="fa fa-check"></i><b>B.2.5</b> Beta-Binomial distribution</a></li>
<li class="chapter" data-level="B.2.6" data-path="selected-discrete-distributions-of-random-variables.html"><a href="selected-discrete-distributions-of-random-variables.html#poisson-distribution"><i class="fa fa-check"></i><b>B.2.6</b> Poisson distribution</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="understanding-distributions-as-random-variables.html"><a href="understanding-distributions-as-random-variables.html"><i class="fa fa-check"></i><b>B.3</b> Understanding distributions as random variables</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="app-92-exponential-family.html"><a href="app-92-exponential-family.html"><i class="fa fa-check"></i><b>C</b> Exponential Family and Maximum Entropy</a><ul>
<li class="chapter" data-level="C.1" data-path="an-important-family-the-exponential-family.html"><a href="an-important-family-the-exponential-family.html"><i class="fa fa-check"></i><b>C.1</b> An important family: The Exponential Family</a></li>
<li class="chapter" data-level="C.2" data-path="excursos-information-entropy-and-maximum-entropy-principal.html"><a href="excursos-information-entropy-and-maximum-entropy-principal.html"><i class="fa fa-check"></i><b>C.2</b> Excursos: “Information Entropy” and “Maximum Entropy Principal”</a><ul>
<li class="chapter" data-level="C.2.1" data-path="excursos-information-entropy-and-maximum-entropy-principal.html"><a href="excursos-information-entropy-and-maximum-entropy-principal.html#information-entropy"><i class="fa fa-check"></i><b>C.2.1</b> Information Entropy</a></li>
<li class="chapter" data-level="C.2.2" data-path="excursos-information-entropy-and-maximum-entropy-principal.html"><a href="excursos-information-entropy-and-maximum-entropy-principal.html#deriving-probability-distributions-using-the-maximum-entropy-principle"><i class="fa fa-check"></i><b>C.2.2</b> Deriving Probability Distributions using the Maximum Entropy Principle</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="D" data-path="app-93-data-sets.html"><a href="app-93-data-sets.html"><i class="fa fa-check"></i><b>D</b> Data sets used in the book</a><ul>
<li class="chapter" data-level="D.1" data-path="app-93-data-sets-mental-chronometry.html"><a href="app-93-data-sets-mental-chronometry.html"><i class="fa fa-check"></i><b>D.1</b> Mental Chronometry</a><ul>
<li class="chapter" data-level="D.1.1" data-path="app-93-data-sets-mental-chronometry.html"><a href="app-93-data-sets-mental-chronometry.html#nature-origin-and-rationale-of-the-data"><i class="fa fa-check"></i><b>D.1.1</b> Nature, origin and rationale of the data</a></li>
<li class="chapter" data-level="D.1.2" data-path="app-93-data-sets-mental-chronometry.html"><a href="app-93-data-sets-mental-chronometry.html#loading-and-preprocessing-the-data"><i class="fa fa-check"></i><b>D.1.2</b> Loading and preprocessing the data</a></li>
<li class="chapter" data-level="D.1.3" data-path="app-93-data-sets-mental-chronometry.html"><a href="app-93-data-sets-mental-chronometry.html#cleaning-the-data-1"><i class="fa fa-check"></i><b>D.1.3</b> Cleaning the data</a></li>
<li class="chapter" data-level="D.1.4" data-path="app-93-data-sets-mental-chronometry.html"><a href="app-93-data-sets-mental-chronometry.html#exploration-summary-stats-plots"><i class="fa fa-check"></i><b>D.1.4</b> Exploration: summary stats &amp; plots</a></li>
<li class="chapter" data-level="D.1.5" data-path="app-93-data-sets-mental-chronometry.html"><a href="app-93-data-sets-mental-chronometry.html#data-analysis"><i class="fa fa-check"></i><b>D.1.5</b> Data analysis</a></li>
</ul></li>
<li class="chapter" data-level="D.2" data-path="simon-task.html"><a href="simon-task.html"><i class="fa fa-check"></i><b>D.2</b> Simon Task</a><ul>
<li class="chapter" data-level="D.2.1" data-path="simon-task.html"><a href="simon-task.html#experiment"><i class="fa fa-check"></i><b>D.2.1</b> Experiment</a></li>
<li class="chapter" data-level="D.2.2" data-path="simon-task.html"><a href="simon-task.html#results"><i class="fa fa-check"></i><b>D.2.2</b> Results</a></li>
<li class="chapter" data-level="D.2.3" data-path="simon-task.html"><a href="simon-task.html#analysis"><i class="fa fa-check"></i><b>D.2.3</b> Analysis</a></li>
</ul></li>
<li class="chapter" data-level="D.3" data-path="world-values-survey-wave-6-2010-2014.html"><a href="world-values-survey-wave-6-2010-2014.html"><i class="fa fa-check"></i><b>D.3</b> World Values Survey (wave 6 | 2010-2014)</a><ul>
<li class="chapter" data-level="D.3.1" data-path="world-values-survey-wave-6-2010-2014.html"><a href="world-values-survey-wave-6-2010-2014.html#nature-origin-and-rationale-of-the-data-1"><i class="fa fa-check"></i><b>D.3.1</b> Nature, origin and rationale of the data</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="app-93-data-sets-king-of-france.html"><a href="app-93-data-sets-king-of-france.html"><i class="fa fa-check"></i><b>D.4</b> King of France</a><ul>
<li class="chapter" data-level="D.4.1" data-path="app-93-data-sets-king-of-france.html"><a href="app-93-data-sets-king-of-france.html#app-93-data-sets-king-of-france-background"><i class="fa fa-check"></i><b>D.4.1</b> Nature, origin and rationale of the data</a></li>
<li class="chapter" data-level="D.4.2" data-path="app-93-data-sets-king-of-france.html"><a href="app-93-data-sets-king-of-france.html#loading-and-preprocessing-the-data-1"><i class="fa fa-check"></i><b>D.4.2</b> Loading and preprocessing the data</a></li>
<li class="chapter" data-level="D.4.3" data-path="app-93-data-sets-king-of-france.html"><a href="app-93-data-sets-king-of-france.html#cleaning-the-data-2"><i class="fa fa-check"></i><b>D.4.3</b> Cleaning the data</a></li>
<li class="chapter" data-level="D.4.4" data-path="app-93-data-sets-king-of-france.html"><a href="app-93-data-sets-king-of-france.html#exploration-summary-stats-plots-1"><i class="fa fa-check"></i><b>D.4.4</b> Exploration: summary stats &amp; plots</a></li>
<li class="chapter" data-level="D.4.5" data-path="app-93-data-sets-king-of-france.html"><a href="app-93-data-sets-king-of-france.html#data-analysis-1"><i class="fa fa-check"></i><b>D.4.5</b> Data analysis</a></li>
</ul></li>
<li class="chapter" data-level="D.5" data-path="app-93-data-sets-BLJM.html"><a href="app-93-data-sets-BLJM.html"><i class="fa fa-check"></i><b>D.5</b> Bio-Logic Jazz-Metal (and where to consume it)</a><ul>
<li class="chapter" data-level="D.5.1" data-path="app-93-data-sets-BLJM.html"><a href="app-93-data-sets-BLJM.html#nature-origin-and-rationale-of-the-data-2"><i class="fa fa-check"></i><b>D.5.1</b> Nature, origin and rationale of the data</a></li>
<li class="chapter" data-level="D.5.2" data-path="app-93-data-sets-BLJM.html"><a href="app-93-data-sets-BLJM.html#loading-and-preprocessing-the-data-2"><i class="fa fa-check"></i><b>D.5.2</b> Loading and preprocessing the data</a></li>
<li class="chapter" data-level="D.5.3" data-path="app-93-data-sets-BLJM.html"><a href="app-93-data-sets-BLJM.html#exploration-counts-plots"><i class="fa fa-check"></i><b>D.5.3</b> Exploration: counts &amp; plots</a></li>
</ul></li>
<li class="chapter" data-level="D.6" data-path="app-93-data-sets-avocado.html"><a href="app-93-data-sets-avocado.html"><i class="fa fa-check"></i><b>D.6</b> Avocado prices</a><ul>
<li class="chapter" data-level="D.6.1" data-path="app-93-data-sets-avocado.html"><a href="app-93-data-sets-avocado.html#nature-origin-and-rationale-of-the-data-3"><i class="fa fa-check"></i><b>D.6.1</b> Nature, origin and rationale of the data</a></li>
<li class="chapter" data-level="D.6.2" data-path="app-93-data-sets-avocado.html"><a href="app-93-data-sets-avocado.html#loading-and-preprocessing-the-data-3"><i class="fa fa-check"></i><b>D.6.2</b> Loading and preprocessing the data</a></li>
<li class="chapter" data-level="D.6.3" data-path="app-93-data-sets-avocado.html"><a href="app-93-data-sets-avocado.html#summary-statistics"><i class="fa fa-check"></i><b>D.6.3</b> Summary statistics</a></li>
<li class="chapter" data-level="D.6.4" data-path="app-93-data-sets-avocado.html"><a href="app-93-data-sets-avocado.html#plots"><i class="fa fa-check"></i><b>D.6.4</b> Plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="snippets" class="section level2">
<h2><span class="header-section-number">14.7</span> Snippets</h2>
<div id="preliminaries" class="section level3">
<h3><span class="header-section-number">14.7.1</span> Preliminaries</h3>
<div class="grey">
<ul>
<li><em>data</em> for simple linear regression:
<ul>
<li>single dependent variable <span class="math inline">\(y\)</span> with observed data points <span class="math inline">\(i: y_1,..., y_n\)</span></li>
<li><span class="math inline">\(k\)</span> predictor variables <span class="math inline">\(x_1, ..., x_k\)</span> and observations <span class="math inline">\(x_{j1}, ..., x_{jn}\)</span> for each <span class="math inline">\(x_j\)</span></li>
</ul></li>
<li>we consider a <em>regression model</em>:
<ul>
<li><span class="math inline">\(y_i \sim Normal(\beta_0 + \beta_1 x_{1i} + ... + \beta_k x_{ki}, \sigma)\)</span></li>
<li><span class="math inline">\(\beta_j\)</span> and <span class="math inline">\(\sigma\)</span> are free parameters, as usual</li>
</ul></li>
<li>further notation:
<ul>
<li><span class="math inline">\(\bar{y} = \frac{1}{n} \sum_{i=1}^n y_i\)</span> - the mean of data observations</li>
<li><span class="math inline">\(\hat{y}_i = \hat{\beta_0} + \hat{\beta_1} x_1i + ... + \hat{\beta_k} x_{ki}\)</span> - prediction for i-th data point for best fitting parameter values</li>
</ul></li>
</ul>
</div>
<hr />
</div>
<div id="definitions" class="section level3">
<h3><span class="header-section-number">14.7.2</span> Definitions</h3>
<div class="grey">
<p><strong>Total sum of squares:</strong> <span class="math display">\[TSS = \sum_{i=1}^n (y_i - \bar{y})^2 \tag{0.1}\]</span>
<strong>Explained sum of squares:</strong> <span class="math display">\[ESS = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2\tag{0.2}\]</span>
<strong>Residual sum of squares:</strong> <span class="math display">\[RSS = \sum_{i=1}^n (y_i - \hat{y}_i)^2\tag{0.3}\]</span>
<strong>Likelihood:</strong> <span class="math display">\[LH = \prod_{i=1}^n Normal(\beta_0 + \beta_1 x_{1i} + ... + \beta_k x_{ki}, \sigma)\tag{0.4}\]</span></p>
</div>
<hr />
</div>
<div id="theorem-1" class="section level3">
<h3><span class="header-section-number">14.7.3</span> Theorem 1</h3>
<div id="theorem-1a---special-case-one-predictor-k1" class="section level4">
<h4><span class="header-section-number">14.7.3.1</span> Theorem 1a - special case: one predictor (<span class="math inline">\(k=1\)</span>)</h4>
<div class="blue">
<p><strong>Closed-form solution for parameter fit under ordinary least squares loss function (special case where <span class="math inline">\(k=1\)</span>).</strong></p>
<p><em>If there is only one predictor variable <span class="math inline">\((k=1)\)</span>, the closed-form solution of predictors in the model that minimize RSS (Residual Sum of Squares) is:</em></p>
<p><span class="math display">\[\begin{align}
\hat{\beta_0} &amp;= \bar{y} - \hat{\beta}_1 \bar{x}\textrm{, and}\\
\\
\hat{\beta_1} &amp;= \frac{Cov(x,y)}{Var(x)}.
\end{align}\]</span></p>
</div>
<div id="proof" class="section level5">
<h5><span class="header-section-number">14.7.3.1.1</span> Proof</h5>
<p><em><span class="citation">(See e.g., Kirchner <a href="#ref-kirchner2003">2003</a>, 1–3; Olive <a href="#ref-olive2017">2017</a>, 57–59)</span></em></p>
<p>Given a set of <span class="math inline">\(n\)</span> observations <span class="math inline">\((X_i,Y_i)\)</span> (or points on a scatter plot), we want to find the best-fit line,
<span class="math display">\[\hat y_i=\hat\beta_0+\hat\beta_1x_i,\tag{1.1.1}\]</span>
such that the sum of squared errors (RSS) in <span class="math inline">\(Y\)</span> is minimized:</p>
<p><span class="math display">\[RSS=\sum_{i=1}^n (y_i - \hat{y}_i)^2 \rightarrow min.\tag{1.1.2}\]</span></p>
<p>Let the <em>Residual Sum of Squares (RSS)</em> be denoted as <span class="math inline">\(Q\)</span> with,</p>
<p><span class="math display">\[\begin{align}
Q=RSS&amp;=\sum_{i=1}^{n}(y_i-\hat y_i)^2\\ &amp;=\sum_{i=1}^{n}(y_i-\hat\beta_0-\hat\beta_1x_i)^2.
\tag{1.1.3}
\end{align}\]</span></p>
<p>We want to minimize <span class="math inline">\(Q\)</span> (that is minimizing <em>RSS</em>) at the values of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> for which <span class="math inline">\(\frac{\partial Q}{\partial \hat\beta_0}=0\)</span> (1) and <span class="math inline">\(\frac{\partial Q}{\partial \hat\beta_1}=0\)</span> (2).</p>
<p>The first condition (1) is,</p>
<p><span class="math display">\[ \begin{align} \frac{\partial Q}{\partial \hat\beta_0}=\sum_{i=1}^{n}-2(y_i-\hat\beta_0-\hat\beta_1x_i)&amp;= 0\\
&amp;=-\sum_{i=1}^ny_i+\sum_{i=1}^n\hat \beta_0+\sum_{i=1}^n\hat\beta_1x_i\\
&amp;=-\sum_{i=1}^ny_i+n\hat\beta_0+\sum_{i=1}^n\hat\beta_1x_i
\tag{1.1.4}
\end{align}\]</span></p>
<p>which, if we solve for <span class="math inline">\(\hat\beta_0\)</span>, becomes</p>
<p><span class="math display">\[\begin{align}
\hat\beta_0&amp;=\frac{1}{n}\sum_{i=1}^{n}y_i-\frac{1}{n}\hat\beta_1\sum_{i=1}^{n}x_i\\
&amp;=\bar y - \hat\beta_1\bar x,
\tag{1.1.5}
\end{align}\]</span></p>
<p>which says that the constant <span class="math inline">\(\hat\beta_0\)</span> (the y-intercept) is set such that the line must go through the mean of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. This makes sense, because this point is the “center” of the data cloud.</p>
<p>The solution is indeed a minimum as the second partial derivative is positive:</p>
<p><span class="math inline">\(\frac{\partial^2 Q}{\partial\hat\beta_0^2}=2n&gt;0. \tag{1.1.6}\)</span></p>
<p>The second condition (2) is,</p>
<p><span class="math display">\[ \begin{align}
\frac{\partial Q}{\partial \hat\beta_1}=\sum_{i=1}^{n}-2x_i(y_i-\hat\beta_0-\hat\beta_1x_i)&amp;=0\\
&amp;=\sum_{i=1}^{n}(-x_iy_i+\hat\beta_0x_i+\hat\beta_1x_i^2)\\
&amp;=-\sum_{i=1}^{n}x_iy_i+\hat\beta_0\sum_{i=1}^{n}x_i+\hat\beta_1\sum_{i=1}^{n}x_i^2
\tag{1.1.7}
\end{align}\]</span></p>
<p>If we substitute the expression by (1.1.5), we get,</p>
<p><span class="math display">\[ \begin{align}
0&amp;=-\sum_{i=1}^{n}x_iy_i+(\bar y - \hat\beta_1\bar x)\sum_{i=1}^{n}x_i+\hat\beta_1\sum_{i=1}^{n}x_i^2\\
&amp;=-\sum_{i=1}^{n}x_iy_i+\bar y\sum_{i=1}^{n}x_i-\hat\beta_1\bar x\sum_{i=1}^{n}x_i+\hat\beta_1\sum_{i=1}^{n}x_i^2
\tag{1.1.8}
\end{align}\]</span></p>
<p>separating this into two sums,</p>
<p><span class="math display">\[ \sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)-\hat\beta_1\sum_{i=1}^{n}\left(x_i^2-x_i\bar x\right)=0 \tag{1.1.9}\]</span></p>
<p>becomes,</p>
<p><span class="math display">\[ \hat\beta_1 = \frac{\sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)}{\sum_{i=1}^{n}\left( x_i^2-x_i\bar x\right)} = \frac{\sum_{i=1}^{n}\left( x_iy_i\right)-n\bar x\bar y}{\sum_{i=1}^{n}\left( x_i^2\right)-n \bar x^2} \tag{1.1.10}\]</span></p>
<p>The model assumes that the deviation from the values from the mean is zero, so that the positive and negative values are in balance, thus</p>
<p><span class="math display">\[ \sum_{i=1}^{n}\left( \bar x^2-x_i\bar x\right)=0, \tag{1.1.11}\]</span></p>
<p>and</p>
<p><span class="math display">\[ \sum_{i=1}^{n}\left(\bar x \bar y - y_i \bar x\right)=0. \tag{1.1.12}\]</span></p>
<p>This can be used in order to expand the previous term and finally to rewrite <span class="math inline">\(\hat\beta_1\)</span> as the ratio of <span class="math inline">\(Cov(x,y)\)</span> to <span class="math inline">\(Var(x)\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\hat\beta_1&amp;=\frac{\sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)+\sum_{i=1}^{n}\left(\bar x\bar y - y_i \bar x\right)}{\sum_{i=1}^{n}\left( x_i^2-x_i\bar x\right)+\sum_{i=1}^{n}\left( \bar x^2-x_i\bar x\right)}=\frac{\sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)+0}{\sum_{i=1}^{n}\left( x_i^2-x_i\bar x\right)+0}\\
\\
&amp;=\frac{\frac{1}{n}\sum_{i=1}^{n}\left( x_i-\bar x\right) \left(y_i- \bar y \right)}{\frac{1}{n}\sum_{i=1}^{n}\left( x_i-\bar x\right)^2}\\
\\
&amp;=\frac{Cov(x,y)}{Var(x)}.
\tag{1.1.13}
\end{align}\]</span></p>
<p>The solution is indeed a minimum as the second partial derivative is positive:</p>
<p><span class="math display">\[\frac{\partial^2Q}{\partial \hat\beta_1^2}= 2 \sum_{i=1}^{n}x_i^2 &gt;0. \tag{1.1.14}\]</span></p>
</div>
</div>
<div id="theorem-1b-generalization-of-theorem-1a-to-k-1" class="section level4">
<h4><span class="header-section-number">14.7.3.2</span> Theorem 1b: Generalization of Theorem 1a to <span class="math inline">\(k &gt;=1\)</span></h4>
<p><em><span class="citation">(see e.g., Bremer <a href="#ref-bremer2012">2012</a>, 21–23; González and Orbe <a href="#ref-gonzalez2014">2014</a>, 5–15)</span></em></p>
<p>The model of multiple linear regression is given by the following expression:</p>
<p><span class="math display">\[y=\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_kx_k+\epsilon \tag{1.2.1}\]</span>
Suppose we have <span class="math inline">\(n\)</span> observations, then we can write:
<span class="math display">\[\begin{align}
y_1&amp;=\beta_0+\beta_{1}x_{11}+\beta_2x_{21}+...+\beta_kx_{k1}+\epsilon_1\\
y_2&amp;=\beta_0+\beta_{1}x_{12}+\beta_2x_{22}+...+\beta_kx_{k2}+\epsilon_2\\
...\\
y_n&amp;=\beta_0+\beta_{1}x_{1n}+\beta_2x_{2n}+...+\beta_kx_{kn}+\epsilon_n
\tag{1.2.2}
\end{align}\]</span>
The model of multiple linear regression is often expressed in matrix notation:</p>
<p><span class="math display">\[\begin{bmatrix} y_1\\y_2\\...\\y_n \end{bmatrix}= \begin{bmatrix}1&amp;x_{11}&amp; x_{21}&amp;...&amp;x_{k1}\\1&amp;x_{12}&amp; x_{22}&amp;...&amp;x_{k2}\\...&amp; ...&amp;...&amp;...&amp;...\\1&amp;x_{1n}&amp;x_{2n}&amp;...&amp;x_{kn}\end{bmatrix}\begin{bmatrix}\beta_0\\\beta_1\\...\\\beta_n \end{bmatrix}+\begin{bmatrix}\epsilon_1\\\epsilon_2\\...\\\epsilon_n \end{bmatrix} \tag{1.2.3}\]</span></p>
<p>Which can be expressed in a compact form as</p>
<p><span class="math display">\[\mathbf{Y=X\beta+\epsilon} \tag{1.2.4}\]</span>
where <span class="math inline">\(y\)</span> is a vector <span class="math inline">\(n\times 1\)</span>, <span class="math inline">\(X\)</span> is a matrix <span class="math inline">\(n \times k\)</span>, <span class="math inline">\(\beta\)</span> is a vector <span class="math inline">\(k \times 1\)</span> and <span class="math inline">\(\epsilon\)</span> is a vector <span class="math inline">\(n \times 1\)</span>.</p>
<p>The OLS estimator is obtained (like in the special case) by minimizing the residual sum of squares (RSS).</p>
<p><span class="math display">\[RSS \rightarrow min.\]</span></p>
<p>The RSS for the multiple linear regression model is</p>
<p><span class="math display">\[Q=RSS=\sum_{i=1}^n \hat\epsilon_i^2=\sum_{i=1}^n (y_i - \hat{y}_i)^2=\sum_{i=1}^n \left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]^2 \tag{1.2.5}\]</span></p>
<p>to apply the least squares criterion in the model of multiple linear regression, thus to minimize <span class="math inline">\(RSS\)</span>, we calculate the first partial derivative from <span class="math inline">\(Q\)</span> with respect to each <span class="math inline">\(\hat\beta_j\)</span>in the expression:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial Q}{\partial\hat\beta_0}&amp;=2\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right][-1]\\
\\
\frac{\partial Q}{\partial\hat\beta_1}&amp;=2\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right][-x_{1i}]\\
\\
\frac{\partial Q}{\partial\hat\beta_2}&amp;=2\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right][-x_{2i}]\\
...\\
\frac{\partial Q}{\partial\hat\beta_k}&amp;=2\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right][-x_{ki}]
\tag{1.2.6}
\end{align}\]</span></p>
<p>Then the derivative of each equation is set to zero:</p>
<p><span class="math display">\[\begin{align}
&amp;\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]=0\\
&amp;\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]x_{1i}=0\\
&amp;\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]x_{2i}=0\\
&amp;...\\
&amp;\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]x_{ki}=0
\tag{1.2.7}
\end{align}\]</span></p>
<p>Alternatively, we can use matrix notation and combine the above equations into the following form:</p>
<p><span class="math display">\[\mathbf{X&#39;Y-X&#39;X\hat\beta=0}.\tag{1.2.8}\]</span></p>
<p>Whereby the following expression is known as <strong>normal equations</strong>:</p>
<p><span class="math display">\[\mathbf{X&#39;X\hat\beta=X&#39;Y}.\tag{1.2.9}\]</span></p>
<p>The system of normal equations in expanded matrix notation is:</p>
<p><span class="math display">\[\begin{bmatrix} n&amp;\sum_{i=1}^nx_{1i}&amp;...&amp;\sum_{i=1}^nx_{ki}\\
\sum_{i=1}^nx_{1i}&amp;\sum_{i=1}^nx_{1i}^2&amp;...&amp;\sum_{i=1}^nx_{1i}x_{ki}\\...&amp;...&amp;...&amp;...\\
\sum_{i=1}^nx_{ki}&amp;\sum_{i=1}^nx_{ki}x_{1i}&amp;...&amp;\sum_{i=1}^nx_{ki}^2\end{bmatrix}\begin{bmatrix}\hat\beta_0\\\hat\beta_1\\...\\\hat\beta_k\end{bmatrix}=\begin{bmatrix}\sum_{i=1}^ny_i\\\sum_{i=1}^nx_{1i}y_i\\...\\\sum_{i=1}^nx_{ki}y_i 
\tag{1.2.10}
\end{bmatrix}\]</span></p>
<p>In order to obtain the estimator <span class="math inline">\(\hat\beta\)</span>, we have to rearrange (1.2.10) and get the solution:</p>
<p><span class="math display">\[\begin{bmatrix}\hat\beta_0\\\hat\beta_1\\...\\\hat\beta_k\end{bmatrix}=\mathbf{\hat\beta}=[\mathbf{X&#39;X}]^{-1}\mathbf{X&#39;Y}\tag{1.2.11}\]</span></p>
<p>Where <span class="math inline">\(\hat\beta\)</span> is a global minimizer of the OLS criterion as the scond order condition is always a semidefinite positive matrix.</p>
<p><span class="math display">\[\frac{\partial^2 Q}{\partial \mathbf{\hat\beta}^2}=2X&#39;X &gt;0.\]</span></p>
<hr />
</div>
</div>
<div id="theorem-2" class="section level3">
<h3><span class="header-section-number">14.7.4</span> Theorem 2</h3>
<div class="blue">
<p><strong>Best fit under OLS is equivalent with best fit under MLE</strong></p>
<p><em>The parameters <span class="math inline">\(\beta_0, ..., \beta_k\)</span> minimize the residual sum of squares (RSS) iff they maximize the (log-)likelihood (LH).</em></p>
</div>
<div id="proof-1" class="section level5">
<h5><span class="header-section-number">14.7.4.0.1</span> Proof</h5>
<p><em><span class="citation">(see e.g., Naveen <a href="#ref-naveen2019">2019</a>; Croot <a href="#ref-croot2010">2010</a>; Eppes <a href="#ref-eppes2019">2019</a>)</span></em></p>
</div>
<div id="maximum-likelihood-estimation" class="section level4">
<h4><span class="header-section-number">14.7.4.1</span> Maximum Likelihood Estimation</h4>
<p>We consider again the linear regression model of the population with:</p>
<p><span class="math display">\[Y= \beta_0 + \beta_1X + \epsilon.\tag{2.1}\]</span></p>
<p>This simplifies to the following form on the observed data:</p>
<p><span class="math display">\[y= \beta_0 + \beta_1x + \epsilon.\tag{2.2}\]</span></p>
<p>Using a sample in order to obtain the <strong>maximum likelihood estimates</strong> the equation simplifies to:</p>
<p><span class="math display">\[y= \hat\beta_0 + \hat\beta_1x. \tag{2.3}\]</span></p>
<p><strong>Assumptions</strong> that we make for the model:</p>
<ul>
<li>True underlying distribution of the errors is Gaussian,</li>
<li>Expected value of the error term is 0,</li>
<li>Variance of the error term is constant with respect to x, and</li>
<li>the ‘lagged’ errors are independent of each other
where the error term is normally distributed.</li>
</ul>
<p>We can write:</p>
<p><span class="math display">\[ \epsilon \sim N(0,\sigma^2).\tag{2.4}\]</span></p>
<p>Since <span class="math inline">\(Y\)</span> is a linear function of <span class="math inline">\(\epsilon\)</span> it will also be normally distributed.</p>
<p><span class="math display">\[f(\epsilon|\beta_0,\beta_1)= \frac{1}{\sqrt{2\pi} \sigma} \exp\left[{-\frac{1}{2}\left(\frac{\epsilon^2}{\sigma^2}\right)}\right]. \tag{2.5}\]</span></p>
<p>Given the whole data set with <span class="math inline">\(i=1,...,n\)</span> observations the <strong>likelihood function</strong> (<span class="math inline">\(LH\)</span>) is the joint density of all the observations, given a value for the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. Since independence is assumed, this is simply the product of the individual densities from the previous equation.</p>
<p><span class="math display">\[LH(\epsilon_i|\beta_0,\beta_1) =\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi} \sigma} \exp\left[{-\frac{1}{2}\left(\frac{\epsilon_i^2}{\sigma^2}\right)}\right].\tag{2.6}\]</span></p>
<p>In order to find the maximum of (2.6) we have to find the first derivative.</p>
<p>But as the derivation of a product with a lot of factors is inconvenient, we take first the logarithm of the likelihood, called <strong>log-likelihhod</strong>. This is possible as <span class="math inline">\(\log\)</span> is a monotone transformation and the maximum likelihood estimate does not change on log transformation.</p>
<p>The log-likelihood is the sum of the logs of the individual densities:</p>
<p><span class="math display">\[\begin{align}
LL&amp;=\log \left(LH(\epsilon_i|\beta_0,\beta_1)\right)\\
&amp;=-\left( \frac{n}{2}\right) \log(2\pi)-\left( \frac{n}{2}\right) \log(\sigma^2)-\left( \frac{1}{2}\sigma^2\right) \sum_{i=1}^n(\epsilon_i)^2
\tag{2.7}
\end{align}\]</span></p>
<p>The log-likelihood can be used in order to find the <em>Maximum Likelihood estimates</em> <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> for the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<p><span class="math display">\[\hat\beta_0,\hat\beta_1=\mathrm{argmax}_{\beta_0,\beta_1}LL=\mathrm{argmax}_{\beta_0,\beta_1}\left[-\left( \frac{n}{2}\right) \log(2\pi)-\left( \frac{n}{2}\right) \log(\sigma^2)-\left( \frac{1}{2}\sigma^2\right) \sum_{i=1}^n(\epsilon_i)^2\right].\tag{2.8}\]</span></p>
<p>Removing the constant terms results in:</p>
<p><span class="math display">\[\hat\beta_0,\hat\beta_1=\mathrm{argmax}_{\beta_0,\beta_1} \sum_{i=1}^n-\epsilon_i^2.\tag{2.9}\]</span></p>
<p>Substituting <span class="math inline">\(\epsilon\)</span>, derived from (2.2), gives:</p>
<p><span class="math display">\[\hat\beta_0,\hat\beta_1=\mathrm{argmax}_{\beta_0,\beta_1} \sum_{i=1}^n-(y-\beta_0-\beta_1x)^2.\tag{2.10}\]</span></p>
<p><strong>Conclusion:</strong></p>
<p>Deriving parameter estimates according to the OLS method:</p>
<p><span class="math display">\[Q_{OLS}=\sum_{i=1}^{n}(y_i-\hat\beta_0-\hat\beta_1x_i)^2 \rightarrow min.\]</span></p>
<p>Deriving parameter estimates according to the ML method:</p>
<p><span class="math display">\[Q_{ML}=\sum_{i=1}^n-(y_i-\hat\beta_0-\hat\beta_1x_i)^2 \rightarrow max.\]</span></p>
<p>Maximizing <span class="math inline">\(-z\)</span> is equivalent to minimizing <span class="math inline">\(z\)</span>, thus, the best parameter fit under ML is equivalent with best fit under OLS.</p>
<hr />
</div>
</div>
<div id="theorem-3" class="section level3">
<h3><span class="header-section-number">14.7.5</span> Theorem 3</h3>
<div class="blue">
<p><strong>The variance in a regression model can be decompossed by using the notion of sum of squares:</strong></p>
<p><em>The following decomposition holds:</em></p>
<p><span class="math display">\[\begin{align}
TSS &amp;= ESS + RSS \textrm{, or}\\
\sum_{i=1}^n (y_i - \bar{y})^2&amp;=\sum_{i=1}^n (\hat{y}_i - \bar{y})^2+ \sum_{i=1}^n \hat u_i^2 
\end{align}\]</span></p>
</div>
<p>with</p>
<p><span class="math display">\[\hat u_i =y_i-\hat y_i \]</span></p>
<p>which can be rewritten as</p>
<p><span class="math display">\[y_i=\hat y_i + \hat u_i \tag{3.1}\]</span></p>
<div id="proof-2" class="section level4">
<h4><span class="header-section-number">14.7.5.1</span> Proof</h4>
<p><span class="citation">(see e.g., González and Orbe <a href="#ref-gonzalez2014">2014</a>, 29–31)</span></p>
<p><span class="math display">\[\begin{align}
TSS=\sum_{i=1}^n (y_i - \bar{y})^2&amp;=\sum_{i=1}^n\left( \hat y_i+\hat u_i - \bar y \right)^2\\
&amp;= \sum_{i=1}^n \left[(\hat y_i-\bar y)+\hat u_i\right]^2\\
&amp;= \sum_{i=1}^n (\hat y_i-\bar y)^2 + \sum_{i=1}^n \hat u_i^2+ 2\sum_{i=1}^n(\hat y_i-\bar y)\hat u_i\\
&amp;= \sum_{i=1}^n (\hat y_i-\bar y)^2 + \sum_{i=1}^n \hat u_i^2\\
\tag{3.2}
\end{align}\]</span></p>
<p>Or, using (3.1) we can write:</p>
<p><span class="math display">\[
TSS=\sum_{i=1}^n (\hat{y}_i - \bar{y})^2+\sum_{i=1}^n (y_i - \hat{y}_i)^2=ESS+RSS.
\tag{3.3}
\]</span></p>
<hr />
</div>
</div>
<div id="theorem-4" class="section level3">
<h3><span class="header-section-number">14.7.6</span> Theorem 4</h3>
<div class="blue">
<p><strong>Expressing variance explained <span class="math inline">\(R^2\)</span> in terms of sum of squares</strong></p>
<p><em>The following relationship holds:</em></p>
<p><span class="math display">\[R^2 = 1 - \frac{RSS}{TSS} = \frac{ESS}{TSS}\]</span></p>
</div>
<div id="proof-3" class="section level4">
<h4><span class="header-section-number">14.7.6.1</span> Proof</h4>
<p><span class="citation">(see e.g., Urban and Mayerl <a href="#ref-urban2018">2018</a>, 56–72, 101–3)</span></p>
<p>We will first derive the variance explained <span class="math inline">\(R^2\)</span> considering a bivariate model (simple linear regression), afterwards some note to the generalized case of multiple regression are made.</p>
<p>The variance explained can be derived in the bivariate model from the regression coefficient <span class="math inline">\(\beta\)</span> and the standard deviation of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, whereby the following holds:</p>
<p><span class="math display">\[R^2=\left( \beta \cdot \frac{S_x}{S_y}\right)^2=\beta^2 \cdot \frac{Var(X)}{Var(Y)}.\tag{4.1}\]</span></p>
<p>We will show that (4.1) holds in the next steps.</p>
<p>Consider on the one hand the calculation of the correlation coefficient:</p>
<p><span class="math display">\[\begin{align}
r_{xy}&amp;=\frac{cov(X,Y)}{S_x S_y}\\
\\
&amp;=\frac{\frac{1}{n}\sum_{i=1}^n(X_i-\bar X)(Y_i-\bar Y)}{\sqrt{\frac{\sum_{i=1}^n(X_i-\bar X)^2}{n}} \sqrt{\frac{\sum_{i=1}^n(Y_i-\bar Y)^2}{n}}}
\tag{4.2}
\end{align}\]</span></p>
<p>On the other hand, if we assume <span class="math inline">\(Y\)</span> as dependent and <span class="math inline">\(X\)</span> as independent variable, we can write:</p>
<p><span class="math display">\[\beta_{yx}=\frac{\sum_{i=1}^{n}X_iY_i}{\sum_{i=1}^n X_i^2}. \tag{4.3}\]</span></p>
<p>Or when <span class="math inline">\(Y\)</span> is the independent and <span class="math inline">\(X\)</span> the dependent variable, then:</p>
<p><span class="math display">\[\beta_{xy}=\frac{\sum_{i=1}^{n}Y_iX_i}{\sum_{i=1}^n Y_i^2}.\tag{4.4}\]</span></p>
<p>Through dividing equations (4.3) and (4.4) by the number of observations, we get the covariances and variances of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[\begin{align}
&amp;\frac{1}{n}\sum_{i=1}^{n}X_iY_i=\frac{1}{n}(X_i-\bar X)(Y_i-\bar Y) = cov(X,Y)\tag{4.5}\\
&amp;\frac{1}{n}\sum_{i=1}^{n}Y_iX_i=\frac{1}{n}(Y_i-\bar Y)(X_i-\bar X) = cov(Y,X)\tag{4.6}\\
&amp;\frac{1}{n}\sum_{i=1}^{n}X_i^2=\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar X)^2=S_x^2\tag{4.7}\\
&amp;\frac{1}{n}\sum_{i=1}^{n}Y_i^2=\frac{1}{n}\sum_{i=1}^{n}(Y_i-\bar Y)^2=S_y^2
\tag{4.8}
\end{align}\]</span></p>
<p>Substituting these results in equations (4.3) and (4.4), we get:</p>
<p><span class="math display">\[\beta_{yx}=\frac{cov(X,Y)}{S_x^2} \textrm{ , and }
\beta_{xy}=\frac{cov(Y,X)}{S_y^2}\tag{4.9}\]</span></p>
<p>For calculating the <em>average</em> between <span class="math inline">\(\beta_{yx}\)</span> and <span class="math inline">\(\beta_{xy}\)</span> we have to use the <em>geometric mean</em>:</p>
<p><span class="math display">\[\bar\beta_{geom}=\sqrt{\frac{cov(X,Y)}{S_x^2}\frac{cov(Y,X)}{S_y^2}}=\frac{cov(X,Y)}{S_x S_y}=r_{xy}\tag{4.10}\]</span></p>
<p>Thus, the geometric mean of <span class="math inline">\(\beta_{yx}\)</span> and <span class="math inline">\(\beta_{xy}\)</span> is identical to the correlation coefficient.</p>
<p>Comparing equations (4.9) and (4.10),</p>
<p><span class="math display">\[\begin{align}
\beta_{yx}&amp;=\frac{cov(X,Y)}{S_x^2}\textrm{ , and}\\
\\
r_{yx}&amp;=\frac{cov(X,Y)}{S_x S_y},
\end{align}\]</span></p>
<p>shows that, we can convert <span class="math inline">\(r_{yx}\)</span> in <span class="math inline">\(\beta_{yx}\)</span>:</p>
<p><span class="math display">\[\frac{cov(X,Y)}{S_x S_y} \cdot \frac{S_y}{S_x}=\frac{cov(X,Y)}{S_x^2}=\beta_{yx}.\tag{4.11}\]</span></p>
<p>Therefore, the relationship stated in (4.1) holds:</p>
<p><span class="math display">\[\beta_{yx}=r_{yx}\cdot \frac{S_y}{S_x}.\]</span></p>
<p>Using this definition we can now derive the <strong>variance explained</strong> <span class="math inline">\(R^2\)</span>.</p>
<p>We have seen that the following holds:</p>
<p><span class="math display">\[\beta_{yx}=r_{yx}\cdot \frac{S_y}{S_x}=r_{yx}\sqrt{\frac{\frac{1}{n}\sum_{i=1}^{n}(Y_i-\bar Y)^2}{\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar X)^2}}.\]</span></p>
<p>Eliminating the number of observations and squaring the equation gives:</p>
<p><span class="math display">\[\beta_{yx}^2=r_{yx}^2\frac{\sum_{i=1}^{n}(Y_i-\bar Y)^2}{\sum_{i=1}^{n}(X_i-\bar X)^2}.\tag{4.12}\]</span></p>
<p>The numerator <span class="math inline">\(\sum_{i=1}^{n}(Y_i-\bar Y)^2\)</span> is the total sum of squares (TSS), thus, from (0.1) follows:</p>
<p><span class="math display">\[\sum_{i=1}^{n}(Y_i-\bar Y)^2=\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2+\sum_{i=1}^{n}(Y_i-\hat Y_i)^2.\tag{4.13}\]</span></p>
<p>The explained sum of squares (ESS) <span class="math inline">\(\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2\)</span> are statistically explained by the determination of the regression line:</p>
<p><span class="math display">\[ESS=\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2=\beta X_i=\beta(X_i-\bar X).\tag{4.14}\]</span></p>
<p>Substituting the result of (4.14) in equation (4.13) gives:</p>
<p><span class="math display">\[\sum_{i=1}^{n}(Y_i-\bar Y)^2=\beta^2\sum_{i=1}^{n}(X_i-\bar X)^2+\sum_{i=1}^{n}(Y_i-\hat Y_i)^2.\tag{4.15}\]</span></p>
<p>As we know from (4.12):</p>
<p><span class="math display">\[\beta_{yx}^2=r_{yx}^2\frac{\sum_{i=1}^{n}(Y_i-\bar Y)^2}{\sum_{i=1}^{n}(X_i-\bar X)^2},\]</span></p>
<p>we can substitute <span class="math inline">\(\beta\)</span> in equation (4.15) with the result from (4.12) and get:</p>
<p><span class="math display">\[\begin{align}
\sum_{i=1}^{n}(Y_i-\bar Y)^2&amp;=r_{xy}^2\frac{\sum_{i=1}^{n}(Y_i-\bar Y)^2}{\sum_{i=1}^{n}(X_i-\bar X)^2}\sum_{i=1}^{n}(X_i-\bar X)^2+\sum_{i=1}^{n}(Y_i-\hat Y_i)^2\\
\\
&amp;=r_{xy}^2 \sum_{i=1}^{n}(Y_i-\bar Y)^2+\sum_{i=1}^{n}(Y_i-\hat Y_i)^2\tag{4.16}\\
\\
\frac{\sum_{i=1}^{n}(Y_i-\bar Y)^2-\sum_{i=1}^{n}(Y_i-\hat Y_i)^2}{\sum_{i=1}^{n}(Y_i-\bar Y)^2}&amp;=r_{xy}^2 \textrm{ ,which is}\\
\\
1-\frac{\sum_{i=1}^{n}(Y_i-\hat Y_i)^2}{\sum_{i=1}^{n}(Y_i-\bar Y)^2}=1-\frac{RSS}{TSS}&amp;=r_{xy}^2.
\tag{4.17}
\end{align}\]</span></p>
<p>Similarly, according to (0.2) the explained sum of squares (ESS) are the deviation between total (TSS) and residual sum of squares (RSS):</p>
<p><span class="math display">\[\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2 = \sum_{i=1}^{n}( Y_i-\bar Y)^2-\sum_{i=1}^{n}(Y_i-\hat Y)^2,\]</span></p>
<p>therefore, we can write equation (4.17) as well as:</p>
<p><span class="math display">\[\begin{align}
\frac{\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2}{\sum_{i=1}^{n}(Y_i-\bar Y)^2}&amp;=r_{xy}^2\\
\\
\frac{ESS}{RSS}&amp;=r_{xy}^2=R^2.
\tag{4.18}
\end{align}\]</span></p>
<p><strong>Final remarks:</strong>
In the bivariate model (simple linear regression) the variance explained is equivalent to the squared bivariate correlation coefficient (Bravais Pearson).
In multiple regression model the relation:</p>
<p><span class="math display">\[\frac{ESS}{TSS}=R^2\]</span></p>
<p>holds as well. Here the variance explained is equivalent to the squared multiple correlation coefficient between the estimated and observed Y-value (<span class="math inline">\(r_{\hat Y Y}\)</span>).</p>
<p>In the multiple linear regression model the variance explained is also dependent in the number of X-variables. Therefore an adjusted <span class="math inline">\(R^2\)</span> measure should be used.</p>

</div>
</div>
</div>
<!-- </div> -->
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-bremer2012">
<p>Bremer, M. 2012. “Multiple Linear Regression.” http://mezeylab.cb.bscb.cornell.edu/labmembers/documents/supplement 5 - multiple regression.pdf.</p>
</div>
<div id="ref-croot2010">
<p>Croot, Ernie. 2010. “Maximum likelihood estimators and least squares.”</p>
</div>
<div id="ref-eppes2019">
<p>Eppes, Marissa. 2019. “Maximum Likelihood Estimation Explained - Normal Distribution.” https://towardsdatascience.com/maximum-likelihood-estimation-explained-normal-distribution-6207b322e47f.</p>
</div>
<div id="ref-gonzalez2014">
<p>González, Pilar, and Susan Orbe. 2014. “The Multiple Regression Model: Estimation.”</p>
</div>
<div id="ref-kirchner2003">
<p>Kirchner, James W. 2003. “Data Analysis Toolkit 10: Simple Linear Regression Derivation of Linear Regression Equations.”</p>
</div>
<div id="ref-naveen2019">
<p>Naveen, Mathew Nathan S. 2019. “Equivalence of MLE and OLS in linear regression.” https://medium.com/@snaveenmathew/equivalence-of-mle-and-ols-in-linear-regression-d3e44e47df3c.</p>
</div>
<div id="ref-olive2017">
<p>Olive, David J. 2017. <em>Linear Regression</em>. Springer International Publishing.</p>
</div>
<div id="ref-urban2018">
<p>Urban, Dieter, and Jochen Mayerl. 2018. <em>Angewandte Regressionsanalyse: Theorie, Technik und Praxis</em>. Springer.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="testing-coefficients.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="logistic-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["I2DA.epub", "I2DA.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
