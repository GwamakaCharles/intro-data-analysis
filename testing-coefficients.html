<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.6 Testing coefficients | Introduction to Data Analysis</title>
  <meta name="description" content="Course Material for I2DA-2019 in Cognitive Science at University Osnabrück" />
  <meta name="generator" content="bookdown 0.17.2 and GitBook 2.6.7" />

  <meta property="og:title" content="3.6 Testing coefficients | Introduction to Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course Material for I2DA-2019 in Cognitive Science at University Osnabrück" />
  <meta name="github-repo" content="michael-franke/intro-data-analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.6 Testing coefficients | Introduction to Data Analysis" />
  
  <meta name="twitter:description" content="Course Material for I2DA-2019 in Cognitive Science at University Osnabrück" />
  

<meta name="author" content="Michael Franke" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="a-bayesian-approach.html"/>
<link rel="next" href="Chap-04-02-beyond-simple-regression.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!--<link rel="stylesheet" href="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-editor-1.0.9.css">-->
<link rel="stylesheet" href="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-viz-0.7.11.css">

<script src="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-editor-1.0.9.js"></script>
<script src="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-viz-0.7.11.js"></script>
<script src="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-v0.9.13.js" defer async></script>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span. { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />
<link rel="stylesheet" href="webppl-editor.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li class="chapter" data-level="1" data-path="general-introduction.html"><a href="general-introduction.html"><i class="fa fa-check"></i><b>1</b> General Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="Chap-01-00-intro-learning-goals.html"><a href="Chap-01-00-intro-learning-goals.html"><i class="fa fa-check"></i><b>1.1</b> Learning goals</a></li>
<li class="chapter" data-level="1.2" data-path="Chap-01-00-intro-course-structure.html"><a href="Chap-01-00-intro-course-structure.html"><i class="fa fa-check"></i><b>1.2</b> Course structure</a></li>
<li class="chapter" data-level="1.3" data-path="Chap-01-00-intro-tools-methods.html"><a href="Chap-01-00-intro-tools-methods.html"><i class="fa fa-check"></i><b>1.3</b> Tools and topics covered (and not covered) here</a></li>
<li class="chapter" data-level="1.4" data-path="Chap-01-00-intro-data-sets.html"><a href="Chap-01-00-intro-data-sets.html"><i class="fa fa-check"></i><b>1.4</b> Data sets covered</a></li>
<li class="chapter" data-level="1.5" data-path="Chap-01-00-intro-installation.html"><a href="Chap-01-00-intro-installation.html"><i class="fa fa-check"></i><b>1.5</b> Installation</a><ul>
<li class="chapter" data-level="1.5.1" data-path="Chap-01-00-intro-installation.html"><a href="Chap-01-00-intro-installation.html#Chap-01-00-intro-installation-VirtualBox"><i class="fa fa-check"></i><b>1.5.1</b> VirtualBox Setup</a></li>
<li class="chapter" data-level="1.5.2" data-path="Chap-01-00-intro-installation.html"><a href="Chap-01-00-intro-installation.html#Chap-01-00-intro-installation-Manual"><i class="fa fa-check"></i><b>1.5.2</b> Manual installation</a></li>
<li class="chapter" data-level="1.5.3" data-path="Chap-01-00-intro-installation.html"><a href="Chap-01-00-intro-installation.html#Chap-01-00-intro-installation-Updating"><i class="fa fa-check"></i><b>1.5.3</b> Updating the course package</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap-01-01-R.html"><a href="Chap-01-01-R.html"><i class="fa fa-check"></i><b>2</b> Basics of R</a><ul>
<li class="chapter" data-level="2.1" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html"><i class="fa fa-check"></i><b>2.1</b> First steps</a><ul>
<li class="chapter" data-level="2.1.1" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#functions"><i class="fa fa-check"></i><b>2.1.1</b> Functions</a></li>
<li class="chapter" data-level="2.1.2" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#variables"><i class="fa fa-check"></i><b>2.1.2</b> Variables</a></li>
<li class="chapter" data-level="2.1.3" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#literate-coding"><i class="fa fa-check"></i><b>2.1.3</b> Literate coding</a></li>
<li class="chapter" data-level="2.1.4" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#objects"><i class="fa fa-check"></i><b>2.1.4</b> Objects</a></li>
<li class="chapter" data-level="2.1.5" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#packages"><i class="fa fa-check"></i><b>2.1.5</b> Packages</a></li>
<li class="chapter" data-level="2.1.6" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#Chap-01-01-R-help"><i class="fa fa-check"></i><b>2.1.6</b> Getting help</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="ch1-data-types.html"><a href="ch1-data-types.html"><i class="fa fa-check"></i><b>2.2</b> Data types</a><ul>
<li class="chapter" data-level="2.2.1" data-path="ch1-data-types.html"><a href="ch1-data-types.html#numeric-vectors-matrices"><i class="fa fa-check"></i><b>2.2.1</b> Numeric vectors &amp; matrices</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch1-data-types.html"><a href="ch1-data-types.html#booleans"><i class="fa fa-check"></i><b>2.2.2</b> Booleans</a></li>
<li class="chapter" data-level="2.2.3" data-path="ch1-data-types.html"><a href="ch1-data-types.html#special-values"><i class="fa fa-check"></i><b>2.2.3</b> Special values</a></li>
<li class="chapter" data-level="2.2.4" data-path="ch1-data-types.html"><a href="ch1-data-types.html#characters-strings"><i class="fa fa-check"></i><b>2.2.4</b> Characters (= strings)</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch1-data-types.html"><a href="ch1-data-types.html#factors"><i class="fa fa-check"></i><b>2.2.5</b> Factors</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch1-data-types.html"><a href="ch1-data-types.html#lists-data-frames-tibbles"><i class="fa fa-check"></i><b>2.2.6</b> Lists, data frames &amp; tibbles</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="Chap-01-01-functions.html"><a href="Chap-01-01-functions.html"><i class="fa fa-check"></i><b>2.3</b> Functions</a><ul>
<li class="chapter" data-level="2.3.1" data-path="Chap-01-01-functions.html"><a href="Chap-01-01-functions.html#some-important-built-in-functions"><i class="fa fa-check"></i><b>2.3.1</b> Some important built-in functions</a></li>
<li class="chapter" data-level="2.3.2" data-path="Chap-01-01-functions.html"><a href="Chap-01-01-functions.html#defining-your-own-functions"><i class="fa fa-check"></i><b>2.3.2</b> Defining your own functions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="ch-01-01-loops-and-maps.html"><a href="ch-01-01-loops-and-maps.html"><i class="fa fa-check"></i><b>2.4</b> Loops and maps</a></li>
<li class="chapter" data-level="2.5" data-path="Chap-01-01-piping.html"><a href="Chap-01-01-piping.html"><i class="fa fa-check"></i><b>2.5</b> Piping</a></li>
<li class="chapter" data-level="2.6" data-path="ch-01-01-Rmarkdown.html"><a href="ch-01-01-Rmarkdown.html"><i class="fa fa-check"></i><b>2.6</b> Rmarkdown</a></li>
</ul></li>
<li class="part"><span><b>II Appliied (generalized) linear modeling</b></span></li>
<li class="chapter" data-level="3" data-path="Chap-04-01-simple-linear-regression.html"><a href="Chap-04-01-simple-linear-regression.html"><i class="fa fa-check"></i><b>3</b> Simple linear regression</a><ul>
<li class="chapter" data-level="3.1" data-path="data-set-murder-data.html"><a href="data-set-murder-data.html"><i class="fa fa-check"></i><b>3.1</b> Data set: murder data</a></li>
<li class="chapter" data-level="3.2" data-path="what-is-a-simple-linear-regression.html"><a href="what-is-a-simple-linear-regression.html"><i class="fa fa-check"></i><b>3.2</b> What is a (simple) linear regression?</a><ul>
<li class="chapter" data-level="3.2.1" data-path="what-is-a-simple-linear-regression.html"><a href="what-is-a-simple-linear-regression.html#prediction-without-any-further-information"><i class="fa fa-check"></i><b>3.2.1</b> Prediction without any further information</a></li>
<li class="chapter" data-level="3.2.2" data-path="what-is-a-simple-linear-regression.html"><a href="what-is-a-simple-linear-regression.html#prediction-with-knowledge-of-unemployment-rate"><i class="fa fa-check"></i><b>3.2.2</b> Prediction with knowledge of unemployment rate</a></li>
<li class="chapter" data-level="3.2.3" data-path="what-is-a-simple-linear-regression.html"><a href="what-is-a-simple-linear-regression.html#simple-linear-regression-general-problem-formulation"><i class="fa fa-check"></i><b>3.2.3</b> Simple linear regression: general problem formulation</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ordinary-least-squares-regression.html"><a href="ordinary-least-squares-regression.html"><i class="fa fa-check"></i><b>3.3</b> Ordinary least-squares regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="ordinary-least-squares-regression.html"><a href="ordinary-least-squares-regression.html#finding-optimal-parameters-with-optim"><i class="fa fa-check"></i><b>3.3.1</b> Finding optimal parameters with <code>optim</code></a></li>
<li class="chapter" data-level="3.3.2" data-path="ordinary-least-squares-regression.html"><a href="ordinary-least-squares-regression.html#fitting-ols-regression-lines-with-lm"><i class="fa fa-check"></i><b>3.3.2</b> Fitting OLS regression lines with <code>lm</code></a></li>
<li class="chapter" data-level="3.3.3" data-path="ordinary-least-squares-regression.html"><a href="ordinary-least-squares-regression.html#finding-optimal-parameter-values-with-math"><i class="fa fa-check"></i><b>3.3.3</b> Finding optimal parameter values with math</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="a-maximum-likelihood-approach.html"><a href="a-maximum-likelihood-approach.html"><i class="fa fa-check"></i><b>3.4</b> A maximum-likelihood approach</a></li>
<li class="chapter" data-level="3.5" data-path="a-bayesian-approach.html"><a href="a-bayesian-approach.html"><i class="fa fa-check"></i><b>3.5</b> A Bayesian approach</a><ul>
<li class="chapter" data-level="3.5.1" data-path="a-bayesian-approach.html"><a href="a-bayesian-approach.html#implementation-in-greta"><i class="fa fa-check"></i><b>3.5.1</b> Implementation in <code>greta</code></a></li>
<li class="chapter" data-level="3.5.2" data-path="a-bayesian-approach.html"><a href="a-bayesian-approach.html#using-the-brms-package"><i class="fa fa-check"></i><b>3.5.2</b> Using the <code>brms</code> package</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="testing-coefficients.html"><a href="testing-coefficients.html"><i class="fa fa-check"></i><b>3.6</b> Testing coefficients</a><ul>
<li class="chapter" data-level="3.6.1" data-path="testing-coefficients.html"><a href="testing-coefficients.html#bayesian-approach"><i class="fa fa-check"></i><b>3.6.1</b> Bayesian approach</a></li>
<li class="chapter" data-level="3.6.2" data-path="testing-coefficients.html"><a href="testing-coefficients.html#frequentist-approach"><i class="fa fa-check"></i><b>3.6.2</b> Frequentist approach</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Chap-04-02-beyond-simple-regression.html"><a href="Chap-04-02-beyond-simple-regression.html"><i class="fa fa-check"></i><b>4</b> Beyond simple linear regression</a><ul>
<li class="chapter" data-level="4.1" data-path="two-categorical-predictors.html"><a href="two-categorical-predictors.html"><i class="fa fa-check"></i><b>4.1</b> Two categorical predictors</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="app-90-further-material.html"><a href="app-90-further-material.html"><i class="fa fa-check"></i><b>A</b> Further useful material</a><ul>
<li class="chapter" data-level="A.1" data-path="material-on-introduction-to-probability.html"><a href="material-on-introduction-to-probability.html"><i class="fa fa-check"></i><b>A.1</b> Material on <em>Introduction to Probability</em>:</a></li>
<li class="chapter" data-level="A.2" data-path="material-on-bayesian-data-analysis.html"><a href="material-on-bayesian-data-analysis.html"><i class="fa fa-check"></i><b>A.2</b> Material on <em>Bayesian Data Analysis</em>:</a></li>
<li class="chapter" data-level="A.3" data-path="material-on-frequentist-statistics.html"><a href="material-on-frequentist-statistics.html"><i class="fa fa-check"></i><b>A.3</b> Material on <em>frequentist statistics</em>:</a></li>
<li class="chapter" data-level="A.4" data-path="material-on-r-tidyverse-etc-.html"><a href="material-on-r-tidyverse-etc-.html"><i class="fa fa-check"></i><b>A.4</b> Material on <em>R, tidyverse, etc.</em>:</a></li>
<li class="chapter" data-level="A.5" data-path="further-information-for-rstudio.html"><a href="further-information-for-rstudio.html"><i class="fa fa-check"></i><b>A.5</b> Further information for RStudio</a></li>
<li class="chapter" data-level="A.6" data-path="resources-on-webppl.html"><a href="resources-on-webppl.html"><i class="fa fa-check"></i><b>A.6</b> Resources on WebPPL</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="testing-coefficients" class="section level2">
<h2><span class="header-section-number">3.6</span> Testing coefficients</h2>
<div id="bayesian-approach" class="section level3">
<h3><span class="header-section-number">3.6.1</span> Bayesian approach</h3>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb168-1" data-line-number="1"><span class="co"># get means and 95% HDI</span></a>
<a class="sourceLine" id="cb168-2" data-line-number="2">Bayes_estimates &lt;-<span class="st"> </span>tidy_draws_murder_data <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb168-3" data-line-number="3"><span class="st">  </span><span class="kw">group_by</span>(Parameter) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb168-4" data-line-number="4"><span class="st">  </span><span class="kw">summarise</span>(</a>
<a class="sourceLine" id="cb168-5" data-line-number="5">    <span class="st">&#39;|95%&#39;</span> =<span class="st"> </span>HDInterval<span class="op">::</span><span class="kw">hdi</span>(value)[<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb168-6" data-line-number="6">    <span class="dt">mean =</span> <span class="kw">mean</span>(value),</a>
<a class="sourceLine" id="cb168-7" data-line-number="7">    <span class="st">&#39;95|%&#39;</span> =<span class="st"> </span>HDInterval<span class="op">::</span><span class="kw">hdi</span>(value)[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb168-8" data-line-number="8">  )</a>
<a class="sourceLine" id="cb168-9" data-line-number="9">Bayes_estimates</a></code></pre></div>
<pre><code>## # A tibble: 3 x 4
##   Parameter `|95%`   mean `95|%`
##   &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1 intercept -39.1  -24.6   -9.51
## 2 sigma       3.78   5.36   7.20
## 3 slope       4.34   6.53   8.53</code></pre>
</div>
<div id="frequentist-approach" class="section level3">
<h3><span class="header-section-number">3.6.2</span> Frequentist approach</h3>
<p>Figure <a href="testing-coefficients.html#fig:Chap-04-01-simple-linear-regression-frequentist-coefficient-testing">3.2</a> shows a frequentist model for testing the hypotheses that the regression coefficients are equal to zero.</p>
<div class="figure" style="text-align: center"><span id="fig:Chap-04-01-simple-linear-regression-frequentist-coefficient-testing"></span>
<img src="visuals/linear-regression-model-frequentist.png" alt="Frequentist model for testing whether regression coefficients are plausibly equal to zero."  />
<p class="caption">
Figure 3.2: Frequentist model for testing whether regression coefficients are plausibly equal to zero.
</p>
</div>
<p>We use this model to compute the test statics for the obsered data:</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb170-1" data-line-number="1"><span class="co"># observed data</span></a>
<a class="sourceLine" id="cb170-2" data-line-number="2">y_obs &lt;-<span class="st"> </span>murder_data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(murder_rate)</a>
<a class="sourceLine" id="cb170-3" data-line-number="3">x_obs &lt;-<span class="st"> </span>murder_data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(unemployment)</a>
<a class="sourceLine" id="cb170-4" data-line-number="4">n_obs &lt;-<span class="st"> </span><span class="kw">length</span>(y_obs)</a>
<a class="sourceLine" id="cb170-5" data-line-number="5"><span class="co"># best-fitting coefficients</span></a>
<a class="sourceLine" id="cb170-6" data-line-number="6">beta_<span class="dv">1</span>_hat &lt;-<span class="st"> </span><span class="kw">cov</span>(x_obs, y_obs) <span class="op">/</span><span class="st"> </span><span class="kw">var</span>(x_obs)</a>
<a class="sourceLine" id="cb170-7" data-line-number="7">beta_<span class="dv">0</span>_hat &lt;-<span class="st"> </span><span class="kw">mean</span>(y_obs) <span class="op">-</span><span class="st"> </span>beta_<span class="dv">1</span>_hat <span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(x_obs)</a>
<a class="sourceLine" id="cb170-8" data-line-number="8"><span class="co"># calculating t-scores</span></a>
<a class="sourceLine" id="cb170-9" data-line-number="9">MSE &lt;-<span class="st"> </span><span class="kw">sum</span>((y_obs <span class="op">-</span><span class="st"> </span>beta_<span class="dv">0</span>_hat <span class="op">-</span><span class="st"> </span>beta_<span class="dv">1</span>_hat <span class="op">*</span><span class="st"> </span>x_obs)<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span>(n_obs<span class="dv">-2</span>) </a>
<a class="sourceLine" id="cb170-10" data-line-number="10">S_xx &lt;-<span class="st"> </span><span class="kw">sum</span>((x_obs <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(x_obs))<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb170-11" data-line-number="11">SE_beta_<span class="dv">1</span>_hat &lt;-<span class="st"> </span><span class="kw">sqrt</span>(MSE  <span class="op">/</span><span class="st"> </span>S_xx)</a>
<a class="sourceLine" id="cb170-12" data-line-number="12">SE_beta_<span class="dv">0</span>_hat &lt;-<span class="st"> </span><span class="kw">sqrt</span>(MSE <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">/</span>n_obs <span class="op">+</span><span class="st"> </span><span class="kw">mean</span>(x_obs)<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>S_xx))</a>
<a class="sourceLine" id="cb170-13" data-line-number="13">t_slope =<span class="st"> </span>(beta_<span class="dv">1</span>_hat) <span class="op">/</span><span class="st"> </span>SE_beta_<span class="dv">1</span>_hat</a>
<a class="sourceLine" id="cb170-14" data-line-number="14">t_intercept =<span class="st"> </span>beta_<span class="dv">0</span>_hat <span class="op">/</span><span class="st"> </span>SE_beta_<span class="dv">0</span>_hat</a>
<a class="sourceLine" id="cb170-15" data-line-number="15"><span class="kw">tibble</span>(t_slope, t_intercept)</a></code></pre></div>
<pre><code>## # A tibble: 1 x 2
##   t_slope t_intercept
##     &lt;dbl&gt;       &lt;dbl&gt;
## 1    7.31       -4.19</code></pre>
<p>Calculate <span class="math inline">\(p\)</span>-values (two sided!) for both of these values:</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb172-1" data-line-number="1">p_value_intercept =<span class="st"> </span><span class="kw">pt</span>(t_intercept, <span class="dt">df =</span> n_obs <span class="dv">-2</span>) <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">-</span><span class="kw">pt</span>(<span class="op">-</span>t_intercept, <span class="dt">df =</span> n_obs <span class="dv">-2</span>)</a>
<a class="sourceLine" id="cb172-2" data-line-number="2">p_value_slope     =<span class="st"> </span><span class="kw">pt</span>(<span class="op">-</span>t_slope, <span class="dt">df =</span> n_obs <span class="dv">-2</span>) <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">-</span><span class="kw">pt</span>(t_slope, <span class="dt">df =</span> n_obs <span class="dv">-2</span>)</a>
<a class="sourceLine" id="cb172-3" data-line-number="3"><span class="kw">tibble</span>(p_value_intercept, p_value_slope)</a></code></pre></div>
<pre><code>## # A tibble: 1 x 2
##   p_value_intercept p_value_slope
##               &lt;dbl&gt;         &lt;dbl&gt;
## 1          0.000554   0.000000866</code></pre>
<p>We compare the manual calculation to the that of the built-in functions <code>lm</code> and <code>glm</code>:</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb174-1" data-line-number="1"><span class="kw">summary</span>(<span class="kw">lm</span>(murder_rate <span class="op">~</span><span class="st"> </span>unemployment, <span class="dt">data =</span> murder_data))</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = murder_rate ~ unemployment, data = murder_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.2415 -3.7728  0.5795  3.2207 10.4221 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -28.5267     6.8137  -4.187 0.000554 ***
## unemployment   7.0796     0.9687   7.309 8.66e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.097 on 18 degrees of freedom
## Multiple R-squared:  0.748,  Adjusted R-squared:  0.7339 
## F-statistic: 53.41 on 1 and 18 DF,  p-value: 8.663e-07</code></pre>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb176-1" data-line-number="1"><span class="kw">summary</span>(<span class="kw">glm</span>(murder_rate <span class="op">~</span><span class="st"> </span>unemployment, <span class="dt">data =</span> murder_data))</a></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = murder_rate ~ unemployment, data = murder_data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -9.2415  -3.7728   0.5795   3.2207  10.4221  
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -28.5267     6.8137  -4.187 0.000554 ***
## unemployment   7.0796     0.9687   7.309 8.66e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 25.9779)
## 
##     Null deviance: 1855.2  on 19  degrees of freedom
## Residual deviance:  467.6  on 18  degrees of freedom
## AIC: 125.8
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<!-- ## Snippets -->
<!-- ### Preliminaries -->
<!-- <div class = "grey"> -->
<!-- - *data* for simple linear regression: -->
<!--     - single dependent variable $y$ with observed data points $i: y_1,..., y_n$ -->
<!--     - $k$ predictor variables $x_1, ..., x_k$ and observations $x_{j1}, ..., x_{jn}$ for each $x_j$ -->
<!-- - we consider a *regression model*: -->
<!--     - $y_i \sim Normal(\beta_0 + \beta_1 x_{1i} + ... + \beta_k x_{ki}, \sigma)$ -->
<!--     - $\beta_j$ and $\sigma$ are free parameters, as usual -->
<!-- - further notation: -->
<!--     - $\bar{y} = \frac{1}{n} \sum_{i=1}^n y_i$    -   the mean of data observations  -->
<!--     - $\hat{y}_i = \hat{\beta_0} + \hat{\beta_1} x_1i + ... + \hat{\beta_k} x_{ki}$ - prediction for i-th data point for best fitting parameter values -->
<!-- </div> -->
<!-- --- -->
<!-- ### Definitions -->
<!-- <div class = "grey"> -->
<!-- **Total sum of squares:**  $$TSS = \sum_{i=1}^n (y_i - \bar{y})^2 \tag{0.1}$$ -->
<!-- **Explained sum of squares:**  $$ESS = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2\tag{0.2}$$ -->
<!-- **Residual sum of squares:**  $$RSS = \sum_{i=1}^n (y_i - \hat{y}_i)^2\tag{0.3}$$ -->
<!-- **Likelihood:**  $$LH = \prod_{i=1}^n Normal(\beta_0 + \beta_1 x_{1i} + ... + \beta_k x_{ki}, \sigma)\tag{0.4}$$ -->
<!-- </div> -->
<!-- --- -->
<!-- ### Theorem 1 -->
<!-- #### Theorem 1a - special case: one predictor ($k=1$) -->
<!-- <div class = "blue"> -->
<!-- **Closed-form solution for parameter fit under ordinary least squares loss function (special case where $k=1$).** -->
<!-- *If there is only one predictor variable $(k=1)$, the closed-form solution of predictors in the model that minimize RSS (Residual Sum of Squares) is:* -->
<!-- $$\begin{align} -->
<!-- \hat{\beta_0} &= \bar{y} - \hat{\beta}_1 \bar{x}\textrm{, and}\\ -->
<!-- \\ -->
<!-- \hat{\beta_1} &= \frac{Cov(x,y)}{Var(x)}. -->
<!-- \end{align}$$ -->
<!-- </div> -->
<!-- ##### Proof -->
<!-- *[See e.g., @kirchner2003, pp. 1-3; @olive2017, pp. 57-59]* -->
<!-- Given a set of $n$ observations $(X_i,Y_i)$ (or points on a scatter plot), we want to find the best-fit line,  -->
<!-- $$\hat y_i=\hat\beta_0+\hat\beta_1x_i,\tag{1.1.1}$$ -->
<!-- such that the sum of squared errors (RSS) in $Y$ is minimized: -->
<!-- $$RSS=\sum_{i=1}^n (y_i - \hat{y}_i)^2 \rightarrow min.\tag{1.1.2}$$ -->
<!-- Let the *Residual Sum of Squares (RSS)*  be denoted as $Q$ with, -->
<!-- $$\begin{align} -->
<!-- Q=RSS&=\sum_{i=1}^{n}(y_i-\hat y_i)^2\\ &=\sum_{i=1}^{n}(y_i-\hat\beta_0-\hat\beta_1x_i)^2. -->
<!-- \tag{1.1.3} -->
<!-- \end{align}$$ -->
<!-- We want to minimize $Q$ (that is minimizing *RSS*) at the values of $\hat\beta_0$ and $\hat\beta_1$ for which $\frac{\partial Q}{\partial \hat\beta_0}=0$ (1) and $\frac{\partial Q}{\partial \hat\beta_1}=0$ (2). -->
<!-- The first condition (1) is, -->
<!-- $$ \begin{align} \frac{\partial Q}{\partial \hat\beta_0}=\sum_{i=1}^{n}-2(y_i-\hat\beta_0-\hat\beta_1x_i)&= 0\\ -->
<!-- &=-\sum_{i=1}^ny_i+\sum_{i=1}^n\hat \beta_0+\sum_{i=1}^n\hat\beta_1x_i\\ -->
<!-- &=-\sum_{i=1}^ny_i+n\hat\beta_0+\sum_{i=1}^n\hat\beta_1x_i -->
<!-- \tag{1.1.4} -->
<!-- \end{align}$$ -->
<!-- which, if we solve for $\hat\beta_0$, becomes -->
<!-- $$\begin{align} -->
<!-- \hat\beta_0&=\frac{1}{n}\sum_{i=1}^{n}y_i-\frac{1}{n}\hat\beta_1\sum_{i=1}^{n}x_i\\ -->
<!-- &=\bar y - \hat\beta_1\bar x, -->
<!-- \tag{1.1.5} -->
<!-- \end{align}$$ -->
<!-- which says that the constant $\hat\beta_0$ (the y-intercept) is set such that the line must go through the mean of $x$ and $y$. This makes sense, because this point is the "center" of the data cloud. -->
<!-- The solution is indeed a minimum as the second partial derivative is positive: -->
<!-- $\frac{\partial^2 Q}{\partial\hat\beta_0^2}=2n>0. \tag{1.1.6}$ -->
<!-- The second condition (2) is, -->
<!-- $$ \begin{align} -->
<!-- \frac{\partial Q}{\partial \hat\beta_1}=\sum_{i=1}^{n}-2x_i(y_i-\hat\beta_0-\hat\beta_1x_i)&=0\\ -->
<!-- &=\sum_{i=1}^{n}(-x_iy_i+\hat\beta_0x_i+\hat\beta_1x_i^2)\\ -->
<!-- &=-\sum_{i=1}^{n}x_iy_i+\hat\beta_0\sum_{i=1}^{n}x_i+\hat\beta_1\sum_{i=1}^{n}x_i^2 -->
<!-- \tag{1.1.7} -->
<!-- \end{align}$$ -->
<!-- If we substitute the expression by (1.1.5), we get, -->
<!-- $$ \begin{align} -->
<!-- 0&=-\sum_{i=1}^{n}x_iy_i+(\bar y - \hat\beta_1\bar x)\sum_{i=1}^{n}x_i+\hat\beta_1\sum_{i=1}^{n}x_i^2\\ -->
<!-- &=-\sum_{i=1}^{n}x_iy_i+\bar y\sum_{i=1}^{n}x_i-\hat\beta_1\bar x\sum_{i=1}^{n}x_i+\hat\beta_1\sum_{i=1}^{n}x_i^2 -->
<!-- \tag{1.1.8} -->
<!-- \end{align}$$ -->
<!-- separating this into two sums, -->
<!-- $$ \sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)-\hat\beta_1\sum_{i=1}^{n}\left(x_i^2-x_i\bar x\right)=0 \tag{1.1.9}$$ -->
<!-- becomes, -->
<!-- $$ \hat\beta_1 = \frac{\sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)}{\sum_{i=1}^{n}\left( x_i^2-x_i\bar x\right)} = \frac{\sum_{i=1}^{n}\left( x_iy_i\right)-n\bar x\bar y}{\sum_{i=1}^{n}\left( x_i^2\right)-n \bar x^2} \tag{1.1.10}$$ -->
<!-- The model assumes that the deviation from the values from the mean is zero, so that the positive and negative values are in balance, thus -->
<!-- $$ \sum_{i=1}^{n}\left( \bar x^2-x_i\bar x\right)=0, \tag{1.1.11}$$ -->
<!-- and -->
<!-- $$ \sum_{i=1}^{n}\left(\bar x \bar y - y_i \bar x\right)=0. \tag{1.1.12}$$ -->
<!-- This can be used in order to expand the previous term and finally to rewrite $\hat\beta_1$ as the ratio of $Cov(x,y)$ to $Var(x)$: -->
<!-- $$ -->
<!-- \begin{align} -->
<!-- \hat\beta_1&=\frac{\sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)+\sum_{i=1}^{n}\left(\bar x\bar y - y_i \bar x\right)}{\sum_{i=1}^{n}\left( x_i^2-x_i\bar x\right)+\sum_{i=1}^{n}\left( \bar x^2-x_i\bar x\right)}=\frac{\sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)+0}{\sum_{i=1}^{n}\left( x_i^2-x_i\bar x\right)+0}\\ -->
<!-- \\ -->
<!-- &=\frac{\frac{1}{n}\sum_{i=1}^{n}\left( x_i-\bar x\right) \left(y_i- \bar y \right)}{\frac{1}{n}\sum_{i=1}^{n}\left( x_i-\bar x\right)^2}\\ -->
<!-- \\ -->
<!-- &=\frac{Cov(x,y)}{Var(x)}. -->
<!-- \tag{1.1.13} -->
<!-- \end{align}$$ -->
<!-- The solution is indeed a minimum as the second partial derivative is positive: -->
<!-- $$\frac{\partial^2Q}{\partial \hat\beta_1^2}= 2 \sum_{i=1}^{n}x_i^2 >0. \tag{1.1.14}$$ -->
<!-- #### Theorem 1b: Generalization of Theorem 1a to $k >=1$ -->
<!-- *[see e.g., @bremer2012, pp. 21-23; @gonzalez2014, pp. 5-15]* -->
<!-- The model of multiple linear regression is given by the following expression: -->
<!-- $$y=\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_kx_k+\epsilon \tag{1.2.1}$$ -->
<!-- Suppose we have $n$ observations, then we can write: -->
<!-- $$\begin{align} -->
<!-- y_1&=\beta_0+\beta_{1}x_{11}+\beta_2x_{21}+...+\beta_kx_{k1}+\epsilon_1\\ -->
<!-- y_2&=\beta_0+\beta_{1}x_{12}+\beta_2x_{22}+...+\beta_kx_{k2}+\epsilon_2\\ -->
<!-- ...\\ -->
<!-- y_n&=\beta_0+\beta_{1}x_{1n}+\beta_2x_{2n}+...+\beta_kx_{kn}+\epsilon_n -->
<!-- \tag{1.2.2} -->
<!-- \end{align}$$ -->
<!-- The model of multiple linear regression is often expressed in matrix notation: -->
<!-- $$\begin{bmatrix} y_1\\y_2\\...\\y_n \end{bmatrix}= \begin{bmatrix}1&x_{11}& x_{21}&...&x_{k1}\\1&x_{12}& x_{22}&...&x_{k2}\\...& ...&...&...&...\\1&x_{1n}&x_{2n}&...&x_{kn}\end{bmatrix}\begin{bmatrix}\beta_0\\\beta_1\\...\\\beta_n \end{bmatrix}+\begin{bmatrix}\epsilon_1\\\epsilon_2\\...\\\epsilon_n \end{bmatrix} \tag{1.2.3}$$ -->
<!-- Which can be expressed in a compact form as -->
<!-- $$\mathbf{Y=X\beta+\epsilon} \tag{1.2.4}$$ -->
<!-- where $y$ is a vector $n\times 1$, $X$ is a matrix $n \times k$, $\beta$ is a vector $k \times 1$ and $\epsilon$ is a vector $n \times 1$. -->
<!-- The OLS estimator is obtained (like in the special case) by minimizing the residual sum of squares (RSS).  -->
<!-- $$RSS \rightarrow min.$$  -->
<!-- The RSS for the multiple linear regression model is -->
<!-- $$Q=RSS=\sum_{i=1}^n \hat\epsilon_i^2=\sum_{i=1}^n (y_i - \hat{y}_i)^2=\sum_{i=1}^n \left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]^2 \tag{1.2.5}$$ -->
<!-- to apply the least squares criterion in the model of multiple linear regression, thus to minimize $RSS$, we calculate the first partial derivative from $Q$ with respect to each $\hat\beta_j$in the expression: -->
<!-- $$\begin{align} -->
<!-- \frac{\partial Q}{\partial\hat\beta_0}&=2\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right][-1]\\ -->
<!-- \\ -->
<!-- \frac{\partial Q}{\partial\hat\beta_1}&=2\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right][-x_{1i}]\\ -->
<!-- \\ -->
<!-- \frac{\partial Q}{\partial\hat\beta_2}&=2\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right][-x_{2i}]\\ -->
<!-- ...\\ -->
<!-- \frac{\partial Q}{\partial\hat\beta_k}&=2\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right][-x_{ki}] -->
<!-- \tag{1.2.6} -->
<!-- \end{align}$$ -->
<!-- Then the derivative of each equation is set to zero: -->
<!-- $$\begin{align} -->
<!-- &\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]=0\\ -->
<!-- &\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]x_{1i}=0\\ -->
<!-- &\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]x_{2i}=0\\ -->
<!-- &...\\ -->
<!-- &\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]x_{ki}=0 -->
<!-- \tag{1.2.7} -->
<!-- \end{align}$$ -->
<!-- Alternatively, we can use matrix notation and combine the above equations into the following form: -->
<!-- $$\mathbf{X'Y-X'X\hat\beta=0}.\tag{1.2.8}$$ -->
<!-- Whereby the following expression is known as **normal equations**: -->
<!-- $$\mathbf{X'X\hat\beta=X'Y}.\tag{1.2.9}$$ -->
<!-- The system of normal equations in expanded matrix notation is: -->
<!-- $$\begin{bmatrix} n&\sum_{i=1}^nx_{1i}&...&\sum_{i=1}^nx_{ki}\\ -->
<!-- \sum_{i=1}^nx_{1i}&\sum_{i=1}^nx_{1i}^2&...&\sum_{i=1}^nx_{1i}x_{ki}\\...&...&...&...\\ -->
<!-- \sum_{i=1}^nx_{ki}&\sum_{i=1}^nx_{ki}x_{1i}&...&\sum_{i=1}^nx_{ki}^2\end{bmatrix}\begin{bmatrix}\hat\beta_0\\\hat\beta_1\\...\\\hat\beta_k\end{bmatrix}=\begin{bmatrix}\sum_{i=1}^ny_i\\\sum_{i=1}^nx_{1i}y_i\\...\\\sum_{i=1}^nx_{ki}y_i  -->
<!-- \tag{1.2.10} -->
<!-- \end{bmatrix}$$ -->
<!-- In order to obtain the estimator $\hat\beta$, we have to rearrange (1.2.10) and get the solution: -->
<!-- $$\begin{bmatrix}\hat\beta_0\\\hat\beta_1\\...\\\hat\beta_k\end{bmatrix}=\mathbf{\hat\beta}=[\mathbf{X'X}]^{-1}\mathbf{X'Y}\tag{1.2.11}$$ -->
<!-- Where $\hat\beta$ is a global minimizer of the OLS criterion as the scond order condition is always a semidefinite positive matrix. -->
<!-- $$\frac{\partial^2 Q}{\partial \mathbf{\hat\beta}^2}=2X'X >0.$$ -->
<!-- --- -->
<!-- ### Theorem 2 -->
<!-- <div class = "blue"> -->
<!-- **Best fit under OLS is equivalent with best fit under MLE** -->
<!-- *The parameters $\beta_0, ..., \beta_k$ minimize the residual sum of squares (RSS) iff they maximize the (log-)likelihood (LH).* -->
<!-- </div> -->
<!-- ##### Proof -->
<!-- *[see e.g., @naveen2019; @croot2010; @eppes2019]* -->
<!-- #### Maximum Likelihood Estimation -->
<!-- We consider again the linear regression model of the population with: -->
<!-- $$Y= \beta_0 + \beta_1X + \epsilon.\tag{2.1}$$ -->
<!-- This simplifies to the following form on the observed data: -->
<!-- $$y= \beta_0 + \beta_1x + \epsilon.\tag{2.2}$$ -->
<!-- Using a sample in order to obtain the **maximum likelihood estimates** the equation simplifies to: -->
<!-- $$y= \hat\beta_0 + \hat\beta_1x. \tag{2.3}$$ -->
<!-- **Assumptions** that we make for the model: -->
<!-- - True underlying distribution of the errors is Gaussian, -->
<!-- - Expected value of the error term is 0, -->
<!-- - Variance of the error term is constant with respect to x, and -->
<!-- - the 'lagged' errors are independent of each other -->
<!-- where the error term is normally distributed. -->
<!-- We can write: -->
<!-- $$ \epsilon \sim N(0,\sigma^2).\tag{2.4}$$ -->
<!-- Since $Y$ is a linear function of $\epsilon$ it will also be normally distributed. -->
<!-- $$f(\epsilon|\beta_0,\beta_1)= \frac{1}{\sqrt{2\pi} \sigma} \exp\left[{-\frac{1}{2}\left(\frac{\epsilon^2}{\sigma^2}\right)}\right]. \tag{2.5}$$  -->
<!-- Given the whole data set with $i=1,...,n$ observations the **likelihood function** ($LH$) is the joint density of all the observations, given a value for the parameters $\beta_0$ and $\beta_1$. Since independence is assumed, this is simply the product of the individual densities from the previous equation.  -->
<!-- $$LH(\epsilon_i|\beta_0,\beta_1) =\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi} \sigma} \exp\left[{-\frac{1}{2}\left(\frac{\epsilon_i^2}{\sigma^2}\right)}\right].\tag{2.6}$$ -->
<!-- In order to find the maximum of (2.6) we have to find the first derivative. -->
<!-- But as the derivation of a product with a lot of factors is inconvenient, we take first the logarithm of the likelihood, called **log-likelihhod**. This is possible as $\log$ is a monotone transformation and the maximum likelihood estimate does not change on log transformation. -->
<!-- The log-likelihood is the sum of the logs of the individual densities: -->
<!-- $$\begin{align} -->
<!-- LL&=\log \left(LH(\epsilon_i|\beta_0,\beta_1)\right)\\ -->
<!-- &=-\left( \frac{n}{2}\right) \log(2\pi)-\left( \frac{n}{2}\right) \log(\sigma^2)-\left( \frac{1}{2}\sigma^2\right) \sum_{i=1}^n(\epsilon_i)^2 -->
<!-- \tag{2.7} -->
<!-- \end{align}$$ -->
<!-- The log-likelihood can be used in order to find the *Maximum Likelihood estimates* $\hat\beta_0$ and $\hat\beta_1$ for the parameters $\beta_0$ and $\beta_1$. -->
<!-- $$\hat\beta_0,\hat\beta_1=\mathrm{argmax}_{\beta_0,\beta_1}LL=\mathrm{argmax}_{\beta_0,\beta_1}\left[-\left( \frac{n}{2}\right) \log(2\pi)-\left( \frac{n}{2}\right) \log(\sigma^2)-\left( \frac{1}{2}\sigma^2\right) \sum_{i=1}^n(\epsilon_i)^2\right].\tag{2.8}$$ -->
<!-- Removing the constant terms results in: -->
<!-- $$\hat\beta_0,\hat\beta_1=\mathrm{argmax}_{\beta_0,\beta_1} \sum_{i=1}^n-\epsilon_i^2.\tag{2.9}$$ -->
<!-- Substituting $\epsilon$, derived from (2.2), gives: -->
<!-- $$\hat\beta_0,\hat\beta_1=\mathrm{argmax}_{\beta_0,\beta_1} \sum_{i=1}^n-(y-\beta_0-\beta_1x)^2.\tag{2.10}$$ -->
<!-- **Conclusion:** -->
<!-- Deriving parameter estimates according to the OLS method: -->
<!-- $$Q_{OLS}=\sum_{i=1}^{n}(y_i-\hat\beta_0-\hat\beta_1x_i)^2 \rightarrow min.$$ -->
<!-- Deriving parameter estimates according to the ML method: -->
<!-- $$Q_{ML}=\sum_{i=1}^n-(y_i-\hat\beta_0-\hat\beta_1x_i)^2 \rightarrow max.$$ -->
<!-- Maximizing $-z$ is equivalent to minimizing $z$, thus, the best parameter fit under ML is equivalent with best fit under OLS. -->
<!-- --- -->
<!-- ### Theorem 3 -->
<!-- <div class = "blue"> -->
<!-- **The variance in a regression model can be decompossed by using the notion of sum of squares:** -->
<!-- *The following decomposition holds:* -->
<!-- $$\begin{align} -->
<!-- TSS &= ESS + RSS \textrm{, or}\\ -->
<!-- \sum_{i=1}^n (y_i - \bar{y})^2&=\sum_{i=1}^n (\hat{y}_i - \bar{y})^2+ \sum_{i=1}^n \hat u_i^2  -->
<!-- \end{align}$$ -->
<!-- </div> -->
<!-- with  -->
<!-- $$\hat u_i =y_i-\hat y_i $$ -->
<!-- which can be rewritten as -->
<!-- $$y_i=\hat y_i + \hat u_i \tag{3.1}$$ -->
<!-- #### Proof -->
<!-- [see e.g., @gonzalez2014 pp. 29-31] -->
<!-- $$\begin{align} -->
<!-- TSS=\sum_{i=1}^n (y_i - \bar{y})^2&=\sum_{i=1}^n\left( \hat y_i+\hat u_i - \bar y \right)^2\\ -->
<!-- &= \sum_{i=1}^n \left[(\hat y_i-\bar y)+\hat u_i\right]^2\\ -->
<!-- &= \sum_{i=1}^n (\hat y_i-\bar y)^2 + \sum_{i=1}^n \hat u_i^2+ 2\sum_{i=1}^n(\hat y_i-\bar y)\hat u_i\\ -->
<!-- &= \sum_{i=1}^n (\hat y_i-\bar y)^2 + \sum_{i=1}^n \hat u_i^2\\ -->
<!-- \tag{3.2} -->
<!-- \end{align}$$ -->
<!-- Or, using (3.1) we can write: -->
<!-- $$ -->
<!-- TSS=\sum_{i=1}^n (\hat{y}_i - \bar{y})^2+\sum_{i=1}^n (y_i - \hat{y}_i)^2=ESS+RSS. -->
<!-- \tag{3.3} -->
<!-- $$ -->
<!-- --- -->
<!-- ### Theorem 4 -->
<!-- <div class = "blue"> -->
<!-- **Expressing variance explained $R^2$ in terms of sum of squares** -->
<!-- *The following relationship holds:*  -->
<!--   $$R^2 = 1 - \frac{RSS}{TSS} = \frac{ESS}{TSS}$$ -->
<!-- </div> -->
<!-- #### Proof -->
<!-- [see e.g., @urban2018 pp. 56-72, 101-103 ] -->
<!-- We will first derive the variance explained $R^2$ considering a bivariate model (simple linear regression), afterwards some note to the generalized case of multiple regression are made. -->
<!-- The variance explained can be derived in the bivariate model from the regression coefficient $\beta$ and the standard deviation of $X$ and $Y$, whereby the following holds: -->
<!-- $$R^2=\left( \beta \cdot \frac{S_x}{S_y}\right)^2=\beta^2 \cdot \frac{Var(X)}{Var(Y)}.\tag{4.1}$$ -->
<!-- We will show that (4.1) holds in the next steps. -->
<!-- Consider on the one hand the calculation of the correlation coefficient: -->
<!-- $$\begin{align} -->
<!-- r_{xy}&=\frac{cov(X,Y)}{S_x S_y}\\ -->
<!-- \\ -->
<!-- &=\frac{\frac{1}{n}\sum_{i=1}^n(X_i-\bar X)(Y_i-\bar Y)}{\sqrt{\frac{\sum_{i=1}^n(X_i-\bar X)^2}{n}} \sqrt{\frac{\sum_{i=1}^n(Y_i-\bar Y)^2}{n}}} -->
<!-- \tag{4.2} -->
<!-- \end{align}$$ -->
<!-- On the other hand, if we assume $Y$ as dependent and $X$ as independent variable, we can write: -->
<!-- $$\beta_{yx}=\frac{\sum_{i=1}^{n}X_iY_i}{\sum_{i=1}^n X_i^2}. \tag{4.3}$$ -->
<!-- Or when $Y$ is the independent and $X$ the dependent variable, then: -->
<!-- $$\beta_{xy}=\frac{\sum_{i=1}^{n}Y_iX_i}{\sum_{i=1}^n Y_i^2}.\tag{4.4}$$ -->
<!-- Through dividing equations (4.3) and (4.4) by the number of observations, we get the covariances and variances of $X$ and $Y$: -->
<!-- $$\begin{align} -->
<!-- &\frac{1}{n}\sum_{i=1}^{n}X_iY_i=\frac{1}{n}(X_i-\bar X)(Y_i-\bar Y) = cov(X,Y)\tag{4.5}\\ -->
<!-- &\frac{1}{n}\sum_{i=1}^{n}Y_iX_i=\frac{1}{n}(Y_i-\bar Y)(X_i-\bar X) = cov(Y,X)\tag{4.6}\\ -->
<!-- &\frac{1}{n}\sum_{i=1}^{n}X_i^2=\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar X)^2=S_x^2\tag{4.7}\\ -->
<!-- &\frac{1}{n}\sum_{i=1}^{n}Y_i^2=\frac{1}{n}\sum_{i=1}^{n}(Y_i-\bar Y)^2=S_y^2 -->
<!-- \tag{4.8} -->
<!-- \end{align}$$ -->
<!-- Substituting these results in equations (4.3) and (4.4), we get: -->
<!-- $$\beta_{yx}=\frac{cov(X,Y)}{S_x^2} \textrm{ , and } -->
<!-- \beta_{xy}=\frac{cov(Y,X)}{S_y^2}\tag{4.9}$$ -->
<!-- For calculating the *average* between $\beta_{yx}$ and $\beta_{xy}$ we have to use the *geometric mean*: -->
<!-- $$\bar\beta_{geom}=\sqrt{\frac{cov(X,Y)}{S_x^2}\frac{cov(Y,X)}{S_y^2}}=\frac{cov(X,Y)}{S_x S_y}=r_{xy}\tag{4.10}$$ -->
<!-- Thus, the geometric mean of $\beta_{yx}$ and $\beta_{xy}$ is identical to the correlation coefficient.  -->
<!-- Comparing equations (4.9) and (4.10), -->
<!-- $$\begin{align} -->
<!-- \beta_{yx}&=\frac{cov(X,Y)}{S_x^2}\textrm{ , and}\\ -->
<!-- \\ -->
<!-- r_{yx}&=\frac{cov(X,Y)}{S_x S_y}, -->
<!-- \end{align}$$ -->
<!-- shows that, we can convert $r_{yx}$ in $\beta_{yx}$: -->
<!-- $$\frac{cov(X,Y)}{S_x S_y} \cdot \frac{S_y}{S_x}=\frac{cov(X,Y)}{S_x^2}=\beta_{yx}.\tag{4.11}$$ -->
<!-- Therefore, the relationship stated in (4.1) holds: -->
<!-- $$\beta_{yx}=r_{yx}\cdot \frac{S_y}{S_x}.$$ -->
<!-- Using this definition we can now derive the **variance explained** $R^2$. -->
<!-- We have seen that the following holds: -->
<!-- $$\beta_{yx}=r_{yx}\cdot \frac{S_y}{S_x}=r_{yx}\sqrt{\frac{\frac{1}{n}\sum_{i=1}^{n}(Y_i-\bar Y)^2}{\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar X)^2}}.$$ -->
<!-- Eliminating the number of observations and squaring the equation gives: -->
<!-- $$\beta_{yx}^2=r_{yx}^2\frac{\sum_{i=1}^{n}(Y_i-\bar Y)^2}{\sum_{i=1}^{n}(X_i-\bar X)^2}.\tag{4.12}$$ -->
<!-- The numerator $\sum_{i=1}^{n}(Y_i-\bar Y)^2$ is the total sum of squares (TSS), thus, from (0.1) follows: -->
<!-- $$\sum_{i=1}^{n}(Y_i-\bar Y)^2=\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2+\sum_{i=1}^{n}(Y_i-\hat Y_i)^2.\tag{4.13}$$ -->
<!-- The explained sum of squares (ESS) $\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2$ are statistically explained by the determination of the regression line: -->
<!-- $$ESS=\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2=\beta X_i=\beta(X_i-\bar X).\tag{4.14}$$ -->
<!-- Substituting the result of (4.14) in equation (4.13) gives: -->
<!-- $$\sum_{i=1}^{n}(Y_i-\bar Y)^2=\beta^2\sum_{i=1}^{n}(X_i-\bar X)^2+\sum_{i=1}^{n}(Y_i-\hat Y_i)^2.\tag{4.15}$$ -->
<!-- As we know from (4.12): -->
<!-- $$\beta_{yx}^2=r_{yx}^2\frac{\sum_{i=1}^{n}(Y_i-\bar Y)^2}{\sum_{i=1}^{n}(X_i-\bar X)^2},$$ -->
<!-- we can substitute $\beta$ in equation (4.15) with the result from (4.12) and get: -->
<!-- $$\begin{align} -->
<!-- \sum_{i=1}^{n}(Y_i-\bar Y)^2&=r_{xy}^2\frac{\sum_{i=1}^{n}(Y_i-\bar Y)^2}{\sum_{i=1}^{n}(X_i-\bar X)^2}\sum_{i=1}^{n}(X_i-\bar X)^2+\sum_{i=1}^{n}(Y_i-\hat Y_i)^2\\ -->
<!-- \\ -->
<!-- &=r_{xy}^2 \sum_{i=1}^{n}(Y_i-\bar Y)^2+\sum_{i=1}^{n}(Y_i-\hat Y_i)^2\tag{4.16}\\ -->
<!-- \\ -->
<!-- \frac{\sum_{i=1}^{n}(Y_i-\bar Y)^2-\sum_{i=1}^{n}(Y_i-\hat Y_i)^2}{\sum_{i=1}^{n}(Y_i-\bar Y)^2}&=r_{xy}^2 \textrm{ ,which is}\\ -->
<!-- \\ -->
<!-- 1-\frac{\sum_{i=1}^{n}(Y_i-\hat Y_i)^2}{\sum_{i=1}^{n}(Y_i-\bar Y)^2}=1-\frac{RSS}{TSS}&=r_{xy}^2. -->
<!-- \tag{4.17} -->
<!-- \end{align}$$ -->
<!-- Similarly, according to (0.2) the explained sum of squares (ESS) are the deviation between total (TSS) and residual sum of squares (RSS): -->
<!-- $$\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2 = \sum_{i=1}^{n}( Y_i-\bar Y)^2-\sum_{i=1}^{n}(Y_i-\hat Y)^2,$$ -->
<!-- therefore, we can write equation (4.17) as well as: -->
<!-- $$\begin{align} -->
<!-- \frac{\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2}{\sum_{i=1}^{n}(Y_i-\bar Y)^2}&=r_{xy}^2\\ -->
<!-- \\ -->
<!-- \frac{ESS}{RSS}&=r_{xy}^2=R^2. -->
<!-- \tag{4.18} -->
<!-- \end{align}$$ -->
<!-- **Final remarks:** -->
<!-- In the bivariate model (simple linear regression) the variance explained is equivalent to the squared bivariate correlation coefficient (Bravais Pearson).  -->
<!-- In multiple regression model the relation: -->
<!-- $$\frac{ESS}{TSS}=R^2$$ -->
<!-- holds as well. Here the variance explained is equivalent to the squared multiple correlation coefficient between the estimated and observed Y-value ($r_{\hat Y Y}$). -->
<!-- In the multiple linear regression model the variance explained is also dependent in the number of X-variables. Therefore an adjusted $R^2$ measure should be used. -->

</div>
</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="a-bayesian-approach.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="Chap-04-02-beyond-simple-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["I2DA.epub", "I2DA.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
